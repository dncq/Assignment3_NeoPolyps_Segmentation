{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":30892,"databundleVersionId":2715462,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary\n!pip install torchgeometry","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-17T12:14:33.285167Z","iopub.execute_input":"2023-11-17T12:14:33.285952Z","iopub.status.idle":"2023-11-17T12:14:56.938394Z","shell.execute_reply.started":"2023-11-17T12:14:33.285913Z","shell.execute_reply":"2023-11-17T12:14:56.937106Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\nRequirement already satisfied: torchgeometry in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install albumentations","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:14:56.940619Z","iopub.execute_input":"2023-11-17T12:14:56.940970Z","iopub.status.idle":"2023-11-17T12:15:08.779364Z","shell.execute_reply.started":"2023-11-17T12:14:56.940942Z","shell.execute_reply":"2023-11-17T12:15:08.778187Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (1.3.1)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.24.3)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.11.3)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.21.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations) (6.0.1)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.0.4)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.8.1.78)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.2.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.5.0)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (10.1.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2023.8.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.16.1->albumentations) (3.0.9)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchsummary import summary\nfrom torchgeometry.losses import one_hot\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport time\nimport imageio\nimport matplotlib.pyplot as plt\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\nfrom torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\nfrom collections import OrderedDict\nimport wandb\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations.augmentations import transforms\nfrom albumentations.core.composition import Compose, OneOf\nfrom albumentations.augmentations.transforms import RandomGamma","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:08.781130Z","iopub.execute_input":"2023-11-17T12:15:08.781482Z","iopub.status.idle":"2023-11-17T12:15:08.790573Z","shell.execute_reply.started":"2023-11-17T12:15:08.781454Z","shell.execute_reply":"2023-11-17T12:15:08.789636Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.compat.v1 import InteractiveSession\n\n# tf.keras.backend.clear_session()\n# config = tf.compat.v1.ConfigProto()\n# config.gpu_options.allow_growth = True\n# session = InteractiveSession(config=config)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:08.793486Z","iopub.execute_input":"2023-11-17T12:15:08.794080Z","iopub.status.idle":"2023-11-17T12:15:08.805279Z","shell.execute_reply.started":"2023-11-17T12:15:08.794047Z","shell.execute_reply":"2023-11-17T12:15:08.804398Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:08.806513Z","iopub.execute_input":"2023-11-17T12:15:08.807256Z","iopub.status.idle":"2023-11-17T12:15:09.788101Z","shell.execute_reply.started":"2023-11-17T12:15:08.807224Z","shell.execute_reply":"2023-11-17T12:15:09.787053Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"GPU 0: Tesla T4 (UUID: GPU-d0d6e30c-cf77-7ca0-5871-f813fa4ed38a)\nGPU 1: Tesla T4 (UUID: GPU-32deceac-4214-62f9-ae6f-02b63feeb9d3)\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:09.789930Z","iopub.execute_input":"2023-11-17T12:15:09.790355Z","iopub.status.idle":"2023-11-17T12:15:09.798169Z","shell.execute_reply.started":"2023-11-17T12:15:09.790302Z","shell.execute_reply":"2023-11-17T12:15:09.797140Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# Number of class in the data set (3: neoplastic, non neoplastic, background)\nnum_classes = 3\n\n# Number of epoch\nepochs = 30\n\n# Hyperparameters for training \nlearning_rate = 2e-04\nbatch_size = 4\ndisplay_step = 50\n\n# Model path\ncheckpoint_path = '/kaggle/working/unet_model.pth'\npretrained_path = \"/kaggle/input/unet-checkpoint/unet_model.pth\"\n# Initialize lists to keep track of loss and accuracy\nloss_epoch_array = []\ntrain_accuracy = []\ntest_accuracy = []\nvalid_accuracy = []","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:09.799714Z","iopub.execute_input":"2023-11-17T12:15:09.800092Z","iopub.status.idle":"2023-11-17T12:15:09.808442Z","shell.execute_reply.started":"2023-11-17T12:15:09.800057Z","shell.execute_reply":"2023-11-17T12:15:09.807572Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"class TrainTransform:\n    def __init__(self):\n        self.transform = A.Compose([\n            A.HorizontalFlip(p=0.3),\n            A.VerticalFlip(p=0.3),\n            A.RandomGamma(gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n            A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n            A.OneOf([A.Blur(), A.GaussianBlur(), A.GlassBlur(), A.MotionBlur(),\n                    A.GaussNoise(), A.Sharpen(), A.MedianBlur(), A.MultiplicativeNoise()]),\n            A.CoarseDropout(p=0.2, max_height=35, max_width=35, fill_value=255),\n            A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.15, brightness_coeff=1.5, p=0.09),\n            A.RandomShadow(p=0.1),\n            A.ShiftScaleRotate(p=0.45, border_mode=cv2.BORDER_CONSTANT, shift_limit=0.15, scale_limit=0.15),\n            A.Resize(256, 256, interpolation=cv2.INTER_LINEAR),\n            A.Normalize(),\n            ToTensorV2(),\n        ])\n\n    def __call__(self, img, mask):\n        return self.transform(image=img, mask=mask)\n\n\nclass ValTransform:\n    def __init__(self):\n        self.transform = A.Compose([\n            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ])\n    def __call__(self, img, mask):\n        return self.transform(image=img, mask=mask)\n\n\nclass TestTransform:\n    def __init__(self):\n        self.transform = A.Compose([\n            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ])\n    def __call__(self, img):\n        return self.transform(image=img)['image']","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:09.812124Z","iopub.execute_input":"2023-11-17T12:15:09.812506Z","iopub.status.idle":"2023-11-17T12:15:09.826719Z","shell.execute_reply.started":"2023-11-17T12:15:09.812481Z","shell.execute_reply":"2023-11-17T12:15:09.825896Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# class UnetDataClass(Dataset):\n#     def __init__(self, image_dir,mask_dir = [],mode = \"train\"):\n#         super(UnetDataClass, self).__init__()\n#         self.mode = mode\n#         if mode == \"train\":\n#             self.train_path = image_dir\n#             self.train_mask_path = mask_dir\n#             self.len = len(self.train_path)\n#             self.train_transform = TrainTransform()\n#         elif mode == \"valid\":\n#             self.val_path = image_dir\n#             self.val_mask_path = mask_dir\n#             self.len = len(self.val_path)\n#             self.val_transform = ValTransform()\n#         elif mode == \"test\":\n#             self.test_path = image_dir\n#             self.len = len(self.test_path)\n#             self.test_transform = TestTransform()\n\n            \n#     def read_mask(self, mask_path):\n#         image = cv2.imread(mask_path)\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n#         image = cv2.resize(image, (256,256))\n#         # lower boundary RED color range values; Hue (0 - 10)\n#         lower1 = np.array([0, 100, 20])\n#         upper1 = np.array([10, 255, 255])\n#         # upper boundary RED color range values; Hue (160 - 180)\n#         lower2 = np.array([160, 100, 20])\n#         upper2 = np.array([179, 255, 255])\n#         lower_mask = cv2.inRange(image, lower1, upper1)\n#         upper_mask = cv2.inRange(image, lower2, upper2)\n\n#         red_mask = lower_mask + upper_mask\n#         red_mask[red_mask != 0] = 1\n\n#         # boundary GREEN color range values; Hue (36 - 70)\n#         green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n#         green_mask[green_mask != 0] = 2\n\n#         full_mask = cv2.bitwise_or(red_mask, green_mask)\n#         full_mask = full_mask.astype(np.uint8)\n#         return full_mask\n\n\n#     def __getitem__(self, index: int):\n#         if self.mode == \"train\":\n#             image = cv2.imread(self.train_path[index])\n#             image = cv2.resize(image, (256,256))\n#             mask = self.read_mask(self.train_mask_path[index])\n#             return self.train_transform(image, mask)\n#         elif self.mode == \"valid\":\n#             image = cv2.imread(self.val_path[index])\n#             image = cv2.resize(image, (256,256))\n#             mask = self.read_mask(self.val_mask_path[index])\n#             return self.val_transform(image, mask)\n#         elif self.mode == \"test\":\n#             image = cv2.imread(self.test_path[index])\n#             H, W, _ = image.shape\n#             image = cv2.resize(image, (256,256))\n#             image = self.test_transform(image)\n            \n#             file_name = self.test_path[index].split('/')[-1].split('.')[0]\n#             return  image, file_name,H, W\n        \n#     def __len__(self):\n#         return self.len","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:09.828088Z","iopub.execute_input":"2023-11-17T12:15:09.828480Z","iopub.status.idle":"2023-11-17T12:15:09.841047Z","shell.execute_reply.started":"2023-11-17T12:15:09.828447Z","shell.execute_reply":"2023-11-17T12:15:09.840277Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class UnetDataClass(Dataset):\n    def __init__(self, image_dir, transform, mask_dir = [],  mode = \"train\"):\n        super(UnetDataClass, self).__init__()\n        self.mode = mode\n        if mode == \"train\":\n            self.train_path = image_dir\n            self.train_mask_path = mask_dir\n            self.len = len(self.train_path)\n            self.train_transform = transform\n        elif mode == \"valid\":\n            self.val_path = image_dir\n            self.val_mask_path = mask_dir\n            self.len = len(self.val_path)\n            self.val_transform = transform\n        elif mode == \"test\":\n            self.test_path = image_dir\n            self.len = len(self.test_path)\n            self.test_transform = transform\n\n            \n    def read_mask(self, mask_path):\n        image = cv2.imread(mask_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        image = cv2.resize(image, (256,256))\n        # lower boundary RED color range values; Hue (0 - 10)\n        lower1 = np.array([0, 100, 20])\n        upper1 = np.array([10, 255, 255])\n        # upper boundary RED color range values; Hue (160 - 180)\n        lower2 = np.array([160, 100, 20])\n        upper2 = np.array([179, 255, 255])\n        lower_mask = cv2.inRange(image, lower1, upper1)\n        upper_mask = cv2.inRange(image, lower2, upper2)\n\n        red_mask = lower_mask + upper_mask\n        red_mask[red_mask != 0] = 1\n\n        # boundary GREEN color range values; Hue (36 - 70)\n        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n        green_mask[green_mask != 0] = 2\n\n        full_mask = cv2.bitwise_or(red_mask, green_mask)\n        full_mask = full_mask.astype(np.uint8)\n        return full_mask\n\n\n    def __getitem__(self, index: int):\n        if self.mode == \"train\":\n            image = cv2.imread(self.train_path[index])\n            image = cv2.resize(image, (256,256))\n            mask = self.read_mask(self.train_mask_path[index])\n            return self.train_transform(image=image, mask=mask)\n        elif self.mode == \"valid\":\n            image = cv2.imread(self.val_path[index])\n            image = cv2.resize(image, (256,256))\n            mask = self.read_mask(self.val_mask_path[index])\n            return self.val_transform(image=image, mask=mask)\n        elif self.mode == \"test\":\n            image = cv2.imread(self.test_path[index])\n            H, W, _ = image.shape\n            image = cv2.resize(image, (256,256))\n            image = self.test_transform(image=image)\n            \n            file_name = self.test_path[index].split('/')[-1].split('.')[0]\n            return  image, file_name,H, W\n        \n    def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:09.845637Z","iopub.execute_input":"2023-11-17T12:15:09.845939Z","iopub.status.idle":"2023-11-17T12:15:09.862321Z","shell.execute_reply.started":"2023-11-17T12:15:09.845915Z","shell.execute_reply":"2023-11-17T12:15:09.861569Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"image_path = []\nimages_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\nfor root, dirs, files in os.walk(images_path):\n    for file in files:\n        path = os.path.join(root,file)\n        image_path.append(path)\n\nmask_path = []\nmasks_path =  \"/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt/\"\nfor root, dirs, files in os.walk(masks_path):\n    for file in files:\n        path = os.path.join(root,file)\n        mask_path.append(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:09.863505Z","iopub.execute_input":"2023-11-17T12:15:09.863817Z","iopub.status.idle":"2023-11-17T12:15:10.236130Z","shell.execute_reply.started":"2023-11-17T12:15:09.863791Z","shell.execute_reply":"2023-11-17T12:15:10.235378Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import random\n\nshuffle_list = list(zip(image_path, mask_path))\nrandom.shuffle(shuffle_list)\nimage_path, mask_path = zip(*shuffle_list)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.237189Z","iopub.execute_input":"2023-11-17T12:15:10.237498Z","iopub.status.idle":"2023-11-17T12:15:10.244357Z","shell.execute_reply.started":"2023-11-17T12:15:10.237471Z","shell.execute_reply":"2023-11-17T12:15:10.243401Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"augment_transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n    A.OneOf([A.Blur(), A.GaussianBlur(), A.GlassBlur(), A.MotionBlur(), A.GaussNoise(), \n             A.Sharpen(), A.MedianBlur(), A.MultiplicativeNoise()]),\n    A.CoarseDropout(p=0.2, max_height=35, max_width=35, fill_value=255),\n    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.15, brightness_coeff=1.5, p=0.09),\n    A.RandomShadow(p=0.1),\n    A.ShiftScaleRotate(p=0.45, border_mode=cv2.BORDER_CONSTANT, shift_limit=0.15, scale_limit=0.15),\n    A.RandomCrop(256, 256),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nnormal_transform = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.245365Z","iopub.execute_input":"2023-11-17T12:15:10.245613Z","iopub.status.idle":"2023-11-17T12:15:10.256603Z","shell.execute_reply.started":"2023-11-17T12:15:10.245590Z","shell.execute_reply":"2023-11-17T12:15:10.255648Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"train_ratio = 0.8\nvalid_ratio = 0.2","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.257766Z","iopub.execute_input":"2023-11-17T12:15:10.258047Z","iopub.status.idle":"2023-11-17T12:15:10.265874Z","shell.execute_reply.started":"2023-11-17T12:15:10.258023Z","shell.execute_reply":"2023-11-17T12:15:10.265076Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"train_size = int(train_ratio * len(image_path))\ntrain_path = image_path[:train_size]\ntrain_mask_path = mask_path[:train_size]\nval_path = image_path[train_size:]\nval_mask_path = mask_path[train_size:]\n\n\nnormal_train_dataset = UnetDataClass(train_path,normal_transform,train_mask_path, mode=\"train\")\naugmented_train_dataset = UnetDataClass(train_path,augment_transform,train_mask_path, mode=\"train\")\n\ntrain_dataset = ConcatDataset([normal_train_dataset, augmented_train_dataset])\nval_dataset = UnetDataClass(val_path,normal_transform,val_mask_path,mode=\"valid\")","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.267230Z","iopub.execute_input":"2023-11-17T12:15:10.267609Z","iopub.status.idle":"2023-11-17T12:15:10.276118Z","shell.execute_reply.started":"2023-11-17T12:15:10.267580Z","shell.execute_reply":"2023-11-17T12:15:10.275382Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.289929Z","iopub.execute_input":"2023-11-17T12:15:10.290222Z","iopub.status.idle":"2023-11-17T12:15:10.298716Z","shell.execute_reply.started":"2023-11-17T12:15:10.290199Z","shell.execute_reply":"2023-11-17T12:15:10.297953Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"print(train_loader.dataset.__getitem__(7)['mask'])\nprint(train_loader.dataset.__getitem__(7)['mask'].shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.299785Z","iopub.execute_input":"2023-11-17T12:15:10.300084Z","iopub.status.idle":"2023-11-17T12:15:10.376674Z","shell.execute_reply.started":"2023-11-17T12:15:10.300060Z","shell.execute_reply":"2023-11-17T12:15:10.375794Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\ntorch.Size([256, 256])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(val_loader.dataset.__getitem__(7)['image'])\nprint(val_loader.dataset.__getitem__(7)['image'].shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.377841Z","iopub.execute_input":"2023-11-17T12:15:10.378145Z","iopub.status.idle":"2023-11-17T12:15:10.465267Z","shell.execute_reply.started":"2023-11-17T12:15:10.378120Z","shell.execute_reply":"2023-11-17T12:15:10.464377Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"tensor([[[-2.1179, -2.0494, -2.0152,  ..., -2.1179, -2.1179, -2.1179],\n         [-2.1008, -1.7583, -1.7240,  ..., -2.1179, -2.1179, -2.1179],\n         [-1.9638,  1.4783,  0.9303,  ..., -2.1179, -2.1179, -2.1179],\n         ...,\n         [-1.5357, -1.5357, -1.5357,  ..., -1.2445, -1.5699, -1.5699],\n         [-1.5528, -1.5528, -1.5528,  ...,  1.7352, -1.5870, -1.5870],\n         [-1.5014, -1.5014, -1.5699,  ..., -1.5357, -1.5699, -1.5699]],\n\n        [[-2.0357, -1.9657, -1.9307,  ..., -2.0357, -2.0357, -2.0357],\n         [-2.0182, -1.6681, -1.6331,  ..., -2.0357, -2.0357, -2.0357],\n         [-1.8782,  1.6408,  1.0805,  ..., -2.0357, -2.0357, -2.0357],\n         ...,\n         [-1.4230, -1.4230, -1.4230,  ..., -1.1078, -1.4405, -1.4405],\n         [-1.4580, -1.4580, -1.4580,  ...,  1.9209, -1.4580, -1.4580],\n         [-1.4055, -1.4055, -1.4755,  ..., -1.4580, -1.4755, -1.4755]],\n\n        [[-1.8044, -1.7347, -1.6999,  ..., -1.8044, -1.8044, -1.8044],\n         [-1.7870, -1.4384, -1.4036,  ..., -1.8044, -1.8044, -1.8044],\n         [-1.6476,  1.8557,  1.2980,  ..., -1.8044, -1.8044, -1.8044],\n         ...,\n         [-1.4733, -1.4733, -1.4733,  ..., -1.1073, -1.4210, -1.4210],\n         [-1.4733, -1.4733, -1.4733,  ...,  1.9428, -1.4384, -1.4384],\n         [-1.4210, -1.4210, -1.4907,  ..., -1.4733, -1.4907, -1.4907]]])\ntorch.Size([3, 256, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BUILD MODELS","metadata":{}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:10.466546Z","iopub.execute_input":"2023-11-17T12:15:10.466854Z","iopub.status.idle":"2023-11-17T12:15:22.264269Z","shell.execute_reply.started":"2023-11-17T12:15:10.466829Z","shell.execute_reply":"2023-11-17T12:15:22.263045Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation-models-pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.15.1)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.1.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.17.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2023.10.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2023.7.22)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ResUnet","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torchvision\n\n# def convrelu(in_channels, out_channels, kernel, padding):\n#     return nn.Sequential(\n#     nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n#     nn.ReLU(inplace=True),\n#   )\n\n# class ResNetUNet(nn.Module):\n#     def __init__(self, n_classes):\n#         super().__init__()\n\n#         self.base_model = torchvision.models.resnet18(pretrained=True)\n#         self.base_layers = list(self.base_model.children())\n\n#         self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n#         self.layer0_1x1 = convrelu(64, 64, 1, 0)\n#         self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n#         self.layer1_1x1 = convrelu(64, 64, 1, 0)\n#         self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n#         self.layer2_1x1 = convrelu(128, 128, 1, 0)\n#         self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n#         self.layer3_1x1 = convrelu(256, 256, 1, 0)\n#         self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n#         self.layer4_1x1 = convrelu(512, 512, 1, 0)\n\n#         self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n#         self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n#         self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n#         self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n#         self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n\n#         self.conv_original_size0 = convrelu(3, 64, 3, 1)\n#         self.conv_original_size1 = convrelu(64, 64, 3, 1)\n#         self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n\n#         self.conv_last = nn.Conv2d(64, n_classes, 1)\n\n#     def forward(self, input):\n#         x_original = self.conv_original_size0(input)\n#         x_original = self.conv_original_size1(x_original)\n\n#         layer0 = self.layer0(input)\n#         layer1 = self.layer1(layer0)\n#         layer2 = self.layer2(layer1)\n#         layer3 = self.layer3(layer2)\n#         layer4 = self.layer4(layer3)\n\n#         layer4 = self.layer4_1x1(layer4)\n#         x = self.upsample(layer4)\n#         layer3 = self.layer3_1x1(layer3)\n#         x = torch.cat([x, layer3], dim=1)\n#         x = self.conv_up3(x)\n\n#         x = self.upsample(x)\n#         layer2 = self.layer2_1x1(layer2)\n#         x = torch.cat([x, layer2], dim=1)\n#         x = self.conv_up2(x)\n\n#         x = self.upsample(x)\n#         layer1 = self.layer1_1x1(layer1)\n#         x = torch.cat([x, layer1], dim=1)\n#         x = self.conv_up1(x)\n\n#         x = self.upsample(x)\n#         layer0 = self.layer0_1x1(layer0)\n#         x = torch.cat([x, layer0], dim=1)\n#         x = self.conv_up0(x)\n\n#         x = self.upsample(x)\n#         x = torch.cat([x, x_original], dim=1)\n#         x = self.conv_original_size2(x)\n\n#         out = self.conv_last(x)\n\n#         return out","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:22.315006Z","iopub.execute_input":"2023-11-17T12:15:22.315547Z","iopub.status.idle":"2023-11-17T12:15:22.337034Z","shell.execute_reply.started":"2023-11-17T12:15:22.315522Z","shell.execute_reply":"2023-11-17T12:15:22.336031Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Entropy DiceLoss Function","metadata":{}},{"cell_type":"code","source":"class CEDiceLoss(nn.Module):\n    def __init__(self, weights) -> None:\n        super(CEDiceLoss, self).__init__()\n        self.eps: float = 1e-6\n        self.weights: torch.Tensor = weights\n\n    def forward(\n            self,\n            input: torch.Tensor,\n            target: torch.Tensor) -> torch.Tensor:\n        if not torch.is_tensor(input):\n            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n                             .format(input.shape))\n        if not input.shape[-2:] == target.shape[-2:]:\n            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n                             .format(input.shape, input.shape))\n        if not input.device == target.device:\n            raise ValueError(\n                \"input and target must be in the same device. Got: {}\" .format(\n                    input.device, target.device))\n        if not self.weights.shape[1] == input.shape[1]:\n            raise ValueError(\"The number of weights must equal the number of classes\")\n        if not torch.sum(self.weights).item() == 1:\n            raise ValueError(\"The sum of all weights must equal 1\")\n            \n        # cross entropy loss\n        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n        \n        # compute softmax over the classes axis\n        input_soft = F.softmax(input, dim=1)\n\n        # create the labels one hot tensor\n        target_one_hot = one_hot(target, num_classes=input.shape[1],\n                                 device=input.device, dtype=input.dtype)\n\n        # compute the actual dice score\n        dims = (2, 3)\n        intersection = torch.sum(input_soft * target_one_hot, dims)\n        cardinality = torch.sum(input_soft + target_one_hot, dims)\n\n        dice_score = 2. * intersection / (cardinality + self.eps)\n        \n        dice_score = torch.sum(dice_score * self.weights, dim=1)\n        \n        return torch.mean(1. - dice_score) + celoss","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:22.338300Z","iopub.execute_input":"2023-11-17T12:15:22.338605Z","iopub.status.idle":"2023-11-17T12:15:22.351970Z","shell.execute_reply.started":"2023-11-17T12:15:22.338582Z","shell.execute_reply":"2023-11-17T12:15:22.351114Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"# Weight Init","metadata":{}},{"cell_type":"code","source":"def weights_init(model):\n    if isinstance(model, nn.Linear):\n        # Xavier Distribution\n        torch.nn.init.xavier_uniform_(model.weight)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:22.353014Z","iopub.execute_input":"2023-11-17T12:15:22.353366Z","iopub.status.idle":"2023-11-17T12:15:22.365004Z","shell.execute_reply.started":"2023-11-17T12:15:22.353319Z","shell.execute_reply":"2023-11-17T12:15:22.364197Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"# Load and Save Model","metadata":{}},{"cell_type":"code","source":"def save_model(model, optimizer, path):\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, path)\n\ndef load_model(model, optimizer, path):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return model, optimizer","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:22.372791Z","iopub.execute_input":"2023-11-17T12:15:22.373266Z","iopub.status.idle":"2023-11-17T12:15:22.381189Z","shell.execute_reply.started":"2023-11-17T12:15:22.373236Z","shell.execute_reply":"2023-11-17T12:15:22.380367Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"# Trainer Setup","metadata":{}},{"cell_type":"code","source":"# Train function for each epoch\ndef train(train_dataloader, valid_dataloader,learing_rate_scheduler, epoch, display_step):\n    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n    start_time = time.time()\n    train_loss_epoch = 0\n    test_loss_epoch = 0\n    last_loss = 999999999\n    model.train()\n    for i, load in enumerate(train_dataloader):\n        \n        # Load data into GPU\n        data, targets = load['image'].to(device), load['mask'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(data)\n\n        # Backpropagation, compute gradients\n        loss = loss_function(outputs, targets.long())\n        loss.backward()\n\n        # Apply gradients\n        optimizer.step()\n        \n        # Save loss\n        train_loss_epoch += loss.item()\n        if (i+1) % display_step == 0:\n#             accuracy = float(test(test_loader))\n            print('Train Epoch: {} [{}/{} ({}%)]\\tLoss: {:.4f}'.format(\n                epoch + 1, (i+1) * len(data), len(train_dataloader.dataset), 100 * (i+1) * len(data) / len(train_dataloader.dataset), \n                loss.item()))\n                  \n    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n    train_loss_epoch/= (i + 1)\n    \n    # Evaluate the validation set\n    model.eval()\n    with torch.no_grad():\n        for load in valid_dataloader:\n            data, target = load['image'].to(device), load['mask'].to(device)\n            test_output = model(data)\n            test_loss = loss_function(test_output, target.long())\n            test_loss_epoch += test_loss.item()\n            \n    test_loss_epoch/= (i+1)\n    \n    return train_loss_epoch , test_loss_epoch","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:22.382686Z","iopub.execute_input":"2023-11-17T12:15:22.383042Z","iopub.status.idle":"2023-11-17T12:15:22.394628Z","shell.execute_reply.started":"2023-11-17T12:15:22.383011Z","shell.execute_reply":"2023-11-17T12:15:22.393711Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# Test Setup","metadata":{}},{"cell_type":"code","source":"# Test function\ndef test(dataloader):\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for i,load in enumerate(dataloader):\n            data, targets = load['image'].to(device), targets.to(device)\n            outputs = model(data)\n            _, pred = torch.max(outputs, 1)\n            test_loss += targets.size(0)\n            correct += torch.sum(pred == targets).item()\n    return 100.0 * correct / test_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:22.395715Z","iopub.execute_input":"2023-11-17T12:15:22.395997Z","iopub.status.idle":"2023-11-17T12:15:22.408227Z","shell.execute_reply.started":"2023-11-17T12:15:22.395974Z","shell.execute_reply":"2023-11-17T12:15:22.407299Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"# UnetPlusPlus","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n\nimport segmentation_models_pytorch as smp\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet34\",      \n    encoder_weights=\"imagenet\",     \n    in_channels=3,                 \n    classes=3                       \n)\n\nmodel = nn.DataParallel(model)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:22.409310Z","iopub.execute_input":"2023-11-17T12:15:22.409606Z","iopub.status.idle":"2023-11-17T12:15:26.866438Z","shell.execute_reply.started":"2023-11-17T12:15:22.409583Z","shell.execute_reply":"2023-11-17T12:15:26.865551Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 244MB/s] \n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): UnetPlusPlus(\n    (encoder): ResNetEncoder(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (3): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (3): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (4): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (5): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): BasicBlock(\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (decoder): UnetPlusPlusDecoder(\n      (center): Identity()\n      (blocks): ModuleDict(\n        (x_0_0): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_0_1): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_1_1): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_0_2): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_1_2): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_2_2): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_0_3): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_1_3): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_2_3): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_3_3): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (x_0_4): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n      )\n    )\n    (segmentation_head): SegmentationHead(\n      (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): Identity()\n      (2): Activation(\n        (activation): Identity()\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# ResUnet","metadata":{}},{"cell_type":"code","source":"# model = ResNetUNet(n_classes = 3)\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:26.867766Z","iopub.execute_input":"2023-11-17T12:15:26.868063Z","iopub.status.idle":"2023-11-17T12:15:26.872113Z","shell.execute_reply.started":"2023-11-17T12:15:26.868037Z","shell.execute_reply":"2023-11-17T12:15:26.871044Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\nloss_function = CEDiceLoss(weights)\n\n# Define the optimizer (Adam optimizer)\noptimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n#optimizer.load_state_dict(checkpoint['optimizer'])\n\n# Learning rate scheduler\nlearing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:26.886649Z","iopub.execute_input":"2023-11-17T12:15:26.887083Z","iopub.status.idle":"2023-11-17T12:15:26.897276Z","shell.execute_reply.started":"2023-11-17T12:15:26.887034Z","shell.execute_reply":"2023-11-17T12:15:26.896513Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"save_model(model, optimizer, checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:26.898305Z","iopub.execute_input":"2023-11-17T12:15:26.898670Z","iopub.status.idle":"2023-11-17T12:15:27.158854Z","shell.execute_reply.started":"2023-11-17T12:15:26.898640Z","shell.execute_reply":"2023-11-17T12:15:27.157719Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"# Wandb","metadata":{}},{"cell_type":"code","source":"wandb.login(\n    key = \"4184ad0bd316668aed39d30ad508ca2d23ef0479\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:27.159927Z","iopub.execute_input":"2023-11-17T12:15:27.160207Z","iopub.status.idle":"2023-11-17T12:15:29.322100Z","shell.execute_reply.started":"2023-11-17T12:15:27.160185Z","shell.execute_reply":"2023-11-17T12:15:29.321077Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcongquyk66hust\u001b[0m (\u001b[33mhelarica\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(\n    project = \"Polyps_Segmentation_3\"\n)\n# Training loop\ntrain_loss_array = []\ntest_loss_array = []\nlast_loss = 9999999999999\nfor epoch in range(epochs):\n    train_loss_epoch = 0\n    test_loss_epoch = 0\n    (train_loss_epoch, test_loss_epoch) = train(train_loader, \n                                              val_loader, \n                                              learing_rate_scheduler, epoch, display_step)\n    \n    if test_loss_epoch < last_loss:\n        save_model(model, optimizer, checkpoint_path)\n        last_loss = test_loss_epoch\n        \n    learing_rate_scheduler.step()\n    train_loss_array.append(train_loss_epoch)\n    test_loss_array.append(test_loss_epoch)\n    wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n#     train_accuracy.append(test(train_loader))\n#     valid_accuracy.append(test(test_loader))\n#     print(\"Epoch {}: loss: {:.4f}, train accuracy: {:.4f}, valid accuracy:{:.4f}\".format(epoch + 1, \n#                                         train_loss_array[-1], train_accuracy[-1], valid_accuracy[-1]))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T12:15:29.323364Z","iopub.execute_input":"2023-11-17T12:15:29.323875Z","iopub.status.idle":"2023-11-17T13:12:03.275065Z","shell.execute_reply.started":"2023-11-17T12:15:29.323845Z","shell.execute_reply":"2023-11-17T13:12:03.273925Z"},"trusted":true},"execution_count":82,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231117_121529-3n1itiwh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/helarica/Polyps_Segmentation_3/runs/3n1itiwh' target=\"_blank\">ethereal-glitter-10</a></strong> to <a href='https://wandb.ai/helarica/Polyps_Segmentation_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/helarica/Polyps_Segmentation_3' target=\"_blank\">https://wandb.ai/helarica/Polyps_Segmentation_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/helarica/Polyps_Segmentation_3/runs/3n1itiwh' target=\"_blank\">https://wandb.ai/helarica/Polyps_Segmentation_3/runs/3n1itiwh</a>"},"metadata":{}},{"name":"stdout","text":"Start epoch #1, learning rate for this epoch: [0.0002]\nTrain Epoch: 1 [200/1600 (12.5%)]\tLoss: 1.7289\nTrain Epoch: 1 [400/1600 (25.0%)]\tLoss: 1.1752\nTrain Epoch: 1 [600/1600 (37.5%)]\tLoss: 0.9933\nTrain Epoch: 1 [800/1600 (50.0%)]\tLoss: 0.8760\nTrain Epoch: 1 [1000/1600 (62.5%)]\tLoss: 0.6752\nTrain Epoch: 1 [1200/1600 (75.0%)]\tLoss: 0.5751\nTrain Epoch: 1 [1400/1600 (87.5%)]\tLoss: 0.9607\nTrain Epoch: 1 [1600/1600 (100.0%)]\tLoss: 0.3443\nDone epoch #1, time for this epoch: 108.69890308380127s\nStart epoch #2, learning rate for this epoch: [0.0002]\nTrain Epoch: 2 [200/1600 (12.5%)]\tLoss: 0.5093\nTrain Epoch: 2 [400/1600 (25.0%)]\tLoss: 0.2088\nTrain Epoch: 2 [600/1600 (37.5%)]\tLoss: 0.3553\nTrain Epoch: 2 [800/1600 (50.0%)]\tLoss: 0.4119\nTrain Epoch: 2 [1000/1600 (62.5%)]\tLoss: 0.3996\nTrain Epoch: 2 [1200/1600 (75.0%)]\tLoss: 0.2930\nTrain Epoch: 2 [1400/1600 (87.5%)]\tLoss: 0.5428\nTrain Epoch: 2 [1600/1600 (100.0%)]\tLoss: 0.4226\nDone epoch #2, time for this epoch: 101.71913695335388s\nStart epoch #3, learning rate for this epoch: [0.0002]\nTrain Epoch: 3 [200/1600 (12.5%)]\tLoss: 0.3074\nTrain Epoch: 3 [400/1600 (25.0%)]\tLoss: 0.3105\nTrain Epoch: 3 [600/1600 (37.5%)]\tLoss: 0.2423\nTrain Epoch: 3 [800/1600 (50.0%)]\tLoss: 0.5646\nTrain Epoch: 3 [1000/1600 (62.5%)]\tLoss: 0.2140\nTrain Epoch: 3 [1200/1600 (75.0%)]\tLoss: 0.1306\nTrain Epoch: 3 [1400/1600 (87.5%)]\tLoss: 0.7614\nTrain Epoch: 3 [1600/1600 (100.0%)]\tLoss: 0.3180\nDone epoch #3, time for this epoch: 102.13673067092896s\nStart epoch #4, learning rate for this epoch: [0.0002]\nTrain Epoch: 4 [200/1600 (12.5%)]\tLoss: 0.1332\nTrain Epoch: 4 [400/1600 (25.0%)]\tLoss: 0.1890\nTrain Epoch: 4 [600/1600 (37.5%)]\tLoss: 0.1301\nTrain Epoch: 4 [800/1600 (50.0%)]\tLoss: 0.5528\nTrain Epoch: 4 [1000/1600 (62.5%)]\tLoss: 0.2468\nTrain Epoch: 4 [1200/1600 (75.0%)]\tLoss: 0.1269\nTrain Epoch: 4 [1400/1600 (87.5%)]\tLoss: 0.6608\nTrain Epoch: 4 [1600/1600 (100.0%)]\tLoss: 0.2171\nDone epoch #4, time for this epoch: 102.11297845840454s\nStart epoch #5, learning rate for this epoch: [0.00012]\nTrain Epoch: 5 [200/1600 (12.5%)]\tLoss: 0.3928\nTrain Epoch: 5 [400/1600 (25.0%)]\tLoss: 0.5651\nTrain Epoch: 5 [600/1600 (37.5%)]\tLoss: 0.4606\nTrain Epoch: 5 [800/1600 (50.0%)]\tLoss: 0.5619\nTrain Epoch: 5 [1000/1600 (62.5%)]\tLoss: 0.2380\nTrain Epoch: 5 [1200/1600 (75.0%)]\tLoss: 0.3532\nTrain Epoch: 5 [1400/1600 (87.5%)]\tLoss: 0.2746\nTrain Epoch: 5 [1600/1600 (100.0%)]\tLoss: 0.3182\nDone epoch #5, time for this epoch: 102.25331020355225s\nStart epoch #6, learning rate for this epoch: [0.00012]\nTrain Epoch: 6 [200/1600 (12.5%)]\tLoss: 0.4161\nTrain Epoch: 6 [400/1600 (25.0%)]\tLoss: 0.2287\nTrain Epoch: 6 [600/1600 (37.5%)]\tLoss: 0.0988\nTrain Epoch: 6 [800/1600 (50.0%)]\tLoss: 0.2303\nTrain Epoch: 6 [1000/1600 (62.5%)]\tLoss: 0.3122\nTrain Epoch: 6 [1200/1600 (75.0%)]\tLoss: 0.4704\nTrain Epoch: 6 [1400/1600 (87.5%)]\tLoss: 0.4952\nTrain Epoch: 6 [1600/1600 (100.0%)]\tLoss: 0.1437\nDone epoch #6, time for this epoch: 102.31994533538818s\nStart epoch #7, learning rate for this epoch: [0.00012]\nTrain Epoch: 7 [200/1600 (12.5%)]\tLoss: 0.3354\nTrain Epoch: 7 [400/1600 (25.0%)]\tLoss: 0.4743\nTrain Epoch: 7 [600/1600 (37.5%)]\tLoss: 0.1647\nTrain Epoch: 7 [800/1600 (50.0%)]\tLoss: 0.6103\nTrain Epoch: 7 [1000/1600 (62.5%)]\tLoss: 0.3378\nTrain Epoch: 7 [1200/1600 (75.0%)]\tLoss: 0.4561\nTrain Epoch: 7 [1400/1600 (87.5%)]\tLoss: 0.3667\nTrain Epoch: 7 [1600/1600 (100.0%)]\tLoss: 0.2321\nDone epoch #7, time for this epoch: 102.08709979057312s\nStart epoch #8, learning rate for this epoch: [0.00012]\nTrain Epoch: 8 [200/1600 (12.5%)]\tLoss: 0.2231\nTrain Epoch: 8 [400/1600 (25.0%)]\tLoss: 0.1050\nTrain Epoch: 8 [600/1600 (37.5%)]\tLoss: 0.5275\nTrain Epoch: 8 [800/1600 (50.0%)]\tLoss: 0.3679\nTrain Epoch: 8 [1000/1600 (62.5%)]\tLoss: 0.3654\nTrain Epoch: 8 [1200/1600 (75.0%)]\tLoss: 0.3535\nTrain Epoch: 8 [1400/1600 (87.5%)]\tLoss: 0.3769\nTrain Epoch: 8 [1600/1600 (100.0%)]\tLoss: 0.1546\nDone epoch #8, time for this epoch: 101.92079329490662s\nStart epoch #9, learning rate for this epoch: [7.2e-05]\nTrain Epoch: 9 [200/1600 (12.5%)]\tLoss: 0.2304\nTrain Epoch: 9 [400/1600 (25.0%)]\tLoss: 0.0998\nTrain Epoch: 9 [600/1600 (37.5%)]\tLoss: 0.0953\nTrain Epoch: 9 [800/1600 (50.0%)]\tLoss: 0.4313\nTrain Epoch: 9 [1000/1600 (62.5%)]\tLoss: 0.1338\nTrain Epoch: 9 [1200/1600 (75.0%)]\tLoss: 0.1814\nTrain Epoch: 9 [1400/1600 (87.5%)]\tLoss: 0.2341\nTrain Epoch: 9 [1600/1600 (100.0%)]\tLoss: 0.3861\nDone epoch #9, time for this epoch: 101.99356603622437s\nStart epoch #10, learning rate for this epoch: [7.2e-05]\nTrain Epoch: 10 [200/1600 (12.5%)]\tLoss: 0.0934\nTrain Epoch: 10 [400/1600 (25.0%)]\tLoss: 0.4227\nTrain Epoch: 10 [600/1600 (37.5%)]\tLoss: 0.2295\nTrain Epoch: 10 [800/1600 (50.0%)]\tLoss: 0.2980\nTrain Epoch: 10 [1000/1600 (62.5%)]\tLoss: 0.1104\nTrain Epoch: 10 [1200/1600 (75.0%)]\tLoss: 0.0856\nTrain Epoch: 10 [1400/1600 (87.5%)]\tLoss: 0.3074\nTrain Epoch: 10 [1600/1600 (100.0%)]\tLoss: 0.3690\nDone epoch #10, time for this epoch: 102.00318050384521s\nStart epoch #11, learning rate for this epoch: [7.2e-05]\nTrain Epoch: 11 [200/1600 (12.5%)]\tLoss: 0.3452\nTrain Epoch: 11 [400/1600 (25.0%)]\tLoss: 0.3930\nTrain Epoch: 11 [600/1600 (37.5%)]\tLoss: 0.2235\nTrain Epoch: 11 [800/1600 (50.0%)]\tLoss: 0.2256\nTrain Epoch: 11 [1000/1600 (62.5%)]\tLoss: 0.2536\nTrain Epoch: 11 [1200/1600 (75.0%)]\tLoss: 0.2241\nTrain Epoch: 11 [1400/1600 (87.5%)]\tLoss: 0.2202\nTrain Epoch: 11 [1600/1600 (100.0%)]\tLoss: 0.0935\nDone epoch #11, time for this epoch: 101.95463109016418s\nStart epoch #12, learning rate for this epoch: [7.2e-05]\nTrain Epoch: 12 [200/1600 (12.5%)]\tLoss: 0.4875\nTrain Epoch: 12 [400/1600 (25.0%)]\tLoss: 0.0831\nTrain Epoch: 12 [600/1600 (37.5%)]\tLoss: 0.3513\nTrain Epoch: 12 [800/1600 (50.0%)]\tLoss: 0.2218\nTrain Epoch: 12 [1000/1600 (62.5%)]\tLoss: 0.5067\nTrain Epoch: 12 [1200/1600 (75.0%)]\tLoss: 0.2509\nTrain Epoch: 12 [1400/1600 (87.5%)]\tLoss: 0.4991\nTrain Epoch: 12 [1600/1600 (100.0%)]\tLoss: 0.2166\nDone epoch #12, time for this epoch: 102.00957226753235s\nStart epoch #13, learning rate for this epoch: [4.32e-05]\nTrain Epoch: 13 [200/1600 (12.5%)]\tLoss: 0.2940\nTrain Epoch: 13 [400/1600 (25.0%)]\tLoss: 0.2146\nTrain Epoch: 13 [600/1600 (37.5%)]\tLoss: 0.0915\nTrain Epoch: 13 [800/1600 (50.0%)]\tLoss: 0.2271\nTrain Epoch: 13 [1000/1600 (62.5%)]\tLoss: 0.2379\nTrain Epoch: 13 [1200/1600 (75.0%)]\tLoss: 0.2167\nTrain Epoch: 13 [1400/1600 (87.5%)]\tLoss: 0.2353\nTrain Epoch: 13 [1600/1600 (100.0%)]\tLoss: 0.2180\nDone epoch #13, time for this epoch: 102.5276529788971s\nStart epoch #14, learning rate for this epoch: [4.32e-05]\nTrain Epoch: 14 [200/1600 (12.5%)]\tLoss: 0.2222\nTrain Epoch: 14 [400/1600 (25.0%)]\tLoss: 0.2964\nTrain Epoch: 14 [600/1600 (37.5%)]\tLoss: 0.1126\nTrain Epoch: 14 [800/1600 (50.0%)]\tLoss: 0.1006\nTrain Epoch: 14 [1000/1600 (62.5%)]\tLoss: 0.1173\nTrain Epoch: 14 [1200/1600 (75.0%)]\tLoss: 0.0991\nTrain Epoch: 14 [1400/1600 (87.5%)]\tLoss: 0.3633\nTrain Epoch: 14 [1600/1600 (100.0%)]\tLoss: 0.2685\nDone epoch #14, time for this epoch: 102.36670279502869s\nStart epoch #15, learning rate for this epoch: [4.32e-05]\nTrain Epoch: 15 [200/1600 (12.5%)]\tLoss: 0.3521\nTrain Epoch: 15 [400/1600 (25.0%)]\tLoss: 0.3611\nTrain Epoch: 15 [600/1600 (37.5%)]\tLoss: 0.1485\nTrain Epoch: 15 [800/1600 (50.0%)]\tLoss: 0.3734\nTrain Epoch: 15 [1000/1600 (62.5%)]\tLoss: 0.2044\nTrain Epoch: 15 [1200/1600 (75.0%)]\tLoss: 0.3903\nTrain Epoch: 15 [1400/1600 (87.5%)]\tLoss: 0.2789\nTrain Epoch: 15 [1600/1600 (100.0%)]\tLoss: 0.2249\nDone epoch #15, time for this epoch: 101.57532238960266s\nStart epoch #16, learning rate for this epoch: [4.32e-05]\nTrain Epoch: 16 [200/1600 (12.5%)]\tLoss: 0.1021\nTrain Epoch: 16 [400/1600 (25.0%)]\tLoss: 0.2467\nTrain Epoch: 16 [600/1600 (37.5%)]\tLoss: 0.3668\nTrain Epoch: 16 [800/1600 (50.0%)]\tLoss: 0.1491\nTrain Epoch: 16 [1000/1600 (62.5%)]\tLoss: 0.3399\nTrain Epoch: 16 [1200/1600 (75.0%)]\tLoss: 0.1111\nTrain Epoch: 16 [1400/1600 (87.5%)]\tLoss: 0.3700\nTrain Epoch: 16 [1600/1600 (100.0%)]\tLoss: 0.2266\nDone epoch #16, time for this epoch: 102.06940603256226s\nStart epoch #17, learning rate for this epoch: [2.592e-05]\nTrain Epoch: 17 [200/1600 (12.5%)]\tLoss: 0.4374\nTrain Epoch: 17 [400/1600 (25.0%)]\tLoss: 0.2116\nTrain Epoch: 17 [600/1600 (37.5%)]\tLoss: 0.3605\nTrain Epoch: 17 [800/1600 (50.0%)]\tLoss: 0.2296\nTrain Epoch: 17 [1000/1600 (62.5%)]\tLoss: 0.2784\nTrain Epoch: 17 [1200/1600 (75.0%)]\tLoss: 0.2258\nTrain Epoch: 17 [1400/1600 (87.5%)]\tLoss: 0.0933\nTrain Epoch: 17 [1600/1600 (100.0%)]\tLoss: 0.2082\nDone epoch #17, time for this epoch: 102.4560215473175s\nStart epoch #18, learning rate for this epoch: [2.592e-05]\nTrain Epoch: 18 [200/1600 (12.5%)]\tLoss: 0.3422\nTrain Epoch: 18 [400/1600 (25.0%)]\tLoss: 0.3558\nTrain Epoch: 18 [600/1600 (37.5%)]\tLoss: 0.2031\nTrain Epoch: 18 [800/1600 (50.0%)]\tLoss: 0.3676\nTrain Epoch: 18 [1000/1600 (62.5%)]\tLoss: 0.2609\nTrain Epoch: 18 [1200/1600 (75.0%)]\tLoss: 0.2245\nTrain Epoch: 18 [1400/1600 (87.5%)]\tLoss: 0.2580\nTrain Epoch: 18 [1600/1600 (100.0%)]\tLoss: 0.3833\nDone epoch #18, time for this epoch: 102.68612790107727s\nStart epoch #19, learning rate for this epoch: [2.592e-05]\nTrain Epoch: 19 [200/1600 (12.5%)]\tLoss: 0.0755\nTrain Epoch: 19 [400/1600 (25.0%)]\tLoss: 0.2423\nTrain Epoch: 19 [600/1600 (37.5%)]\tLoss: 0.0774\nTrain Epoch: 19 [800/1600 (50.0%)]\tLoss: 0.2175\nTrain Epoch: 19 [1000/1600 (62.5%)]\tLoss: 0.0945\nTrain Epoch: 19 [1200/1600 (75.0%)]\tLoss: 0.2497\nTrain Epoch: 19 [1400/1600 (87.5%)]\tLoss: 0.0828\nTrain Epoch: 19 [1600/1600 (100.0%)]\tLoss: 0.2144\nDone epoch #19, time for this epoch: 103.34476828575134s\nStart epoch #20, learning rate for this epoch: [2.592e-05]\nTrain Epoch: 20 [200/1600 (12.5%)]\tLoss: 0.0813\nTrain Epoch: 20 [400/1600 (25.0%)]\tLoss: 0.3611\nTrain Epoch: 20 [600/1600 (37.5%)]\tLoss: 0.0772\nTrain Epoch: 20 [800/1600 (50.0%)]\tLoss: 0.2057\nTrain Epoch: 20 [1000/1600 (62.5%)]\tLoss: 0.0900\nTrain Epoch: 20 [1200/1600 (75.0%)]\tLoss: 0.3424\nTrain Epoch: 20 [1400/1600 (87.5%)]\tLoss: 0.1153\nTrain Epoch: 20 [1600/1600 (100.0%)]\tLoss: 0.4286\nDone epoch #20, time for this epoch: 102.23634314537048s\nStart epoch #21, learning rate for this epoch: [1.5552e-05]\nTrain Epoch: 21 [200/1600 (12.5%)]\tLoss: 0.2232\nTrain Epoch: 21 [400/1600 (25.0%)]\tLoss: 0.4917\nTrain Epoch: 21 [600/1600 (37.5%)]\tLoss: 0.2165\nTrain Epoch: 21 [800/1600 (50.0%)]\tLoss: 0.1063\nTrain Epoch: 21 [1000/1600 (62.5%)]\tLoss: 0.4666\nTrain Epoch: 21 [1200/1600 (75.0%)]\tLoss: 0.3398\nTrain Epoch: 21 [1400/1600 (87.5%)]\tLoss: 0.2080\nTrain Epoch: 21 [1600/1600 (100.0%)]\tLoss: 0.4703\nDone epoch #21, time for this epoch: 102.26530075073242s\nStart epoch #22, learning rate for this epoch: [1.5552e-05]\nTrain Epoch: 22 [200/1600 (12.5%)]\tLoss: 0.4223\nTrain Epoch: 22 [400/1600 (25.0%)]\tLoss: 0.0700\nTrain Epoch: 22 [600/1600 (37.5%)]\tLoss: 0.2140\nTrain Epoch: 22 [800/1600 (50.0%)]\tLoss: 0.2029\nTrain Epoch: 22 [1000/1600 (62.5%)]\tLoss: 0.4790\nTrain Epoch: 22 [1200/1600 (75.0%)]\tLoss: 0.1022\nTrain Epoch: 22 [1400/1600 (87.5%)]\tLoss: 0.2298\nTrain Epoch: 22 [1600/1600 (100.0%)]\tLoss: 0.0854\nDone epoch #22, time for this epoch: 102.7163290977478s\nStart epoch #23, learning rate for this epoch: [1.5552e-05]\nTrain Epoch: 23 [200/1600 (12.5%)]\tLoss: 0.2200\nTrain Epoch: 23 [400/1600 (25.0%)]\tLoss: 0.2491\nTrain Epoch: 23 [600/1600 (37.5%)]\tLoss: 0.0950\nTrain Epoch: 23 [800/1600 (50.0%)]\tLoss: 0.0834\nTrain Epoch: 23 [1000/1600 (62.5%)]\tLoss: 0.1382\nTrain Epoch: 23 [1200/1600 (75.0%)]\tLoss: 0.1611\nTrain Epoch: 23 [1400/1600 (87.5%)]\tLoss: 0.3547\nTrain Epoch: 23 [1600/1600 (100.0%)]\tLoss: 0.1744\nDone epoch #23, time for this epoch: 102.4271023273468s\nStart epoch #24, learning rate for this epoch: [1.5552e-05]\nTrain Epoch: 24 [200/1600 (12.5%)]\tLoss: 0.3789\nTrain Epoch: 24 [400/1600 (25.0%)]\tLoss: 0.3671\nTrain Epoch: 24 [600/1600 (37.5%)]\tLoss: 0.0761\nTrain Epoch: 24 [800/1600 (50.0%)]\tLoss: 0.3458\nTrain Epoch: 24 [1000/1600 (62.5%)]\tLoss: 0.2173\nTrain Epoch: 24 [1200/1600 (75.0%)]\tLoss: 0.0855\nTrain Epoch: 24 [1400/1600 (87.5%)]\tLoss: 0.3438\nTrain Epoch: 24 [1600/1600 (100.0%)]\tLoss: 0.1189\nDone epoch #24, time for this epoch: 102.83337998390198s\nStart epoch #25, learning rate for this epoch: [9.3312e-06]\nTrain Epoch: 25 [200/1600 (12.5%)]\tLoss: 0.0848\nTrain Epoch: 25 [400/1600 (25.0%)]\tLoss: 0.2116\nTrain Epoch: 25 [600/1600 (37.5%)]\tLoss: 0.0847\nTrain Epoch: 25 [800/1600 (50.0%)]\tLoss: 0.0847\nTrain Epoch: 25 [1000/1600 (62.5%)]\tLoss: 0.1077\nTrain Epoch: 25 [1200/1600 (75.0%)]\tLoss: 0.3744\nTrain Epoch: 25 [1400/1600 (87.5%)]\tLoss: 0.1317\nTrain Epoch: 25 [1600/1600 (100.0%)]\tLoss: 0.1565\nDone epoch #25, time for this epoch: 102.21505284309387s\nStart epoch #26, learning rate for this epoch: [9.3312e-06]\nTrain Epoch: 26 [200/1600 (12.5%)]\tLoss: 0.3328\nTrain Epoch: 26 [400/1600 (25.0%)]\tLoss: 0.1156\nTrain Epoch: 26 [600/1600 (37.5%)]\tLoss: 0.0795\nTrain Epoch: 26 [800/1600 (50.0%)]\tLoss: 0.2127\nTrain Epoch: 26 [1000/1600 (62.5%)]\tLoss: 0.1767\nTrain Epoch: 26 [1200/1600 (75.0%)]\tLoss: 0.2669\nTrain Epoch: 26 [1400/1600 (87.5%)]\tLoss: 0.3324\nTrain Epoch: 26 [1600/1600 (100.0%)]\tLoss: 0.2058\nDone epoch #26, time for this epoch: 101.33375597000122s\nStart epoch #27, learning rate for this epoch: [9.3312e-06]\nTrain Epoch: 27 [200/1600 (12.5%)]\tLoss: 0.0983\nTrain Epoch: 27 [400/1600 (25.0%)]\tLoss: 0.0788\nTrain Epoch: 27 [600/1600 (37.5%)]\tLoss: 0.0742\nTrain Epoch: 27 [800/1600 (50.0%)]\tLoss: 0.2150\nTrain Epoch: 27 [1000/1600 (62.5%)]\tLoss: 0.4724\nTrain Epoch: 27 [1200/1600 (75.0%)]\tLoss: 0.0766\nTrain Epoch: 27 [1400/1600 (87.5%)]\tLoss: 0.3277\nTrain Epoch: 27 [1600/1600 (100.0%)]\tLoss: 0.4773\nDone epoch #27, time for this epoch: 101.28236746788025s\nStart epoch #28, learning rate for this epoch: [9.3312e-06]\nTrain Epoch: 28 [200/1600 (12.5%)]\tLoss: 0.2118\nTrain Epoch: 28 [400/1600 (25.0%)]\tLoss: 0.2263\nTrain Epoch: 28 [600/1600 (37.5%)]\tLoss: 0.3372\nTrain Epoch: 28 [800/1600 (50.0%)]\tLoss: 0.3430\nTrain Epoch: 28 [1000/1600 (62.5%)]\tLoss: 0.2011\nTrain Epoch: 28 [1200/1600 (75.0%)]\tLoss: 0.3207\nTrain Epoch: 28 [1400/1600 (87.5%)]\tLoss: 0.5222\nTrain Epoch: 28 [1600/1600 (100.0%)]\tLoss: 0.2148\nDone epoch #28, time for this epoch: 101.7602767944336s\nStart epoch #29, learning rate for this epoch: [5.59872e-06]\nTrain Epoch: 29 [200/1600 (12.5%)]\tLoss: 0.2092\nTrain Epoch: 29 [400/1600 (25.0%)]\tLoss: 0.3431\nTrain Epoch: 29 [600/1600 (37.5%)]\tLoss: 0.0719\nTrain Epoch: 29 [800/1600 (50.0%)]\tLoss: 0.3370\nTrain Epoch: 29 [1000/1600 (62.5%)]\tLoss: 0.3659\nTrain Epoch: 29 [1200/1600 (75.0%)]\tLoss: 0.3192\nTrain Epoch: 29 [1400/1600 (87.5%)]\tLoss: 0.2186\nTrain Epoch: 29 [1600/1600 (100.0%)]\tLoss: 0.2084\nDone epoch #29, time for this epoch: 101.76941466331482s\nStart epoch #30, learning rate for this epoch: [5.59872e-06]\nTrain Epoch: 30 [200/1600 (12.5%)]\tLoss: 0.2409\nTrain Epoch: 30 [400/1600 (25.0%)]\tLoss: 0.1963\nTrain Epoch: 30 [600/1600 (37.5%)]\tLoss: 0.0824\nTrain Epoch: 30 [800/1600 (50.0%)]\tLoss: 0.0984\nTrain Epoch: 30 [1000/1600 (62.5%)]\tLoss: 0.0858\nTrain Epoch: 30 [1200/1600 (75.0%)]\tLoss: 0.2303\nTrain Epoch: 30 [1400/1600 (87.5%)]\tLoss: 0.2143\nTrain Epoch: 30 [1600/1600 (100.0%)]\tLoss: 0.1232\nDone epoch #30, time for this epoch: 101.71524047851562s\n","output_type":"stream"}]},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.276633Z","iopub.execute_input":"2023-11-17T13:12:03.277049Z","iopub.status.idle":"2023-11-17T13:12:03.282814Z","shell.execute_reply.started":"2023-11-17T13:12:03.277013Z","shell.execute_reply":"2023-11-17T13:12:03.281582Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# load_model(model, checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.284607Z","iopub.execute_input":"2023-11-17T13:12:03.284962Z","iopub.status.idle":"2023-11-17T13:12:03.297602Z","shell.execute_reply.started":"2023-11-17T13:12:03.284928Z","shell.execute_reply":"2023-11-17T13:12:03.296578Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 90\nplt.rcParams['figure.figsize'] = (6, 4)\nepochs_array = range(epochs)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.298910Z","iopub.execute_input":"2023-11-17T13:12:03.299387Z","iopub.status.idle":"2023-11-17T13:12:03.308892Z","shell.execute_reply.started":"2023-11-17T13:12:03.299332Z","shell.execute_reply":"2023-11-17T13:12:03.307780Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# Plot Training and Test loss\nplt.plot(epochs_array, train_loss_array, 'g', label='Training loss')\n# plt.plot(epochs_array, test_loss_array, 'b', label='Test loss')\nplt.title('Training and Test loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.310262Z","iopub.execute_input":"2023-11-17T13:12:03.310860Z","iopub.status.idle":"2023-11-17T13:12:03.644981Z","shell.execute_reply.started":"2023-11-17T13:12:03.310809Z","shell.execute_reply":"2023-11-17T13:12:03.643805Z"},"trusted":true},"execution_count":86,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 540x360 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# from torch.jit import load\n# model = UNet()\n# optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n\n# checkpoint = torch.load(pretrained_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.646483Z","iopub.execute_input":"2023-11-17T13:12:03.647283Z","iopub.status.idle":"2023-11-17T13:12:03.652083Z","shell.execute_reply.started":"2023-11-17T13:12:03.647251Z","shell.execute_reply":"2023-11-17T13:12:03.651069Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# optimizer.load_state_dict(checkpoint['optimizer'])","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.653460Z","iopub.execute_input":"2023-11-17T13:12:03.653794Z","iopub.status.idle":"2023-11-17T13:12:03.662056Z","shell.execute_reply.started":"2023-11-17T13:12:03.653765Z","shell.execute_reply":"2023-11-17T13:12:03.661144Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# from collections import OrderedDict\n# new_state_dict = OrderedDict()\n# for k, v in checkpoint['model'].items():\n#     name = k[7:] # remove `module.`\n#     new_state_dict[name] = v\n# # load params\n# model.load_state_dict(new_state_dict)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.663186Z","iopub.execute_input":"2023-11-17T13:12:03.663469Z","iopub.status.idle":"2023-11-17T13:12:03.671864Z","shell.execute_reply.started":"2023-11-17T13:12:03.663445Z","shell.execute_reply":"2023-11-17T13:12:03.670825Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"for i, load in enumerate(train_loader):\n    img = load['image']\n    mask = load['mask']\n    break","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.673081Z","iopub.execute_input":"2023-11-17T13:12:03.673464Z","iopub.status.idle":"2023-11-17T13:12:03.817751Z","shell.execute_reply.started":"2023-11-17T13:12:03.673430Z","shell.execute_reply":"2023-11-17T13:12:03.816536Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"test_path = []\ntests_path = \"/kaggle/input/bkai-igh-neopolyp/test/test/\"\nfor root, dirs, files in os.walk(tests_path):\n    for file in files:\n        path = os.path.join(root,file)\n        test_path.append(path)\nlen(test_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.819238Z","iopub.execute_input":"2023-11-17T13:12:03.819625Z","iopub.status.idle":"2023-11-17T13:12:03.833758Z","shell.execute_reply.started":"2023-11-17T13:12:03.819591Z","shell.execute_reply":"2023-11-17T13:12:03.832411Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"200"},"metadata":{}}]},{"cell_type":"code","source":"unet_test_dataset = UnetDataClass(test_path, normal_transform, mode = \"test\")\n# unet_test_dataset = UNetTestDataClass(test_path, mode = \"test\")\ntest_dataloader = DataLoader(unet_test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:12:03.835077Z","iopub.execute_input":"2023-11-17T13:12:03.835375Z","iopub.status.idle":"2023-11-17T13:12:03.843827Z","shell.execute_reply.started":"2023-11-17T13:12:03.835327Z","shell.execute_reply":"2023-11-17T13:12:03.842775Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"unet_test_dataset.__getitem__(7)[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:15:09.290585Z","iopub.execute_input":"2023-11-17T13:15:09.291278Z","iopub.status.idle":"2023-11-17T13:15:09.321735Z","shell.execute_reply.started":"2023-11-17T13:15:09.291244Z","shell.execute_reply":"2023-11-17T13:15:09.320734Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"{'image': tensor([[[-2.0837, -2.1179, -2.0152,  ..., -2.1179, -2.1179, -2.1179],\n          [-2.1008, -2.0837, -1.8097,  ..., -2.1179, -2.1179, -2.1179],\n          [-2.0837,  0.1083, -2.0665,  ..., -2.1179, -2.1179, -2.1179],\n          ...,\n          [-1.5357, -1.5357, -1.5357,  ..., -1.2445, -1.5699, -1.5699],\n          [-1.5528, -1.5528, -1.5528,  ...,  1.7352, -1.5870, -1.5870],\n          [-1.5014, -1.5014, -1.5699,  ..., -1.5357, -1.5699, -1.5699]],\n \n         [[-2.0007, -2.0357, -1.9307,  ..., -2.0357, -2.0357, -2.0357],\n          [-2.0182, -2.0007, -1.7206,  ..., -2.0357, -2.0357, -2.0357],\n          [-2.0007,  0.2402, -1.9832,  ..., -2.0357, -2.0357, -2.0357],\n          ...,\n          [-1.4230, -1.4230, -1.4230,  ..., -1.1078, -1.4405, -1.4405],\n          [-1.4580, -1.4580, -1.4580,  ...,  1.9209, -1.4580, -1.4580],\n          [-1.4055, -1.4055, -1.4755,  ..., -1.4580, -1.4755, -1.4755]],\n \n         [[-1.7696, -1.8044, -1.6999,  ..., -1.8044, -1.8044, -1.8044],\n          [-1.7870, -1.7696, -1.4907,  ..., -1.8044, -1.8044, -1.8044],\n          [-1.7696,  0.4614, -1.7522,  ..., -1.8044, -1.8044, -1.8044],\n          ...,\n          [-1.4733, -1.4733, -1.4733,  ..., -1.1073, -1.4210, -1.4210],\n          [-1.4733, -1.4733, -1.4733,  ...,  1.9428, -1.4384, -1.4384],\n          [-1.4210, -1.4210, -1.4907,  ..., -1.4733, -1.4907, -1.4907]]])}"},"metadata":{}}]},{"cell_type":"code","source":"for i, (data, path, h, w) in enumerate(test_dataloader):\n    img = data['image']\n    break","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:19:05.310440Z","iopub.execute_input":"2023-11-17T13:19:05.311210Z","iopub.status.idle":"2023-11-17T13:19:05.469808Z","shell.execute_reply.started":"2023-11-17T13:19:05.311178Z","shell.execute_reply":"2023-11-17T13:19:05.468396Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"model.eval()\ndef mask2rgb(mask):\n    color_dict = {0: torch.tensor([0, 0, 0]),\n                  1: torch.tensor([1, 0, 0]),\n                  2: torch.tensor([0, 1, 0])}\n    output = torch.zeros((mask.shape[0], mask.shape[1], 3)).long()\n    for k in color_dict.keys():\n        output[mask.long() == k] = color_dict[k]\n    return output.to(mask.device)\n\nif not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n    os.mkdir(\"/kaggle/working/predicted_masks\")\nfor _, (img, path, H, W) in enumerate(test_dataloader):\n    a = path\n    b = img['image']\n    h = H\n    w = W\n    \n    with torch.no_grad():\n        predicted_mask = model(b)\n    for i in range(len(a)):\n        image_id = a[i].split('/')[-1].split('.')[0]\n        filename = image_id + \".png\"\n        argmax = torch.argmax(predicted_mask[i], 0)\n        one_hot = mask2rgb(argmax).float().permute(2, 0, 1)\n        mask2img = Resize((H[i].item(), W[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(one_hot))\n#         mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:19:08.186365Z","iopub.execute_input":"2023-11-17T13:19:08.186741Z","iopub.status.idle":"2023-11-17T13:19:22.686550Z","shell.execute_reply.started":"2023-11-17T13:19:08.186711Z","shell.execute_reply":"2023-11-17T13:19:22.685447Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 0] = 255\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef mask2string(dir):\n    ## mask --> string\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\ndf.to_csv(r'submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T13:19:39.482400Z","iopub.execute_input":"2023-11-17T13:19:39.483233Z","iopub.status.idle":"2023-11-17T13:19:42.141847Z","shell.execute_reply.started":"2023-11-17T13:19:39.483184Z","shell.execute_reply":"2023-11-17T13:19:42.140807Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"/kaggle/working/predicted_masks/5664c1711b62f15ec83b97bb11e8e0c4.png\n/kaggle/working/predicted_masks/df8e26031fbb5e52c41545ba55aadaa7.png\n/kaggle/working/predicted_masks/c41545ba55aadaa77712a48e11d579d9.png\n/kaggle/working/predicted_masks/e4a17af18f72c8e6166a915669c99390.png\n/kaggle/working/predicted_masks/2d9e593b6be1ac29adbe86f03d900fd1.png\n/kaggle/working/predicted_masks/7936140a2d5fc1443c4e445927738677.png\n/kaggle/working/predicted_masks/692195f853af7f8a4df1ec859759b7c8.png\n/kaggle/working/predicted_masks/be86f03d900fd197cd955fa095f97845.png\n/kaggle/working/predicted_masks/41ed86e58224cb76a67d4dcf9596154e.png\n/kaggle/working/predicted_masks/936de314f2d95e6c487ffa651b477422.png\n/kaggle/working/predicted_masks/05b78a91391adc0bb223c4eaf3372eae.png\n/kaggle/working/predicted_masks/e2cd066b9fdbc3bbc04a3afe1f119f21.png\n/kaggle/working/predicted_masks/2a365b5574868eb60861ee1ff0b8a4f6.png\n/kaggle/working/predicted_masks/63b8318ecf467d7ad048df39beb17636.png\n/kaggle/working/predicted_masks/82ea2c193ac8d551c149b60f2965341c.png\n/kaggle/working/predicted_masks/d077bad31c8c5f54ffaa27a623511c38.png\n/kaggle/working/predicted_masks/3bbc04a3afe1f119f21b248d152b672a.png\n/kaggle/working/predicted_masks/710d568df17586ad8f3297c819c90895.png\n/kaggle/working/predicted_masks/cb1b387133b51209db6dcdda5cc8a788.png\n/kaggle/working/predicted_masks/6ad1468996b4a9ce6d840b53a6558038.png\n/kaggle/working/predicted_masks/fb905b78a91391adc0bb223c4eaf3372.png\n/kaggle/working/predicted_masks/d3694abb47953b0e4909384b57bb6a05.png\n/kaggle/working/predicted_masks/4ca6160127cd1d5ff99c267599fc487b.png\n/kaggle/working/predicted_masks/13dd311a65d2b46d0a6085835c525af6.png\n/kaggle/working/predicted_masks/72d9e593b6be1ac29adbe86f03d900fd.png\n/kaggle/working/predicted_masks/1002ec4a1fe748f3085f1ce88cbdf366.png\n/kaggle/working/predicted_masks/7fda8019410b1fcf0625f608b4ce9762.png\n/kaggle/working/predicted_masks/8cbdf366e057db382b8564872a27301a.png\n/kaggle/working/predicted_masks/a51625559c7e610b1531871f2fd85a04.png\n/kaggle/working/predicted_masks/425b976973f13dd311a65d2b46d0a608.png\n/kaggle/working/predicted_masks/6f67b5df7cdf3f33c3ca4d5060a633a8.png\n/kaggle/working/predicted_masks/6679bff55177a34fc01019eec999fd84.png\n/kaggle/working/predicted_masks/ff55177a34fc01019eec999fd84e679b.png\n/kaggle/working/predicted_masks/e7998934d417cb2eb1ef57af2ed9fbb6.png\n/kaggle/working/predicted_masks/6d3694abb47953b0e4909384b57bb6a0.png\n/kaggle/working/predicted_masks/e8bfb905b78a91391adc0bb223c4eaf3.png\n/kaggle/working/predicted_masks/77e004e8bfb905b78a91391adc0bb223.png\n/kaggle/working/predicted_masks/a6d9ba9d45c3dbc695325ded465efde9.png\n/kaggle/working/predicted_masks/a3657e4314fe384eb2ba3adfda6c1899.png\n/kaggle/working/predicted_masks/4417fda8019410b1fcf0625f608b4ce9.png\n/kaggle/working/predicted_masks/0619ebebe9e9c9d00a4262b4fe4a5a95.png\n/kaggle/working/predicted_masks/4baddc22268d4b4ef4d95ceea1195799.png\n/kaggle/working/predicted_masks/87133b51209db6dcdda5cc8a788edaeb.png\n/kaggle/working/predicted_masks/1b62f15ec83b97bb11e8e0c4416c1931.png\n/kaggle/working/predicted_masks/318ecf467d7ad048df39beb176363408.png\n/kaggle/working/predicted_masks/c656702fa602bb3c7abacdbd7e6afd56.png\n/kaggle/working/predicted_masks/94a7f32574d6c748c41743c6c08a1d1a.png\n/kaggle/working/predicted_masks/314fe384eb2ba3adfda6c1899fdc9837.png\n/kaggle/working/predicted_masks/e5e8f14e1e0ae936de314f2d95e6c487.png\n/kaggle/working/predicted_masks/285e26c90e1797c77826f9a7021bab9f.png\n/kaggle/working/predicted_masks/8954bb13d3727c7e5e1069646f2f0bb8.png\n/kaggle/working/predicted_masks/d6240619ebebe9e9c9d00a4262b4fe4a.png\n/kaggle/working/predicted_masks/4f437f0019f7e6af7d7147763bdfb928.png\n/kaggle/working/predicted_masks/461c2a337948a41964c1d4f50a5f3601.png\n/kaggle/working/predicted_masks/39d6aad6bb0170a40ed32deef71fbe08.png\n/kaggle/working/predicted_masks/71f2fd85a04faeeb2b535797395305af.png\n/kaggle/working/predicted_masks/7cb2eb1ef57af2ed9fbb63b28163a745.png\n/kaggle/working/predicted_masks/7b5df7cdf3f33c3ca4d5060a633a8d5b.png\n/kaggle/working/predicted_masks/60b246359c68c836f843dcf41f4dce3c.png\n/kaggle/working/predicted_masks/9c7976c1182df0de51d32128c358d1fd.png\n/kaggle/working/predicted_masks/391adc0bb223c4eaf3372eae567c94ea.png\n/kaggle/working/predicted_masks/02fa602bb3c7abacdbd7e6afd56ea7bc.png\n/kaggle/working/predicted_masks/5026b3550534bca540e24f489284b8e6.png\n/kaggle/working/predicted_masks/88e16d4ca6160127cd1d5ff99c267599.png\n/kaggle/working/predicted_masks/eb1ef57af2ed9fbb63b28163a745959c.png\n/kaggle/working/predicted_masks/a15fc656702fa602bb3c7abacdbd7e6a.png\n/kaggle/working/predicted_masks/9fc7330398846f67b5df7cdf3f33c3ca.png\n/kaggle/working/predicted_masks/97e1c0e9082ea2c193ac8d551c149b60.png\n/kaggle/working/predicted_masks/8395e56a6d9ba9d45c3dbc695325ded4.png\n/kaggle/working/predicted_masks/cbb2a365b5574868eb60861ee1ff0b8a.png\n/kaggle/working/predicted_masks/8eb5a9a8a8d7fcc9df8e5ad89d284483.png\n/kaggle/working/predicted_masks/f8e26031fbb5e52c41545ba55aadaa77.png\n/kaggle/working/predicted_masks/c5a0808bee60b246359c68c836f843dc.png\n/kaggle/working/predicted_masks/3c3ca4d5060a633a8d5b2b2b55157b77.png\n/kaggle/working/predicted_masks/998906d3694abb47953b0e4909384b57.png\n/kaggle/working/predicted_masks/ff05dec1eb3a70b145a7d8d3b6c0ed75.png\n/kaggle/working/predicted_masks/6b83ef461c2a337948a41964c1d4f50a.png\n/kaggle/working/predicted_masks/39dda50f954ba59c7de13a35276a4764.png\n/kaggle/working/predicted_masks/80cae6daedd989517cb8041ed86e5822.png\n/kaggle/working/predicted_masks/c4be73749a0d21db70dd094a7f32574d.png\n/kaggle/working/predicted_masks/c22268d4b4ef4d95ceea11957998906d.png\n/kaggle/working/predicted_masks/5beb48f0be11d0309d1dff09b8405734.png\n/kaggle/working/predicted_masks/395e56a6d9ba9d45c3dbc695325ded46.png\n/kaggle/working/predicted_masks/4fda8daadc8dd23ae214d84b5dec33fd.png\n/kaggle/working/predicted_masks/5a51625559c7e610b1531871f2fd85a0.png\n/kaggle/working/predicted_masks/aeeb2b535797395305af926a6f23c5d6.png\n/kaggle/working/predicted_masks/26679bff55177a34fc01019eec999fd8.png\n/kaggle/working/predicted_masks/50534bca540e24f489284b8e6953ad88.png\n/kaggle/working/predicted_masks/0fca6a4248a41e8db8b4ed633b456aaa.png\n/kaggle/working/predicted_masks/0398846f67b5df7cdf3f33c3ca4d5060.png\n/kaggle/working/predicted_masks/ad43fe2cd066b9fdbc3bbc04a3afe1f1.png\n/kaggle/working/predicted_masks/7330398846f67b5df7cdf3f33c3ca4d5.png\n/kaggle/working/predicted_masks/fdbc3bbc04a3afe1f119f21b248d152b.png\n/kaggle/working/predicted_masks/9632a3c6f7f7fb2a643f15bd0249ddcc.png\n/kaggle/working/predicted_masks/27738677a6b1f2c6d40b3bbba8f6c704.png\n/kaggle/working/predicted_masks/0626ab4ec3d46e602b296cc5cfd263f1.png\n/kaggle/working/predicted_masks/cf6644589e532a9ee954f81faedbce39.png\n/kaggle/working/predicted_masks/1ad4f13ccf1f4b331a412fc44655fb51.png\n/kaggle/working/predicted_masks/30c2f4fc276ed9f178dc2f4af6266509.png\n/kaggle/working/predicted_masks/b70dd094a7f32574d6c748c41743c6c0.png\n/kaggle/working/predicted_masks/8fa8625605da2023387fd56c04414eaa.png\n/kaggle/working/predicted_masks/2ed9fbb63b28163a745959c03983064a.png\n/kaggle/working/predicted_masks/eecd70ebce6347c491b37c8c2e5a64a8.png\n/kaggle/working/predicted_masks/dc70626ab4ec3d46e602b296cc5cfd26.png\n/kaggle/working/predicted_masks/af35b65bd9ea42cfcfedb5eb2a0e4b50.png\n/kaggle/working/predicted_masks/d694539ef2424a9218697283baa3657e.png\n/kaggle/working/predicted_masks/7cdf3f33c3ca4d5060a633a8d5b2b2b5.png\n/kaggle/working/predicted_masks/df366e057db382b8564872a27301a654.png\n/kaggle/working/predicted_masks/ea42b4eebc9e5a87e443434ac60af150.png\n/kaggle/working/predicted_masks/cc5cfd263f1f90be28799235026b3550.png\n/kaggle/working/predicted_masks/bec33b5e3d68f9d4c331587f9b9d49e2.png\n/kaggle/working/predicted_masks/4ef4d95ceea11957998906d3694abb47.png\n/kaggle/working/predicted_masks/e19769fa2d37d32780fd497e1c0e9082.png\n/kaggle/working/predicted_masks/3657e4314fe384eb2ba3adfda6c1899f.png\n/kaggle/working/predicted_masks/f62f215f0da4ad3a7ab8df9da7386835.png\n/kaggle/working/predicted_masks/e56a6d9ba9d45c3dbc695325ded465ef.png\n/kaggle/working/predicted_masks/45b21960c94b0aab4c024a573c692195.png\n/kaggle/working/predicted_masks/c193ac8d551c149b60f2965341caf528.png\n/kaggle/working/predicted_masks/0af3feff05dec1eb3a70b145a7d8d3b6.png\n/kaggle/working/predicted_masks/98da48d679d7c7c8d3d96fb2b87fbbcf.png\n/kaggle/working/predicted_masks/0a0317371a966bf4b3466463a3c64db1.png\n/kaggle/working/predicted_masks/a48847ae8395e56a6d9ba9d45c3dbc69.png\n/kaggle/working/predicted_masks/cf464aa36bf7c09a3bb0e5ca159410b9.png\n/kaggle/working/predicted_masks/e3c84417fda8019410b1fcf0625f608b.png\n/kaggle/working/predicted_masks/8b8ec74baddc22268d4b4ef4d95ceea1.png\n/kaggle/working/predicted_masks/5b21960c94b0aab4c024a573c692195f.png\n/kaggle/working/predicted_masks/0a5f3601ad4f13ccf1f4b331a412fc44.png\n/kaggle/working/predicted_masks/343f27ebc5d92b9076135d76d0bbd4ce.png\n/kaggle/working/predicted_masks/faef7fdb2d45b21960c94b0aab4c024a.png\n/kaggle/working/predicted_masks/e9082ea2c193ac8d551c149b60f29653.png\n/kaggle/working/predicted_masks/677a6b1f2c6d40b3bbba8f6c704801b3.png\n/kaggle/working/predicted_masks/1c0e9082ea2c193ac8d551c149b60f29.png\n/kaggle/working/predicted_masks/6240619ebebe9e9c9d00a4262b4fe4a5.png\n/kaggle/working/predicted_masks/d5060a633a8d5b2b2b55157b7781e2c7.png\n/kaggle/working/predicted_masks/cdf3f33c3ca4d5060a633a8d5b2b2b55.png\n/kaggle/working/predicted_masks/6ddca6ee1af35b65bd9ea42cfcfedb5e.png\n/kaggle/working/predicted_masks/3f33c3ca4d5060a633a8d5b2b2b55157.png\n/kaggle/working/predicted_masks/1db239dda50f954ba59c7de13a35276a.png\n/kaggle/working/predicted_masks/60a633a8d5b2b2b55157b7781e2c706c.png\n/kaggle/working/predicted_masks/625559c7e610b1531871f2fd85a04fae.png\n/kaggle/working/predicted_masks/54ba59c7de13a35276a476420655433a.png\n/kaggle/working/predicted_masks/3425b976973f13dd311a65d2b46d0a60.png\n/kaggle/working/predicted_masks/7f0019f7e6af7d7147763bdfb928d788.png\n/kaggle/working/predicted_masks/7af2ed9fbb63b28163a745959c039830.png\n/kaggle/working/predicted_masks/85a04faeeb2b535797395305af926a6f.png\n/kaggle/working/predicted_masks/c7e610b1531871f2fd85a04faeeb2b53.png\n/kaggle/working/predicted_masks/e73749a0d21db70dd094a7f32574d6c7.png\n/kaggle/working/predicted_masks/15fc656702fa602bb3c7abacdbd7e6af.png\n/kaggle/working/predicted_masks/a6e51d077bad31c8c5f54ffaa27a6235.png\n/kaggle/working/predicted_masks/559c7e610b1531871f2fd85a04faeeb2.png\n/kaggle/working/predicted_masks/fe1f119f21b248d152b672ab3492fc62.png\n/kaggle/working/predicted_masks/f8e5ad89d2844837f2a0f1536ad3f6a5.png\n/kaggle/working/predicted_masks/f13dd311a65d2b46d0a6085835c525af.png\n/kaggle/working/predicted_masks/4e8bfb905b78a91391adc0bb223c4eaf.png\n/kaggle/working/predicted_masks/a6a4248a41e8db8b4ed633b456aaafac.png\n/kaggle/working/predicted_masks/633a8d5b2b2b55157b7781e2c706c75c.png\n/kaggle/working/predicted_masks/c695325ded465efde988dfb96d081533.png\n/kaggle/working/predicted_masks/782707d7c359e27888daefee82519763.png\n/kaggle/working/predicted_masks/2cd066b9fdbc3bbc04a3afe1f119f21b.png\n/kaggle/working/predicted_masks/780fd497e1c0e9082ea2c193ac8d551c.png\n/kaggle/working/predicted_masks/db5eb2a0e4b50889d874c68c030b9afe.png\n/kaggle/working/predicted_masks/6f4d4987ea3b4bae5672a230194c5a08.png\n/kaggle/working/predicted_masks/5c1346e62522325c1b9c4fc9cbe1eca1.png\n/kaggle/working/predicted_masks/626650908b1cb932a767bf5487ced51b.png\n/kaggle/working/predicted_masks/4e2a6e51d077bad31c8c5f54ffaa27a6.png\n/kaggle/working/predicted_masks/80c643782707d7c359e27888daefee82.png\n/kaggle/working/predicted_masks/a9d45c3dbc695325ded465efde988dfb.png\n/kaggle/working/predicted_masks/67d4dcf9596154efb7cef748d9cbd617.png\n/kaggle/working/predicted_masks/aafac813fe3ccba3e032dd2948a80c64.png\n/kaggle/working/predicted_masks/dd78294679c9cbb2a365b5574868eb60.png\n/kaggle/working/predicted_masks/66e057db382b8564872a27301a654864.png\n/kaggle/working/predicted_masks/05734fbeedd0f9da760db74a29abdb04.png\n/kaggle/working/predicted_masks/019410b1fcf0625f608b4ce97629ab55.png\n/kaggle/working/predicted_masks/afe1f119f21b248d152b672ab3492fc6.png\n/kaggle/working/predicted_masks/1531871f2fd85a04faeeb2b535797395.png\n/kaggle/working/predicted_masks/5e8f14e1e0ae936de314f2d95e6c487f.png\n/kaggle/working/predicted_masks/3c84417fda8019410b1fcf0625f608b4.png\n/kaggle/working/predicted_masks/eff05dec1eb3a70b145a7d8d3b6c0ed7.png\n/kaggle/working/predicted_masks/1209db6dcdda5cc8a788edaeb6aa460a.png\n/kaggle/working/predicted_masks/68d4b4ef4d95ceea11957998906d3694.png\n/kaggle/working/predicted_masks/fcd6da15fc656702fa602bb3c7abacdb.png\n/kaggle/working/predicted_masks/f14e1e0ae936de314f2d95e6c487ffa6.png\n/kaggle/working/predicted_masks/dd094a7f32574d6c748c41743c6c08a1.png\n/kaggle/working/predicted_masks/cb2eb1ef57af2ed9fbb63b28163a7459.png\n/kaggle/working/predicted_masks/f7fdb2d45b21960c94b0aab4c024a573.png\n/kaggle/working/predicted_masks/268d4b4ef4d95ceea11957998906d369.png\n/kaggle/working/predicted_masks/3dd311a65d2b46d0a6085835c525af63.png\n/kaggle/working/predicted_masks/4c1711b62f15ec83b97bb11e8e0c4416.png\n/kaggle/working/predicted_masks/be4d18d5401f659532897255ce2dd4ae.png\n/kaggle/working/predicted_masks/7ad1cf2eb9d32a3dc907950289e976c7.png\n/kaggle/working/predicted_masks/3c692195f853af7f8a4df1ec859759b7.png\n/kaggle/working/predicted_masks/3b8318ecf467d7ad048df39beb176363.png\n/kaggle/working/predicted_masks/dc0bb223c4eaf3372eae567c94ea04c6.png\n/kaggle/working/predicted_masks/e1e0ae936de314f2d95e6c487ffa651b.png\n/kaggle/working/predicted_masks/e1797c77826f9a7021bab9fc73303988.png\n/kaggle/working/predicted_masks/6231002ec4a1fe748f3085f1ce88cbdf.png\n/kaggle/working/predicted_masks/ca4d5060a633a8d5b2b2b55157b7781e.png\n/kaggle/working/predicted_masks/d6bf62f215f0da4ad3a7ab8df9da7386.png\n/kaggle/working/predicted_masks/b21960c94b0aab4c024a573c692195f8.png\n/kaggle/working/predicted_masks/7f32574d6c748c41743c6c08a1d1ad8f.png\n","output_type":"stream"}]}]}