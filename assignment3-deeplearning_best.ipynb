{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9293066a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:02.438314Z",
     "iopub.status.busy": "2023-11-17T11:23:02.437913Z",
     "iopub.status.idle": "2023-11-17T11:23:29.413367Z",
     "shell.execute_reply": "2023-11-17T11:23:29.412073Z"
    },
    "papermill": {
     "duration": 26.996313,
     "end_time": "2023-11-17T11:23:29.416022",
     "exception": false,
     "start_time": "2023-11-17T11:23:02.419709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "Collecting torchgeometry\r\n",
      "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\r\n",
      "Installing collected packages: torchgeometry\r\n",
      "Successfully installed torchgeometry-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "!pip install torchgeometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d0dc0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:29.449779Z",
     "iopub.status.busy": "2023-11-17T11:23:29.449435Z",
     "iopub.status.idle": "2023-11-17T11:23:41.348211Z",
     "shell.execute_reply": "2023-11-17T11:23:41.347158Z"
    },
    "papermill": {
     "duration": 11.91829,
     "end_time": "2023-11-17T11:23:41.350726",
     "exception": false,
     "start_time": "2023-11-17T11:23:29.432436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (1.3.1)\r\n",
      "Requirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.11.3)\r\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.21.0)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations) (6.0.1)\r\n",
      "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.0.4)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.8.1.78)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.2.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.5.0)\r\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (3.1)\r\n",
      "Requirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (10.1.0)\r\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2.31.1)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2023.8.12)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\r\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\r\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.16.1->albumentations) (3.0.9)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7413c9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:41.387068Z",
     "iopub.status.busy": "2023-11-17T11:23:41.386744Z",
     "iopub.status.idle": "2023-11-17T11:23:54.601159Z",
     "shell.execute_reply": "2023-11-17T11:23:54.600329Z"
    },
    "papermill": {
     "duration": 13.235324,
     "end_time": "2023-11-17T11:23:54.603667",
     "exception": false,
     "start_time": "2023-11-17T11:23:41.368343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from torchgeometry.losses import one_hot\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\n",
    "from collections import OrderedDict\n",
    "import wandb\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose, OneOf\n",
    "from albumentations.augmentations.transforms import RandomGamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1cf4f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:54.640822Z",
     "iopub.status.busy": "2023-11-17T11:23:54.639766Z",
     "iopub.status.idle": "2023-11-17T11:23:54.644341Z",
     "shell.execute_reply": "2023-11-17T11:23:54.643456Z"
    },
    "papermill": {
     "duration": 0.025826,
     "end_time": "2023-11-17T11:23:54.646502",
     "exception": false,
     "start_time": "2023-11-17T11:23:54.620676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b1fd88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:54.682610Z",
     "iopub.status.busy": "2023-11-17T11:23:54.682350Z",
     "iopub.status.idle": "2023-11-17T11:23:55.685837Z",
     "shell.execute_reply": "2023-11-17T11:23:55.684658Z"
    },
    "papermill": {
     "duration": 1.023818,
     "end_time": "2023-11-17T11:23:55.688110",
     "exception": false,
     "start_time": "2023-11-17T11:23:54.664292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-9953165e-0e31-26cf-b40d-96f3f83f8b17)\r\n",
      "GPU 1: Tesla T4 (UUID: GPU-59167f3a-11af-a34a-65c7-735b8f1eb6fd)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c192f428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:55.724159Z",
     "iopub.status.busy": "2023-11-17T11:23:55.723811Z",
     "iopub.status.idle": "2023-11-17T11:23:55.831029Z",
     "shell.execute_reply": "2023-11-17T11:23:55.830128Z"
    },
    "papermill": {
     "duration": 0.127255,
     "end_time": "2023-11-17T11:23:55.832996",
     "exception": false,
     "start_time": "2023-11-17T11:23:55.705741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e121c2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:55.870476Z",
     "iopub.status.busy": "2023-11-17T11:23:55.869801Z",
     "iopub.status.idle": "2023-11-17T11:23:55.875909Z",
     "shell.execute_reply": "2023-11-17T11:23:55.874864Z"
    },
    "papermill": {
     "duration": 0.027138,
     "end_time": "2023-11-17T11:23:55.877858",
     "exception": false,
     "start_time": "2023-11-17T11:23:55.850720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of class in the data set (3: neoplastic, non neoplastic, background)\n",
    "num_classes = 3\n",
    "\n",
    "# Number of epoch\n",
    "epochs = 30\n",
    "\n",
    "# Hyperparameters for training \n",
    "learning_rate = 2e-04\n",
    "batch_size = 4\n",
    "display_step = 50\n",
    "\n",
    "# Model path\n",
    "checkpoint_path = '/kaggle/working/unet_model.pth'\n",
    "pretrained_path = \"/kaggle/input/unet-checkpoint/unet_model.pth\"\n",
    "# Initialize lists to keep track of loss and accuracy\n",
    "loss_epoch_array = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "valid_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ed7053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:55.912966Z",
     "iopub.status.busy": "2023-11-17T11:23:55.912669Z",
     "iopub.status.idle": "2023-11-17T11:23:55.925421Z",
     "shell.execute_reply": "2023-11-17T11:23:55.924549Z"
    },
    "papermill": {
     "duration": 0.032724,
     "end_time": "2023-11-17T11:23:55.927331",
     "exception": false,
     "start_time": "2023-11-17T11:23:55.894607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainTransform:\n",
    "    def __init__(self):\n",
    "        self.transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomGamma(gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
    "            A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "            A.OneOf([A.Blur(), A.GaussianBlur(), A.GlassBlur(), A.MotionBlur(),\n",
    "                    A.GaussNoise(), A.Sharpen(), A.MedianBlur(), A.MultiplicativeNoise()]),\n",
    "            A.CoarseDropout(p=0.2, max_height=35, max_width=35, fill_value=255),\n",
    "            A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.15, brightness_coeff=1.5, p=0.09),\n",
    "            A.RandomShadow(p=0.1),\n",
    "            A.ShiftScaleRotate(p=0.45, border_mode=cv2.BORDER_CONSTANT, shift_limit=0.15, scale_limit=0.15),\n",
    "            A.Resize(256, 256, interpolation=cv2.INTER_LINEAR),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        return self.transform(image=img, mask=mask)\n",
    "\n",
    "\n",
    "class ValTransform:\n",
    "    def __init__(self):\n",
    "        self.transform = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    def __call__(self, img, mask):\n",
    "        return self.transform(image=img, mask=mask)\n",
    "\n",
    "\n",
    "class TestTransform:\n",
    "    def __init__(self):\n",
    "        self.transform = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    def __call__(self, img):\n",
    "        return self.transform(image=img)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31195f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:55.961523Z",
     "iopub.status.busy": "2023-11-17T11:23:55.961239Z",
     "iopub.status.idle": "2023-11-17T11:23:55.976764Z",
     "shell.execute_reply": "2023-11-17T11:23:55.976069Z"
    },
    "papermill": {
     "duration": 0.034744,
     "end_time": "2023-11-17T11:23:55.978584",
     "exception": false,
     "start_time": "2023-11-17T11:23:55.943840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnetDataClass(Dataset):\n",
    "    def __init__(self, image_dir,mask_dir = [],mode = \"train\"):\n",
    "        super(UnetDataClass, self).__init__()\n",
    "        self.mode = mode\n",
    "        if mode == \"train\":\n",
    "            self.train_path = image_dir\n",
    "            self.train_mask_path = mask_dir\n",
    "            self.len = len(self.train_path)\n",
    "            self.train_transform = TrainTransform()\n",
    "        elif mode == \"valid\":\n",
    "            self.val_path = image_dir\n",
    "            self.val_mask_path = mask_dir\n",
    "            self.len = len(self.val_path)\n",
    "            self.val_transform = ValTransform()\n",
    "        elif mode == \"test\":\n",
    "            self.test_path = image_dir\n",
    "            self.len = len(self.test_path)\n",
    "            self.test_transform = TestTransform()\n",
    "\n",
    "            \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        image = cv2.resize(image, (256,256))\n",
    "        # lower boundary RED color range values; Hue (0 - 10)\n",
    "        lower1 = np.array([0, 100, 20])\n",
    "        upper1 = np.array([10, 255, 255])\n",
    "        # upper boundary RED color range values; Hue (160 - 180)\n",
    "        lower2 = np.array([160, 100, 20])\n",
    "        upper2 = np.array([179, 255, 255])\n",
    "        lower_mask = cv2.inRange(image, lower1, upper1)\n",
    "        upper_mask = cv2.inRange(image, lower2, upper2)\n",
    "\n",
    "        red_mask = lower_mask + upper_mask\n",
    "        red_mask[red_mask != 0] = 1\n",
    "\n",
    "        # boundary GREEN color range values; Hue (36 - 70)\n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "\n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        return full_mask\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if self.mode == \"train\":\n",
    "            image = cv2.imread(self.train_path[index])\n",
    "            image = cv2.resize(image, (256,256))\n",
    "            mask = self.read_mask(self.train_mask_path[index])\n",
    "            return self.train_transform(image, mask)\n",
    "        elif self.mode == \"valid\":\n",
    "            image = cv2.imread(self.val_path[index])\n",
    "            image = cv2.resize(image, (256,256))\n",
    "            mask = self.read_mask(self.val_mask_path[index])\n",
    "            return self.val_transform(image, mask)\n",
    "        elif self.mode == \"test\":\n",
    "            image = cv2.imread(self.test_path[index])\n",
    "            H, W, _ = image.shape\n",
    "            image = cv2.resize(image, (256,256))\n",
    "            image = self.test_transform(image)\n",
    "            \n",
    "            file_name = self.test_path[index].split('/')[-1].split('.')[0]\n",
    "            return  image, file_name,H, W\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd969450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:56.014477Z",
     "iopub.status.busy": "2023-11-17T11:23:56.014206Z",
     "iopub.status.idle": "2023-11-17T11:23:57.700014Z",
     "shell.execute_reply": "2023-11-17T11:23:57.698941Z"
    },
    "papermill": {
     "duration": 1.707368,
     "end_time": "2023-11-17T11:23:57.702606",
     "exception": false,
     "start_time": "2023-11-17T11:23:55.995238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = []\n",
    "images_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\n",
    "for root, dirs, files in os.walk(images_path):\n",
    "    for file in files:\n",
    "        path = os.path.join(root,file)\n",
    "        image_path.append(path)\n",
    "\n",
    "mask_path = []\n",
    "masks_path =  \"/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt/\"\n",
    "for root, dirs, files in os.walk(masks_path):\n",
    "    for file in files:\n",
    "        path = os.path.join(root,file)\n",
    "        mask_path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06aab6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:57.783901Z",
     "iopub.status.busy": "2023-11-17T11:23:57.783176Z",
     "iopub.status.idle": "2023-11-17T11:23:57.787649Z",
     "shell.execute_reply": "2023-11-17T11:23:57.786665Z"
    },
    "papermill": {
     "duration": 0.068176,
     "end_time": "2023-11-17T11:23:57.789663",
     "exception": false,
     "start_time": "2023-11-17T11:23:57.721487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "valid_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4793d54e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:57.826798Z",
     "iopub.status.busy": "2023-11-17T11:23:57.826484Z",
     "iopub.status.idle": "2023-11-17T11:23:57.832850Z",
     "shell.execute_reply": "2023-11-17T11:23:57.831985Z"
    },
    "papermill": {
     "duration": 0.02738,
     "end_time": "2023-11-17T11:23:57.834978",
     "exception": false,
     "start_time": "2023-11-17T11:23:57.807598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "shuffle_list = list(zip(image_path, mask_path))\n",
    "random.shuffle(shuffle_list)\n",
    "image_path, mask_path = zip(*shuffle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e930723f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:57.872551Z",
     "iopub.status.busy": "2023-11-17T11:23:57.872249Z",
     "iopub.status.idle": "2023-11-17T11:23:57.878812Z",
     "shell.execute_reply": "2023-11-17T11:23:57.877956Z"
    },
    "papermill": {
     "duration": 0.02693,
     "end_time": "2023-11-17T11:23:57.880690",
     "exception": false,
     "start_time": "2023-11-17T11:23:57.853760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(train_ratio * len(image_path))\n",
    "train_path = image_path[:train_size]\n",
    "train_mask_path = mask_path[:train_size]\n",
    "val_path = image_path[train_size:]\n",
    "val_mask_path = mask_path[train_size:]\n",
    "train_dataset = UnetDataClass(train_path, train_mask_path, mode=\"train\")\n",
    "val_dataset = UnetDataClass(val_path, val_mask_path, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38e5773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:57.915360Z",
     "iopub.status.busy": "2023-11-17T11:23:57.914788Z",
     "iopub.status.idle": "2023-11-17T11:23:57.919806Z",
     "shell.execute_reply": "2023-11-17T11:23:57.918965Z"
    },
    "papermill": {
     "duration": 0.024595,
     "end_time": "2023-11-17T11:23:57.921815",
     "exception": false,
     "start_time": "2023-11-17T11:23:57.897220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6522d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:57.955982Z",
     "iopub.status.busy": "2023-11-17T11:23:57.955459Z",
     "iopub.status.idle": "2023-11-17T11:23:58.215158Z",
     "shell.execute_reply": "2023-11-17T11:23:58.214119Z"
    },
    "papermill": {
     "duration": 0.279266,
     "end_time": "2023-11-17T11:23:58.217529",
     "exception": false,
     "start_time": "2023-11-17T11:23:57.938263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset.__getitem__(7)['mask'])\n",
    "print(train_loader.dataset.__getitem__(7)['mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d8be59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:58.255110Z",
     "iopub.status.busy": "2023-11-17T11:23:58.254753Z",
     "iopub.status.idle": "2023-11-17T11:23:58.389912Z",
     "shell.execute_reply": "2023-11-17T11:23:58.388731Z"
    },
    "papermill": {
     "duration": 0.156112,
     "end_time": "2023-11-17T11:23:58.392110",
     "exception": false,
     "start_time": "2023-11-17T11:23:58.235998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "         [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "         [-2.1008, -2.1008, -2.0152,  ..., -2.1008, -2.1008, -2.1008],\n",
      "         ...,\n",
      "         [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "         [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "         [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "        [[-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-2.0182, -2.0182, -1.9307,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         ...,\n",
      "         [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "        [[-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "         [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "         [-1.7870, -1.7870, -1.6999,  ..., -1.7870, -1.7870, -1.7870],\n",
      "         ...,\n",
      "         [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "         [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "         [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870]]])\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(val_loader.dataset.__getitem__(7)['image'])\n",
    "print(val_loader.dataset.__getitem__(7)['image'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399b762",
   "metadata": {
    "papermill": {
     "duration": 0.016509,
     "end_time": "2023-11-17T11:23:58.427706",
     "exception": false,
     "start_time": "2023-11-17T11:23:58.411197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "182f919d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:23:58.462902Z",
     "iopub.status.busy": "2023-11-17T11:23:58.462085Z",
     "iopub.status.idle": "2023-11-17T11:24:17.542948Z",
     "shell.execute_reply": "2023-11-17T11:24:17.541734Z"
    },
    "papermill": {
     "duration": 19.10097,
     "end_time": "2023-11-17T11:24:17.545370",
     "exception": false,
     "start_time": "2023-11-17T11:23:58.444400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\r\n",
      "  Obtaining dependency information for segmentation-models-pytorch from https://files.pythonhosted.org/packages/cb/70/4aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d/segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata\r\n",
      "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.15.1)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting timm==0.9.2 (from segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for timm==0.9.2 from https://files.pythonhosted.org/packages/29/90/94f5deb8d76e24a89813aef95e8809ca8fd7414490428480eda19b133d4a/timm-0.9.2-py3-none-any.whl.metadata\r\n",
      "  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.1.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\r\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for munch from https://files.pythonhosted.org/packages/56/b3/7c69b37f03260a061883bec0e7b05be7117c1b1c85f5212c72c8c2bc3c8c/munch-4.0.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.17.3)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2023.10.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (21.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2023.7.22)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\r\n",
      "Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=6f4082e823cc49a39acca90aff4d08d342129db0b9f8dce61df12807928818b1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=e7a1d94d83e4f842d76e6a7f9d8b15f1aa2936eec091ea08db2e1335bd22b198\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 0.9.10\r\n",
      "    Uninstalling timm-0.9.10:\r\n",
      "      Successfully uninstalled timm-0.9.10\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64df4b9",
   "metadata": {
    "papermill": {
     "duration": 0.020281,
     "end_time": "2023-11-17T11:24:17.586653",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.566372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NORMAL CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768edcdb",
   "metadata": {
    "papermill": {
     "duration": 0.020108,
     "end_time": "2023-11-17T11:24:17.626435",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.606327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8391fd86",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:17.671110Z",
     "iopub.status.busy": "2023-11-17T11:24:17.670683Z",
     "iopub.status.idle": "2023-11-17T11:24:17.676320Z",
     "shell.execute_reply": "2023-11-17T11:24:17.675426Z"
    },
    "papermill": {
     "duration": 0.030767,
     "end_time": "2023-11-17T11:24:17.678396",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.647629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class encoder_block(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(encoder_block, self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(p=0.3)\n",
    "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         next_layer = self.max_pool(x)\n",
    "#         skip_layer = x\n",
    "        \n",
    "#         return next_layer, skip_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082b8fd",
   "metadata": {
    "papermill": {
     "duration": 0.021031,
     "end_time": "2023-11-17T11:24:17.720309",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.699278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46fdb96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:17.762710Z",
     "iopub.status.busy": "2023-11-17T11:24:17.762337Z",
     "iopub.status.idle": "2023-11-17T11:24:17.767294Z",
     "shell.execute_reply": "2023-11-17T11:24:17.766469Z"
    },
    "papermill": {
     "duration": 0.028389,
     "end_time": "2023-11-17T11:24:17.769235",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.740846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class decoder_block(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(decoder_block, self).__init__()\n",
    "        \n",
    "#         self.transpose_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(2 * out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "#         self.relu = nn.ReLU() \n",
    "#         self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "#     def forward(self, x, skip_layer):\n",
    "#         x = self.transpose_conv(x)\n",
    "#         x = torch.cat([x, skip_layer], axis=1)\n",
    "        \n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2666c",
   "metadata": {
    "papermill": {
     "duration": 0.019439,
     "end_time": "2023-11-17T11:24:17.808954",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.789515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76ba578e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:17.849362Z",
     "iopub.status.busy": "2023-11-17T11:24:17.849043Z",
     "iopub.status.idle": "2023-11-17T11:24:17.853733Z",
     "shell.execute_reply": "2023-11-17T11:24:17.852890Z"
    },
    "papermill": {
     "duration": 0.027222,
     "end_time": "2023-11-17T11:24:17.855622",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.828400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class bottleneck_block(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(bottleneck_block, self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731edbd",
   "metadata": {
    "papermill": {
     "duration": 0.019244,
     "end_time": "2023-11-17T11:24:17.895760",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.876516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Normal Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98cbd6cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:17.936232Z",
     "iopub.status.busy": "2023-11-17T11:24:17.935940Z",
     "iopub.status.idle": "2023-11-17T11:24:17.940853Z",
     "shell.execute_reply": "2023-11-17T11:24:17.939971Z"
    },
    "papermill": {
     "duration": 0.027296,
     "end_time": "2023-11-17T11:24:17.942862",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.915566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # UNet model\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_class=3):\n",
    "#         super(UNet, self).__init__()\n",
    "#         # Encoder blocks\n",
    "#         self.enc1 = encoder_block(3, 64)\n",
    "#         self.enc2 = encoder_block(64, 128)\n",
    "#         self.enc3 = encoder_block(128, 256)\n",
    "#         self.enc4 = encoder_block(256, 512)\n",
    "        \n",
    "#         # Bottleneck block\n",
    "#         self.bottleneck = bottleneck_block(512, 1024)\n",
    "        \n",
    "#         # Decoder blocks\n",
    "#         self.dec1 = decoder_block(1024, 512)\n",
    "#         self.dec2 = decoder_block(512, 256)\n",
    "#         self.dec3 = decoder_block(256, 128)\n",
    "#         self.dec4 = decoder_block(128, 64)\n",
    "        \n",
    "#         # 1x1 convolution\n",
    "#         self.out = nn.Conv2d(64, n_class, kernel_size=1, padding='same')\n",
    "        \n",
    "#     def forward(self, image):\n",
    "#         n1, s1 = self.enc1(image)\n",
    "#         n2, s2 = self.enc2(n1)\n",
    "#         n3, s3 = self.enc3(n2)\n",
    "#         n4, s4 = self.enc4(n3)\n",
    "        \n",
    "#         n5 = self.bottleneck(n4)\n",
    "        \n",
    "#         n6 = self.dec1(n5, s4)\n",
    "#         n7 = self.dec2(n6, s3)\n",
    "#         n8 = self.dec3(n7, s2)\n",
    "#         n9 = self.dec4(n8, s1)\n",
    "        \n",
    "#         output = self.out(n9)\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970b257",
   "metadata": {
    "papermill": {
     "duration": 0.019903,
     "end_time": "2023-11-17T11:24:17.982303",
     "exception": false,
     "start_time": "2023-11-17T11:24:17.962400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ResUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19265709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:18.025046Z",
     "iopub.status.busy": "2023-11-17T11:24:18.024691Z",
     "iopub.status.idle": "2023-11-17T11:24:18.044092Z",
     "shell.execute_reply": "2023-11-17T11:24:18.043196Z"
    },
    "papermill": {
     "duration": 0.043583,
     "end_time": "2023-11-17T11:24:18.046100",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.002517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "    nn.ReLU(inplace=True),\n",
    "  )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb609a",
   "metadata": {
    "papermill": {
     "duration": 0.020142,
     "end_time": "2023-11-17T11:24:18.086951",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.066809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross-Entropy DiceLoss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ae86197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:18.129392Z",
     "iopub.status.busy": "2023-11-17T11:24:18.129064Z",
     "iopub.status.idle": "2023-11-17T11:24:18.140739Z",
     "shell.execute_reply": "2023-11-17T11:24:18.139896Z"
    },
    "papermill": {
     "duration": 0.034528,
     "end_time": "2023-11-17T11:24:18.142698",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.108170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CEDiceLoss(nn.Module):\n",
    "    def __init__(self, weights) -> None:\n",
    "        super(CEDiceLoss, self).__init__()\n",
    "        self.eps: float = 1e-6\n",
    "        self.weights: torch.Tensor = weights\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input: torch.Tensor,\n",
    "            target: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(input):\n",
    "            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(input)))\n",
    "        if not len(input.shape) == 4:\n",
    "            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
    "                             .format(input.shape))\n",
    "        if not input.shape[-2:] == target.shape[-2:]:\n",
    "            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
    "                             .format(input.shape, input.shape))\n",
    "        if not input.device == target.device:\n",
    "            raise ValueError(\n",
    "                \"input and target must be in the same device. Got: {}\" .format(\n",
    "                    input.device, target.device))\n",
    "        if not self.weights.shape[1] == input.shape[1]:\n",
    "            raise ValueError(\"The number of weights must equal the number of classes\")\n",
    "        if not torch.sum(self.weights).item() == 1:\n",
    "            raise ValueError(\"The sum of all weights must equal 1\")\n",
    "            \n",
    "        # cross entropy loss\n",
    "        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n",
    "        \n",
    "        # compute softmax over the classes axis\n",
    "        input_soft = F.softmax(input, dim=1)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = one_hot(target, num_classes=input.shape[1],\n",
    "                                 device=input.device, dtype=input.dtype)\n",
    "\n",
    "        # compute the actual dice score\n",
    "        dims = (2, 3)\n",
    "        intersection = torch.sum(input_soft * target_one_hot, dims)\n",
    "        cardinality = torch.sum(input_soft + target_one_hot, dims)\n",
    "\n",
    "        dice_score = 2. * intersection / (cardinality + self.eps)\n",
    "        \n",
    "        dice_score = torch.sum(dice_score * self.weights, dim=1)\n",
    "        \n",
    "        return torch.mean(1. - dice_score) + celoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49ee12",
   "metadata": {
    "papermill": {
     "duration": 0.019939,
     "end_time": "2023-11-17T11:24:18.183188",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.163249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weight Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41a198a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:18.225740Z",
     "iopub.status.busy": "2023-11-17T11:24:18.225384Z",
     "iopub.status.idle": "2023-11-17T11:24:18.230795Z",
     "shell.execute_reply": "2023-11-17T11:24:18.229864Z"
    },
    "papermill": {
     "duration": 0.029271,
     "end_time": "2023-11-17T11:24:18.232898",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.203627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        # Xavier Distribution\n",
    "        torch.nn.init.xavier_uniform_(model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0c46f",
   "metadata": {
    "papermill": {
     "duration": 0.020557,
     "end_time": "2023-11-17T11:24:18.274824",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.254267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3245606d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:18.317008Z",
     "iopub.status.busy": "2023-11-17T11:24:18.316679Z",
     "iopub.status.idle": "2023-11-17T11:24:18.322377Z",
     "shell.execute_reply": "2023-11-17T11:24:18.321518Z"
    },
    "papermill": {
     "duration": 0.02887,
     "end_time": "2023-11-17T11:24:18.324195",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.295325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, path):\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "def load_model(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051d32e",
   "metadata": {
    "papermill": {
     "duration": 0.020202,
     "end_time": "2023-11-17T11:24:18.364344",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.344142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f226d3af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:18.406258Z",
     "iopub.status.busy": "2023-11-17T11:24:18.405910Z",
     "iopub.status.idle": "2023-11-17T11:24:18.417395Z",
     "shell.execute_reply": "2023-11-17T11:24:18.416620Z"
    },
    "papermill": {
     "duration": 0.035546,
     "end_time": "2023-11-17T11:24:18.419698",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.384152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train function for each epoch\n",
    "def train(train_dataloader, valid_dataloader,learing_rate_scheduler, epoch, display_step):\n",
    "    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n",
    "    start_time = time.time()\n",
    "    train_loss_epoch = 0\n",
    "    test_loss_epoch = 0\n",
    "    last_loss = 999999999\n",
    "    model.train()\n",
    "    for i, load in enumerate(train_dataloader):\n",
    "        \n",
    "        # Load data into GPU\n",
    "        data, targets = load['image'].to(device), load['mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Backpropagation, compute gradients\n",
    "        loss = loss_function(outputs, targets.long())\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Save loss\n",
    "        train_loss_epoch += loss.item()\n",
    "        if (i+1) % display_step == 0:\n",
    "#             accuracy = float(test(test_loader))\n",
    "            print('Train Epoch: {} [{}/{} ({}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch + 1, (i+1) * len(data), len(train_dataloader.dataset), 100 * (i+1) * len(data) / len(train_dataloader.dataset), \n",
    "                loss.item()))\n",
    "                  \n",
    "    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n",
    "    train_loss_epoch/= (i + 1)\n",
    "    \n",
    "    # Evaluate the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for load in valid_dataloader:\n",
    "            data, target = load['image'].to(device), load['mask'].to(device)\n",
    "            test_output = model(data)\n",
    "            test_loss = loss_function(test_output, target.long())\n",
    "            test_loss_epoch += test_loss.item()\n",
    "            \n",
    "    test_loss_epoch/= (i+1)\n",
    "    \n",
    "    return train_loss_epoch , test_loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41332f84",
   "metadata": {
    "papermill": {
     "duration": 0.020833,
     "end_time": "2023-11-17T11:24:18.461530",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.440697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f3ee98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:18.504327Z",
     "iopub.status.busy": "2023-11-17T11:24:18.503722Z",
     "iopub.status.idle": "2023-11-17T11:24:18.509825Z",
     "shell.execute_reply": "2023-11-17T11:24:18.508983Z"
    },
    "papermill": {
     "duration": 0.029587,
     "end_time": "2023-11-17T11:24:18.511721",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.482134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(dataloader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i,load in enumerate(dataloader):\n",
    "            data, targets = load['image'].to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            test_loss += targets.size(0)\n",
    "            correct += torch.sum(pred == targets).item()\n",
    "    return 100.0 * correct / test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0355453",
   "metadata": {
    "papermill": {
     "duration": 0.019147,
     "end_time": "2023-11-17T11:24:18.550743",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.531596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# UnetPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7ecf6d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:18.591679Z",
     "iopub.status.busy": "2023-11-17T11:24:18.591013Z",
     "iopub.status.idle": "2023-11-17T11:24:26.308099Z",
     "shell.execute_reply": "2023-11-17T11:24:26.307234Z"
    },
    "papermill": {
     "duration": 7.740705,
     "end_time": "2023-11-17T11:24:26.311013",
     "exception": false,
     "start_time": "2023-11-17T11:24:18.570308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 139MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): UnetPlusPlus(\n",
       "    (encoder): ResNetEncoder(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): UnetPlusPlusDecoder(\n",
       "      (center): Identity()\n",
       "      (blocks): ModuleDict(\n",
       "        (x_0_0): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_0_1): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_1_1): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_0_2): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_1_2): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_2_2): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_0_3): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_1_3): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_2_3): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_3_3): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (x_0_4): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (segmentation_head): SegmentationHead(\n",
       "      (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Identity()\n",
       "      (2): Activation(\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# model = UNet(3)\n",
    "import segmentation_models_pytorch as smp\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet34\",      \n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,                 \n",
    "    classes=3                       \n",
    ")\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a6188",
   "metadata": {
    "papermill": {
     "duration": 0.020549,
     "end_time": "2023-11-17T11:24:26.353248",
     "exception": false,
     "start_time": "2023-11-17T11:24:26.332699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ResUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9078d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:26.396149Z",
     "iopub.status.busy": "2023-11-17T11:24:26.395797Z",
     "iopub.status.idle": "2023-11-17T11:24:26.399651Z",
     "shell.execute_reply": "2023-11-17T11:24:26.398841Z"
    },
    "papermill": {
     "duration": 0.02767,
     "end_time": "2023-11-17T11:24:26.401744",
     "exception": false,
     "start_time": "2023-11-17T11:24:26.374074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = ResNetUNet(n_classes = 3)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e5bec",
   "metadata": {
    "papermill": {
     "duration": 0.02034,
     "end_time": "2023-11-17T11:24:26.443062",
     "exception": false,
     "start_time": "2023-11-17T11:24:26.422722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Normal Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "555e0530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:26.487518Z",
     "iopub.status.busy": "2023-11-17T11:24:26.487216Z",
     "iopub.status.idle": "2023-11-17T11:24:26.491421Z",
     "shell.execute_reply": "2023-11-17T11:24:26.490678Z"
    },
    "papermill": {
     "duration": 0.028243,
     "end_time": "2023-11-17T11:24:26.493402",
     "exception": false,
     "start_time": "2023-11-17T11:24:26.465159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = UNet()\n",
    "# model.apply(weights_init)\n",
    "# # model = nn.DataParallel(model)\n",
    "# #checkpoint = torch.load(pretrained_path)\n",
    "\n",
    "# #new_state_dict = OrderedDict()\n",
    "# #for k, v in checkpoint['model'].items():\n",
    "# #    name = k[7:] # remove `module.`\n",
    "# #    new_state_dict[name] = v\n",
    "# # load params\n",
    "# #model.load_state_dict(new_state_dict)\n",
    "# model = nn.DataParallel(model)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88959351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:26.536550Z",
     "iopub.status.busy": "2023-11-17T11:24:26.536070Z",
     "iopub.status.idle": "2023-11-17T11:24:26.543068Z",
     "shell.execute_reply": "2023-11-17T11:24:26.542203Z"
    },
    "papermill": {
     "duration": 0.031082,
     "end_time": "2023-11-17T11:24:26.544982",
     "exception": false,
     "start_time": "2023-11-17T11:24:26.513900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\n",
    "loss_function = CEDiceLoss(weights)\n",
    "\n",
    "# Define the optimizer (Adam optimizer)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "learing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de395244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:26.587402Z",
     "iopub.status.busy": "2023-11-17T11:24:26.587139Z",
     "iopub.status.idle": "2023-11-17T11:24:26.761273Z",
     "shell.execute_reply": "2023-11-17T11:24:26.760186Z"
    },
    "papermill": {
     "duration": 0.198017,
     "end_time": "2023-11-17T11:24:26.763680",
     "exception": false,
     "start_time": "2023-11-17T11:24:26.565663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model(model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e167d6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:26.807306Z",
     "iopub.status.busy": "2023-11-17T11:24:26.806982Z",
     "iopub.status.idle": "2023-11-17T11:24:28.287247Z",
     "shell.execute_reply": "2023-11-17T11:24:28.286423Z"
    },
    "papermill": {
     "duration": 1.504014,
     "end_time": "2023-11-17T11:24:28.289218",
     "exception": false,
     "start_time": "2023-11-17T11:24:26.785204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(\n",
    "    key = \"4184ad0bd316668aed39d30ad508ca2d23ef0479\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d18cbce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:24:28.333451Z",
     "iopub.status.busy": "2023-11-17T11:24:28.333152Z",
     "iopub.status.idle": "2023-11-17T11:57:49.572784Z",
     "shell.execute_reply": "2023-11-17T11:57:49.571833Z"
    },
    "papermill": {
     "duration": 2001.264717,
     "end_time": "2023-11-17T11:57:49.575260",
     "exception": false,
     "start_time": "2023-11-17T11:24:28.310543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcongquyk66hust\u001b[0m (\u001b[33mhelarica\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20231117_112428-wgilnsd2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcelestial-dust-9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/helarica/Polyps_Segmentation_3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/helarica/Polyps_Segmentation_3/runs/wgilnsd2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch #1, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 1 [200/800 (25.0%)]\tLoss: 0.9714\n",
      "Train Epoch: 1 [400/800 (50.0%)]\tLoss: 0.6571\n",
      "Train Epoch: 1 [600/800 (75.0%)]\tLoss: 0.4473\n",
      "Train Epoch: 1 [800/800 (100.0%)]\tLoss: 0.4889\n",
      "Done epoch #1, time for this epoch: 77.73257112503052s\n",
      "Start epoch #2, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 2 [200/800 (25.0%)]\tLoss: 0.7320\n",
      "Train Epoch: 2 [400/800 (50.0%)]\tLoss: 0.7336\n",
      "Train Epoch: 2 [600/800 (75.0%)]\tLoss: 0.3258\n",
      "Train Epoch: 2 [800/800 (100.0%)]\tLoss: 0.4655\n",
      "Done epoch #2, time for this epoch: 54.53241586685181s\n",
      "Start epoch #3, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 3 [200/800 (25.0%)]\tLoss: 0.4190\n",
      "Train Epoch: 3 [400/800 (50.0%)]\tLoss: 0.4419\n",
      "Train Epoch: 3 [600/800 (75.0%)]\tLoss: 0.3735\n",
      "Train Epoch: 3 [800/800 (100.0%)]\tLoss: 0.3565\n",
      "Done epoch #3, time for this epoch: 55.362605571746826s\n",
      "Start epoch #4, learning rate for this epoch: [0.0002]\n",
      "Train Epoch: 4 [200/800 (25.0%)]\tLoss: 0.6008\n",
      "Train Epoch: 4 [400/800 (50.0%)]\tLoss: 0.2596\n",
      "Train Epoch: 4 [600/800 (75.0%)]\tLoss: 0.1859\n",
      "Train Epoch: 4 [800/800 (100.0%)]\tLoss: 0.4199\n",
      "Done epoch #4, time for this epoch: 54.82560873031616s\n",
      "Start epoch #5, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 5 [200/800 (25.0%)]\tLoss: 0.3885\n",
      "Train Epoch: 5 [400/800 (50.0%)]\tLoss: 0.2822\n",
      "Train Epoch: 5 [600/800 (75.0%)]\tLoss: 0.3073\n",
      "Train Epoch: 5 [800/800 (100.0%)]\tLoss: 0.4193\n",
      "Done epoch #5, time for this epoch: 54.17617893218994s\n",
      "Start epoch #6, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 6 [200/800 (25.0%)]\tLoss: 0.1854\n",
      "Train Epoch: 6 [400/800 (50.0%)]\tLoss: 0.2238\n",
      "Train Epoch: 6 [600/800 (75.0%)]\tLoss: 0.3749\n",
      "Train Epoch: 6 [800/800 (100.0%)]\tLoss: 0.3820\n",
      "Done epoch #6, time for this epoch: 54.64711260795593s\n",
      "Start epoch #7, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 7 [200/800 (25.0%)]\tLoss: 0.4110\n",
      "Train Epoch: 7 [400/800 (50.0%)]\tLoss: 0.2260\n",
      "Train Epoch: 7 [600/800 (75.0%)]\tLoss: 0.2865\n",
      "Train Epoch: 7 [800/800 (100.0%)]\tLoss: 0.2697\n",
      "Done epoch #7, time for this epoch: 54.37965369224548s\n",
      "Start epoch #8, learning rate for this epoch: [0.00012]\n",
      "Train Epoch: 8 [200/800 (25.0%)]\tLoss: 0.2746\n",
      "Train Epoch: 8 [400/800 (50.0%)]\tLoss: 0.3224\n",
      "Train Epoch: 8 [600/800 (75.0%)]\tLoss: 0.3280\n",
      "Train Epoch: 8 [800/800 (100.0%)]\tLoss: 0.3218\n",
      "Done epoch #8, time for this epoch: 55.510159492492676s\n",
      "Start epoch #9, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 9 [200/800 (25.0%)]\tLoss: 0.1276\n",
      "Train Epoch: 9 [400/800 (50.0%)]\tLoss: 0.2806\n",
      "Train Epoch: 9 [600/800 (75.0%)]\tLoss: 0.4050\n",
      "Train Epoch: 9 [800/800 (100.0%)]\tLoss: 0.4208\n",
      "Done epoch #9, time for this epoch: 54.327635765075684s\n",
      "Start epoch #10, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 10 [200/800 (25.0%)]\tLoss: 0.1627\n",
      "Train Epoch: 10 [400/800 (50.0%)]\tLoss: 0.4935\n",
      "Train Epoch: 10 [600/800 (75.0%)]\tLoss: 0.4332\n",
      "Train Epoch: 10 [800/800 (100.0%)]\tLoss: 0.3523\n",
      "Done epoch #10, time for this epoch: 54.852415323257446s\n",
      "Start epoch #11, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 11 [200/800 (25.0%)]\tLoss: 0.4046\n",
      "Train Epoch: 11 [400/800 (50.0%)]\tLoss: 0.1957\n",
      "Train Epoch: 11 [600/800 (75.0%)]\tLoss: 0.3339\n",
      "Train Epoch: 11 [800/800 (100.0%)]\tLoss: 0.4471\n",
      "Done epoch #11, time for this epoch: 54.592549085617065s\n",
      "Start epoch #12, learning rate for this epoch: [7.2e-05]\n",
      "Train Epoch: 12 [200/800 (25.0%)]\tLoss: 0.3792\n",
      "Train Epoch: 12 [400/800 (50.0%)]\tLoss: 0.2816\n",
      "Train Epoch: 12 [600/800 (75.0%)]\tLoss: 0.2334\n",
      "Train Epoch: 12 [800/800 (100.0%)]\tLoss: 0.1818\n",
      "Done epoch #12, time for this epoch: 54.300204277038574s\n",
      "Start epoch #13, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 13 [200/800 (25.0%)]\tLoss: 0.2522\n",
      "Train Epoch: 13 [400/800 (50.0%)]\tLoss: 0.3524\n",
      "Train Epoch: 13 [600/800 (75.0%)]\tLoss: 0.4827\n",
      "Train Epoch: 13 [800/800 (100.0%)]\tLoss: 0.1226\n",
      "Done epoch #13, time for this epoch: 54.467167139053345s\n",
      "Start epoch #14, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 14 [200/800 (25.0%)]\tLoss: 0.2307\n",
      "Train Epoch: 14 [400/800 (50.0%)]\tLoss: 0.5096\n",
      "Train Epoch: 14 [600/800 (75.0%)]\tLoss: 0.1178\n",
      "Train Epoch: 14 [800/800 (100.0%)]\tLoss: 0.2844\n",
      "Done epoch #14, time for this epoch: 54.61764097213745s\n",
      "Start epoch #15, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 15 [200/800 (25.0%)]\tLoss: 0.1659\n",
      "Train Epoch: 15 [400/800 (50.0%)]\tLoss: 0.3575\n",
      "Train Epoch: 15 [600/800 (75.0%)]\tLoss: 0.2491\n",
      "Train Epoch: 15 [800/800 (100.0%)]\tLoss: 0.1167\n",
      "Done epoch #15, time for this epoch: 54.07396411895752s\n",
      "Start epoch #16, learning rate for this epoch: [4.32e-05]\n",
      "Train Epoch: 16 [200/800 (25.0%)]\tLoss: 0.2750\n",
      "Train Epoch: 16 [400/800 (50.0%)]\tLoss: 0.0887\n",
      "Train Epoch: 16 [600/800 (75.0%)]\tLoss: 0.2850\n",
      "Train Epoch: 16 [800/800 (100.0%)]\tLoss: 0.3836\n",
      "Done epoch #16, time for this epoch: 53.97117614746094s\n",
      "Start epoch #17, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 17 [200/800 (25.0%)]\tLoss: 0.4267\n",
      "Train Epoch: 17 [400/800 (50.0%)]\tLoss: 0.2536\n",
      "Train Epoch: 17 [600/800 (75.0%)]\tLoss: 0.4859\n",
      "Train Epoch: 17 [800/800 (100.0%)]\tLoss: 0.2506\n",
      "Done epoch #17, time for this epoch: 54.263980865478516s\n",
      "Start epoch #18, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 18 [200/800 (25.0%)]\tLoss: 0.4482\n",
      "Train Epoch: 18 [400/800 (50.0%)]\tLoss: 0.2259\n",
      "Train Epoch: 18 [600/800 (75.0%)]\tLoss: 0.3861\n",
      "Train Epoch: 18 [800/800 (100.0%)]\tLoss: 0.2395\n",
      "Done epoch #18, time for this epoch: 54.53084897994995s\n",
      "Start epoch #19, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 19 [200/800 (25.0%)]\tLoss: 0.2865\n",
      "Train Epoch: 19 [400/800 (50.0%)]\tLoss: 0.1330\n",
      "Train Epoch: 19 [600/800 (75.0%)]\tLoss: 0.3014\n",
      "Train Epoch: 19 [800/800 (100.0%)]\tLoss: 0.3569\n",
      "Done epoch #19, time for this epoch: 54.56373739242554s\n",
      "Start epoch #20, learning rate for this epoch: [2.592e-05]\n",
      "Train Epoch: 20 [200/800 (25.0%)]\tLoss: 0.2371\n",
      "Train Epoch: 20 [400/800 (50.0%)]\tLoss: 0.2338\n",
      "Train Epoch: 20 [600/800 (75.0%)]\tLoss: 0.2265\n",
      "Train Epoch: 20 [800/800 (100.0%)]\tLoss: 0.4186\n",
      "Done epoch #20, time for this epoch: 54.90722727775574s\n",
      "Start epoch #21, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 21 [200/800 (25.0%)]\tLoss: 0.2624\n",
      "Train Epoch: 21 [400/800 (50.0%)]\tLoss: 0.2421\n",
      "Train Epoch: 21 [600/800 (75.0%)]\tLoss: 0.4160\n",
      "Train Epoch: 21 [800/800 (100.0%)]\tLoss: 0.3581\n",
      "Done epoch #21, time for this epoch: 53.9772584438324s\n",
      "Start epoch #22, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 22 [200/800 (25.0%)]\tLoss: 0.5825\n",
      "Train Epoch: 22 [400/800 (50.0%)]\tLoss: 0.4163\n",
      "Train Epoch: 22 [600/800 (75.0%)]\tLoss: 0.2499\n",
      "Train Epoch: 22 [800/800 (100.0%)]\tLoss: 0.1712\n",
      "Done epoch #22, time for this epoch: 54.35048842430115s\n",
      "Start epoch #23, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 23 [200/800 (25.0%)]\tLoss: 0.1316\n",
      "Train Epoch: 23 [400/800 (50.0%)]\tLoss: 0.2718\n",
      "Train Epoch: 23 [600/800 (75.0%)]\tLoss: 0.2603\n",
      "Train Epoch: 23 [800/800 (100.0%)]\tLoss: 0.3538\n",
      "Done epoch #23, time for this epoch: 54.20731449127197s\n",
      "Start epoch #24, learning rate for this epoch: [1.5552e-05]\n",
      "Train Epoch: 24 [200/800 (25.0%)]\tLoss: 0.3815\n",
      "Train Epoch: 24 [400/800 (50.0%)]\tLoss: 0.4791\n",
      "Train Epoch: 24 [600/800 (75.0%)]\tLoss: 0.1008\n",
      "Train Epoch: 24 [800/800 (100.0%)]\tLoss: 0.2388\n",
      "Done epoch #24, time for this epoch: 54.419753551483154s\n",
      "Start epoch #25, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 25 [200/800 (25.0%)]\tLoss: 0.0896\n",
      "Train Epoch: 25 [400/800 (50.0%)]\tLoss: 0.1735\n",
      "Train Epoch: 25 [600/800 (75.0%)]\tLoss: 0.3545\n",
      "Train Epoch: 25 [800/800 (100.0%)]\tLoss: 0.2335\n",
      "Done epoch #25, time for this epoch: 54.68810677528381s\n",
      "Start epoch #26, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 26 [200/800 (25.0%)]\tLoss: 0.4102\n",
      "Train Epoch: 26 [400/800 (50.0%)]\tLoss: 0.5027\n",
      "Train Epoch: 26 [600/800 (75.0%)]\tLoss: 0.1137\n",
      "Train Epoch: 26 [800/800 (100.0%)]\tLoss: 0.3905\n",
      "Done epoch #26, time for this epoch: 54.13709020614624s\n",
      "Start epoch #27, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 27 [200/800 (25.0%)]\tLoss: 0.3827\n",
      "Train Epoch: 27 [400/800 (50.0%)]\tLoss: 0.2621\n",
      "Train Epoch: 27 [600/800 (75.0%)]\tLoss: 0.1376\n",
      "Train Epoch: 27 [800/800 (100.0%)]\tLoss: 0.2230\n",
      "Done epoch #27, time for this epoch: 54.21847414970398s\n",
      "Start epoch #28, learning rate for this epoch: [9.3312e-06]\n",
      "Train Epoch: 28 [200/800 (25.0%)]\tLoss: 0.4028\n",
      "Train Epoch: 28 [400/800 (50.0%)]\tLoss: 0.4372\n",
      "Train Epoch: 28 [600/800 (75.0%)]\tLoss: 0.3516\n",
      "Train Epoch: 28 [800/800 (100.0%)]\tLoss: 0.4427\n",
      "Done epoch #28, time for this epoch: 54.193519115448s\n",
      "Start epoch #29, learning rate for this epoch: [5.59872e-06]\n",
      "Train Epoch: 29 [200/800 (25.0%)]\tLoss: 0.2240\n",
      "Train Epoch: 29 [400/800 (50.0%)]\tLoss: 0.3867\n",
      "Train Epoch: 29 [600/800 (75.0%)]\tLoss: 0.2123\n",
      "Train Epoch: 29 [800/800 (100.0%)]\tLoss: 0.5175\n",
      "Done epoch #29, time for this epoch: 55.64640498161316s\n",
      "Start epoch #30, learning rate for this epoch: [5.59872e-06]\n",
      "Train Epoch: 30 [200/800 (25.0%)]\tLoss: 0.4459\n",
      "Train Epoch: 30 [400/800 (50.0%)]\tLoss: 0.2192\n",
      "Train Epoch: 30 [600/800 (75.0%)]\tLoss: 0.1529\n",
      "Train Epoch: 30 [800/800 (100.0%)]\tLoss: 0.2681\n",
      "Done epoch #30, time for this epoch: 54.866888761520386s\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project = \"Polyps_Segmentation_3\"\n",
    ")\n",
    "# Training loop\n",
    "train_loss_array = []\n",
    "test_loss_array = []\n",
    "last_loss = 9999999999999\n",
    "for epoch in range(epochs):\n",
    "    train_loss_epoch = 0\n",
    "    test_loss_epoch = 0\n",
    "    (train_loss_epoch, test_loss_epoch) = train(train_loader, \n",
    "                                              val_loader, \n",
    "                                              learing_rate_scheduler, epoch, display_step)\n",
    "    \n",
    "    if test_loss_epoch < last_loss:\n",
    "        save_model(model, optimizer, checkpoint_path)\n",
    "        last_loss = test_loss_epoch\n",
    "        \n",
    "    learing_rate_scheduler.step()\n",
    "    train_loss_array.append(train_loss_epoch)\n",
    "    test_loss_array.append(test_loss_epoch)\n",
    "    wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n",
    "#     train_accuracy.append(test(train_loader))\n",
    "#     valid_accuracy.append(test(test_loader))\n",
    "#     print(\"Epoch {}: loss: {:.4f}, train accuracy: {:.4f}, valid accuracy:{:.4f}\".format(epoch + 1, \n",
    "#                                         train_loss_array[-1], train_accuracy[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2e78fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:49.648137Z",
     "iopub.status.busy": "2023-11-17T11:57:49.647213Z",
     "iopub.status.idle": "2023-11-17T11:57:49.651751Z",
     "shell.execute_reply": "2023-11-17T11:57:49.650856Z"
    },
    "papermill": {
     "duration": 0.043227,
     "end_time": "2023-11-17T11:57:49.653857",
     "exception": false,
     "start_time": "2023-11-17T11:57:49.610630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8313f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:49.726265Z",
     "iopub.status.busy": "2023-11-17T11:57:49.725296Z",
     "iopub.status.idle": "2023-11-17T11:57:49.729612Z",
     "shell.execute_reply": "2023-11-17T11:57:49.728870Z"
    },
    "papermill": {
     "duration": 0.04209,
     "end_time": "2023-11-17T11:57:49.731364",
     "exception": false,
     "start_time": "2023-11-17T11:57:49.689274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load_model(model, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "673abe7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:49.802161Z",
     "iopub.status.busy": "2023-11-17T11:57:49.801423Z",
     "iopub.status.idle": "2023-11-17T11:57:49.806137Z",
     "shell.execute_reply": "2023-11-17T11:57:49.805171Z"
    },
    "papermill": {
     "duration": 0.041414,
     "end_time": "2023-11-17T11:57:49.808150",
     "exception": false,
     "start_time": "2023-11-17T11:57:49.766736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 90\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "epochs_array = range(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bece9e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:49.881520Z",
     "iopub.status.busy": "2023-11-17T11:57:49.880907Z",
     "iopub.status.idle": "2023-11-17T11:57:50.264707Z",
     "shell.execute_reply": "2023-11-17T11:57:50.263676Z"
    },
    "papermill": {
     "duration": 0.422196,
     "end_time": "2023-11-17T11:57:50.266941",
     "exception": false,
     "start_time": "2023-11-17T11:57:49.844745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFiCAYAAADIhcACAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA3XAAAN1wFCKJt4AABBUUlEQVR4nO3deVxU9f4/8NfAwCDMwLAMmygqiIIbmphruaS5lVbmlje1tFDBvF3TckFKJM1rarlmmlbaYvdnWS5Fpn2l654UgSmogKDs67APnN8fXE6QIAzMBryej8c8Gg5nec/p1IvP53zmcySCIAggIiIiozMzdgFERERUhaFMRERkIhjKREREJoKhTEREZCIYykRERCaCoUxERGQiGMpEREQmgqFMRERkIhjK1KpJJJIGX2fOnGnSvhMSEiCRSPDdd99ptd2ZM2cgkUjwxx9/NOm4pmjKlCkYPnx4nb+rPk8NvRISEppVwzvvvNOof5et8fxT6yE1dgFE+nTu3DnxfXFxMUaOHIlVq1ZhwoQJ4nI/P78m7dvNzQ3nzp1D9+7dtdquX79+OHfuHLy8vJp03Jam+jxVu3XrFp577jls374d/fr1q7Vec7zzzjsICgqq948DopaAoUyt2sCBA8X3arUaAODl5VVreU0VFRWoqKiApaVlg/uWyWT17udBbG1tm7RdS/X38ySXywFU/THUls4DUWOw+5ratDlz5qB///74+uuv0aNHD1hZWeHChQu4d+8eXnjhBXTp0gXt2rWDj48PVq1ahbKyMnHburqvO3XqhKVLl2Lz5s3w8PCAvb09pk+fjtzcXHGdurpPJRIJtm7dihUrVkClUsHZ2RmLFi1CaWlprXrPnDmD3r17w8rKCgEBAbh48SKcnJwQGhr6wM+5adMmBAQEwM7ODi4uLnjiiScQHx9fa53hw4djypQpOHToELy9vWFra4tx48YhOTm51np37tzB+PHj0a5dO3Tq1AkffvhhY093vSorK7F+/Xp4e3tDJpPBx8cHBw4cqLVOZGQkhg0bBltbW9ja2sLf3x+HDx8GUHXes7Ky8OabbzbptkRRUREWL14MV1dX8dz+8MMPjT4+ABw9ehQPPfQQbGxsYG9vj4cffhg///xz008KtUlsKVObl5CQgGXLliEkJASurq7o3LkzMjMz4eDggHfffRf29va4ceMGQkNDkZGRgd27dz9wf19++SV69+6NDz74AMnJyXj11VexYsUK7Nix44Hbbdq0CSNHjsSnn36K33//HW+88QY8PT2xbNkyAEBKSgrGjx+PwYMHIzw8HKmpqXjuuedQXFzc4GdMTk5GUFAQPD09kZ+fj127dmHw4MGIi4uDnZ2duN6FCxdw9+5dbNq0CcXFxXjllVfw0ksv4fjx4wAAQRAwadIkZGZmYu/evbCyssKaNWuQnZ2Nrl27NlhHfYKDg3HgwAGEhISgX79+iIiIwAsvvABHR0dMnDgR+fn5mDhxIiZNmoSQkBAIgoDo6Gjxj50jR45gxIgRmDJlCubNmwdAu9sS8+fPx9GjRxEeHg5vb2/s2bMHEyZMwOnTpzF06NAGj3/z5k1MmTIFr7zyCjZu3IiSkhJcuXIF2dnZTT4n1EYJRG1EQUGBAED46KOPxGWzZ88WAAhXr1594Lbl5eXCwYMHBZlMJpSWlgqCIAi3b98WAAjffvutuJ6np6fQpUsXoby8XFz2yiuvCC4uLuLPp0+fFgAI0dHR4jIAwrBhw2odc9KkScLDDz8s/rx06VLB0dFRKCoqEpd98cUXAgBhzZo1jToHgiAIGo1GKCoqEuRyuXDgwAFx+aOPPirY2toK2dnZ4rLNmzcLAMRjHjt2TAAgnD9/XlwnISFBMDc3Fx599NFGHT86OloAIJw+fVoQBEGIi4sTJBKJsH///lrr/eMf/xD69+8vCIIgXLp0SQAg5Ofn17tfR0fHRp2Hv5//2NjY+45fUVEh9OjRQxgzZkyjjn/48GHBwcGhwWMTNYTd19TmtW/fHv7+/rWWCYKALVu2wM/PD+3atYOFhQWee+45lJaWIikp6YH7GzFiBKTSvzqh/Pz8kJ6ejvLy8gduN2bMmFo/+/n51eo6vnTpEkaPHo127dqJy5588smGPh4A4Pz58xg9ejQcHR0hlUphbW0NtVqNGzdu1FovICAA9vb2tWoAqlrpAHDx4kW4uLjg4YcfFtfx9PTEQw891Kg66nLq1CmYmZnhqaeegkajEV+jRo1CVFQUKioq4OXlBblcjpkzZ+Kbb76pdTuguS5dugRBEPDss8+Ky8zMzPDss88iMjISABo8fq9evZCXl4fZs2fjhx9+QGFhoc7qo7aFoUxtnouLy33LtmzZgqVLl+Kpp57CN998g4sXL2L79u0AgJKSkgfuT6lU1vrZ0tISgiDcd3+4MdvVPFZqaipUKlWtdaysrMSBU/VJSkrCmDFjIAgCdu/ejV9++QWXLl2Cs7PzfZ+lrhqAvz5zamoqnJ2d7ztGXcsaKzMzExUVFbCzs4OFhYX4mjNnDjQaDe7duwd7e3tERESgvLwcU6dOhUqlwoQJE3Dr1q0mH7favXv3IJfLYW1tXWu5i4sLioqKUFpa2uDxu3Xrhm+++Qa3bt3C+PHj4eTkhJkzZyIjI6PZ9VHbwnvK1OZJJJL7lh0+fBhTpkzBunXrxGWxsbGGLOs+rq6u9/1PvqSkRBxVXp+TJ0+iqKgI33zzDWxsbAAAGo2mSfc7XV1dkZ6eft/y9PT0Wi14bTg4OEAqleKXX36Bmdn97YTqwB84cCBOnjyJ4uJi/Pjjj3j11Vcxc+ZMnD9/vknHrebm5ga1Wo2ioqJawZyWlgZra2vIZLJGHX/ChAmYMGEC8vLycOzYMSxZsgTBwcH4/PPPm1UftS1sKRPVobi4WPyfcbWDBw8aqZoqAQEBiIiIqDWw6+jRow1uV1xcDDMzs1pd6l9++SU0Gk2TakhLS8OFCxfEZUlJSfj111+13le1kSNHoqKiAnl5eejfv/99r79/Pa1du3Z44okn8MILL9T6Q+nvPQvafCaJRIKvvvpKXCYIAr766isMHTr0vvXrO341Ozs7zJw5E0899ZTR/5CjloctZaI6jB49Gu+99x4efvhheHl54eDBg/d9hcjQlixZgu3bt+OJJ57AP//5T6SmpmL9+vWwtraus4VZrTr05s6dixdffBExMTH497//fV9XdWOMHz8effr0wbPPPosNGzZAJpNhzZo1zeq+7tatGwIDAzF9+nQsW7YM/fv3R0lJCWJiYnDjxg18+OGHOHbsGPbt24fJkyejY8eOSElJwe7duzFy5EhxP927d8exY8cwduxYyOVydOvWDQqFosHj+/r6YsaMGQgKCkJBQQG8vLywZ88e/Pnnn9i5cycANHj83bt349y5cxg7dizc3d0RFxeHw4cP4/nnn2/yeaG2iaFMVIeQkBBkZGRg1apVAICnn34a7733Hp544gmj1dS+fXscO3YMr7zyCp5++mn4+vpi3759GD16NGxtbevdrlevXti/fz9CQ0Nx5MgR9OnTB4cPH8a0adO0rkEikeDo0aN46aWX8MILL8DZ2RkrVqxAREQEMjMzm/zZtm/fDh8fH+zZswchISGwtbWFn58fXnzxRQCAt7c3JBIJVqxYgfT0dKhUKkycOBHh4eHiPjZu3IhFixZhwoQJKCoqwunTpxs9u9eePXuwfPlyvPXWW8jNzUWvXr3w3XffiS3lho7fu3dvHD16FK+++iqys7Ph5uaG+fPn46233mryOaG2SSIIgmDsIoioaaontPjpp58wYsQIY5dDRM3EUCZqQZYvX46+ffvC1dUV169fx9q1a+Ho6IirV68+sAubiFoGdl8TtSClpaV47bXXkJaWBoVCgTFjxuDdd99lIBO1EmwpExERmQj+eU1ERGQiGMpEREQmgqFMRERkIlr9QK+6plAkIiIypMYO32r1oQw0/mQQERHpmjaNQ3ZfExERmQiGMhERkYlgKBMREZmINnFPmYjI2ARBgEaj4RiXVkgikUAqlepkYDFDmYhIjwRBQGZmJrKyshjIrZhUKkXnzp1rPbe8KVr9NJsSiYT/IRCR0WRkZCArKwuurq6wtrY2djmkB4IgICUlBZaWlvDw8Ljv99rkEFvKRER6Ut1KdnNzg1KpNHY5pEcuLi5ISkpCZWVlsx4Qw4FeRER6otFoAIAt5DbAwsICAFBRUdGs/TCUiYj0hLfO2p7m/js3SiiXl5cjKCgI9vb2cHBwQHBwsPgX5d+lpKRg8uTJcHR0hJOTE6ZOnYqMjAyD1ltQWoCImxE4n3zeoMclIqK2xSihHBYWhsjISMTGxiImJgZnz55FeHh4nesuWrQIAJCYmIjbt2+jpKQEixcvNmS5uJVzC2M+HYOw/wsz6HGJiFqSpKQkyOVy5OXlNWr9cePGYceOHXqpZf/+/fD399fLvvXJKKG8b98+rFq1Cm5ubnBzc8PKlSuxd+/eOte9desWpk6dCrlcDoVCgWnTpiE6Otqg9apsVACAjCLDttCJiPRNLpeLL3Nzc8hkMvHncePGabWvjh07Qq1Ww87OrlHrnzhxAgsXLmxK2a2WwUM5JycHycnJtf6C8ff3R1JSUp1/Xb366qs4fPgw8vLykJubi88++wxPPPGEASsGHNs5AgAyizINelwiIn1Tq9Xia9iwYdiwYYP484kTJ8T1OPGJYRg8lNVqNQDU+npA9fuCgoL71h8yZAjS09PF+885OTl444036t1/aGgoJBKJ+NIFmVQGW5ktMgrZUiaitkMikWDbtm3o2bMnbGxsoFar8e6776Jr165QKBTw8vLCtm3bxPUTEhIgkUiQm5sLAJgzZw7mz5+P6dOnQ6FQoFu3bjhz5oy4/vDhw7FlyxYAwJkzZ6BUKvHhhx+iQ4cOcHR0xLJly2rV8/7774u/W7VqFfz9/bF///5GfZa0tDRMnToVKpUKHTt2xMqVK8WxTNnZ2Xjqqadgb28PpVKJhx56CImJiQCAgwcPip+3ffv2WLt2bdNOZiMZ/HvKcrkcAJCXlwcnJyfxPQAoFIpa61ZWVmL06NGYOnUqIiIiAFSF7pgxY3D+fN2DrkJDQxEaGir+rKtgVlmrcDPnJko1pZBJZTrZJxG1PZ5bPJFX0rh7rs1hZ2WHxCWJzd7PoUOH8MMPP8DR0REWFhbw9PTETz/9BA8PD5w5cwbjx49H3759MWTIkDq3/+KLL3D06FEcPHgQb7/9NubMmYOEhIQ61y0oKEBsbCzi4uJw+/Zt9O/fH+PHj8fw4cNx6tQphISE4Pvvv4e/vz/CwsIQExPT6M8xc+ZMuLq64vbt28jKysL48eNhY2ODFStW4N///jc0Gg1SUlIgk8kQHR0NhUKBwsJCzJkzB6dOncIjjzyC3NxcxMXFNeU0NprBW8r29vbw8PBAVFSUuCwqKgodOnS47z5EdnY2EhMTsXjxYlhbW8Pa2hrBwcG4cOECMjMN25XM+8pE1BYtW7YM7u7ukMlkMDMzwzPPPIMOHTpAIpFgxIgRePzxx2u1fv+uOlTNzc0xd+5cJCYmIisrq851BUFAWFgYrKys4Ovri8GDB+PKlSsAqv44eO655zBgwABYWlpi9erVsLGxadRnSElJwU8//YR3330Xcrkcnp6eWLlypdjKtrCwQFZWFuLi4mBubg5/f384ODiIv7t27Rry8/OhVCoREBDQ+JPXBEaZ0Wvu3LlYt26d+JdVeHg45s2bd996Tk5O8Pb2xvbt27FmzRoAwPbt2+Hh4SG2sg1FZf2/UC7MgIft/dOoERE1hi5ar4bUsWPHWj8fPHgQmzZtQkJCAiorK1FUVITOnTvXu72rq6v4vjpECwoK4OjoeN+6tra2tSZasbGxEW9r3r17F8OHDxd/Z2FhATc3t0Z9huTkZFhZWcHFxUVc1qVLFyQnJwMAXnvtNZSUlGDq1KnIy8vDtGnTsH79etjY2ODbb7/Fpk2bsGzZMvTq1Qtr167FiBEjGnXcpjDK6OvVq1dj0KBB8PX1ha+vL4YMGYIVK1YAAAIDAxEYGCiu+8033+DXX39F+/bt4ebmhosXL+Lo0aMGr9nJuuqPAA72IqK2pOaUkUlJSZg9ezbeeecdpKenIzc3F+PHjzfIADB3d3fcuXNH/Fmj0eDevXuN2tbDwwMlJSVIS0sTlyUkJIjzVMvlcmzYsAHXr1/HuXPncOrUKfGrWqNGjcLx48eRmZmJZ599FpMnT0ZlZaUOP1ltRgllCwsLbN++HTk5OcjJycH7778vPllj165d2LVrl7iun58fvv/+e2RlZSEnJwc//fQT+vbta/CaxZYyu6+JqI1Sq9UQBAHOzs4wMzPD8ePH8cMPPxjk2DNmzMChQ4dw+fJllJeXIywsDIWFhY3atn379hgxYgSWLl2KwsJCJCUlYd26dZg9ezYA4LvvvsONGzdQWVkJW1tbWFhYQCqVIi0tDUeOHEFBQQGkUilsbW2b/RSohnCazUYS7ylzBDYRtVF+fn5YuXIlRo4cCUdHR3zxxRd48sknDXLsxx57DGvWrMHkyZPh6uoKjUYDHx8fyGSNG3h76NAhFBcXw9PTE0OGDMGECRPE0d3x8fEYO3YsFAoF/Pz8MGjQICxYsACVlZXYunWrOOZp+/bt+Oqrr5r1wImG8NGNjXQg6gDmfDMHK4etRNhIzuxFRA0rKyvDzZs34eXlBUtLS2OX06qUlZXB0dERJ0+erHfkt6Hrqe/ftTY5xJZyI7GlTERkXP/v//0/FBcXo7CwEMuXL4ejo6PeR0MbGkO5karvKWcWc6AXEZExfPLJJ3Bzc4O7uzt+/fVXHD16tNX1QBjlK1EtUfXoa7aUiYiM48iRI8YuQe/YUm4kTh5CRET6xlBuJBsLG1hJrdhSJqJGq57mt5WPp6Uamju1M7uvG0kikUBlrUJyfjIqKitgbmZu7JKIyMRJpVJIpVKkpKTAxcUFFhYWxi6J9CQzMxPm5ubN/h4zQ1kLKhsV7uTfQXZxttidTURUH4lEgs6dOyM1NRVJSUnGLof0yNzcXJwTvDkYyloQB3sVZTCUiahRpFIpPDw8UFlZiYqKCnZlt0ISiQRSqVQnTyVkKGuh5kMpwEwmIi2YmZnpdSYoah14hWiB818TEZE+MZS1wFm9iIhInxjKWhBn9eLjG4mISA8YylqoOdCLiIhI1xjKWuCsXkREpE8MZS3UGn1NRESkYwxlLbClTERE+sRQ1oLSSglziTkHehERkV4wlLVgJjGDo7UjMgozOCsPERHpHENZSyprFcory5Ffmm/sUoiIqJVhKGuJ95WJiEhfGMpa4ghsIiLSF4ayljj/NRER6QtDWUvV3dccgU1ERLrGUNaSONUmu6+JiEjHGMpaYvc1ERHpC0NZSxx9TURE+sJQ1hJHXxMRkb4wlLXEgV5ERKQvDGUtObZzBMDuayIi0j2GspYszC2gtFKy+5qIiHSOodwEKmsVCssLUVxebOxSiIioFWEoNwFHYBMRkT4wlJugegQ2B3sREZEuMZSbgLN6ERGRPjCUm4CzehERkT4YJZTLy8sRFBQEe3t7ODg4IDg4GBqNps515XJ5rZeFhQV69+5t4IprE+8ps6VMREQ6ZJRQDgsLQ2RkJGJjYxETE4OzZ88iPDy8znXVanWtl6+vL6ZPn27gimtjS5mIiPTBKKG8b98+rFq1Cm5ubnBzc8PKlSuxd+/eBre7ePEiYmNjMWfOHP0X+QCc1YuIiPTB4KGck5OD5ORk+Pv7i8v8/f2RlJSEvLy8B267d+9ejBs3Du7u7nqu8sHYUiYiIn0weCir1WoAgFKpFJdVvy8oKKh3u8LCQnz++eeYN2/eA/cfGhoKiUQivvSBo6+JiEgfDB7KcrkcAGq1iqvfKxSKerc7fPgwrK2tMWHChAfuPzQ0FIIgiC994OQhRESkDwYPZXt7e3h4eCAqKkpcFhUVhQ4dOsDOzq7e7T788EPMnj0bUqnUAFU+mLWFNawtrNlSJiIinTLKQK+5c+di3bp1SE1NRWpqKsLDwx/YLX39+nX897//xYsvvmjAKh9MZa1CTkkONJV1f5WLiIhIW0Zpdq5evRpZWVnw9fUFAMyaNQsrVqwAAAQGBgIAdu3aJa6/d+9eDBs2DF27djV8sfVQ2aiQmJeIrKIsuMhdjF0OERG1AhJBXzdeTYREItHLveVxB8fhZPxJRC+IRk/nnjrfPxERtQ7a5BCn2Wwi8WtRvK9MREQ6wlBuIn5XmYiIdI2h3ESc1YuIiHSNodxE7L4mIiJdYyg3kTirF7uviYhIRxjKTcRZvYiISNcYyk3E7msiItI1hnITcaAXERHpGkO5iexkdpCaSdl9TUREOsNQbiKJRAInaydkFmXq7WlURETUtjCUm0FlrYKmUoPcklxjl0JERK0AQ7kZOAKbiIh0iaHcDNUjsDnYi4iIdIGh3Az8WhQREekSQ7kZ2H1NRES6xFBuBnGqTbaUiYhIBxjKzcDHNxIRkS4xlJuBs3oREZEuMZSbgS1lIiLSJYZyM4gDvXhPmYiIdICh3AwO7RwAsKVMRES6wVBuBqmZFA7tHNhSJiIinWAoN5PKWoViTTEKywqNXQoREbVwDOVm4ghsIiLSFYZyM3EENhER6QpDuZk4qxcREekKQ7mZ2FImIiJdYSg3E7+rTEREusJQbiY+U5mIiHSFodxMfHwjERHpCkO5mcSBXgxlIiJqJoZyM4kDvXhPmYiImomh3EzsviYiIl1hKDeTldQKcks5B3oREVGzMZR1QGWtQm5JLsoryo1dChERtWAMZR3g/NdERKQLDGUd4AhsIiLSBaOEcnl5OYKCgmBvbw8HBwcEBwdDo9HUu/7Ro0fh7+8PGxsbuLu7Y9euXQastmEcgU1ERLpglFAOCwtDZGQkYmNjERMTg7NnzyI8PLzOdU+ePImFCxdiy5YtyM/PR0xMDIYPH27YghvAWb2IiEgXjBLK+/btw6pVq+Dm5gY3NzesXLkSe/furXPd1atXIyQkBMOHD4e5uTns7e3RvXt3A1f8YPxaFBER6YLBQzknJwfJycnw9/cXl/n7+yMpKQl5eXm11i0sLMSVK1eQkpICHx8fuLq64tlnn8W9e/cMXPWDsfuaiIh0weChrFarAQBKpVJcVv2+oKCg1ro5OTkQBAFff/01IiIiEB8fD5lMhlmzZtW7/9DQUEgkEvFlCBzoRUREumDwUJbL5QBQq1Vc/V6hUNS57uLFi+Hp6Qm5XI4333wTp0+fRmFhYZ37Dw0NhSAI4ssQ2H1NRES6YPBQtre3h4eHB6KiosRlUVFR6NChA+zs7Gqtq1Qq0bFjxzr3Y6jAbQwO9CIiIl0wykCvuXPnYt26dUhNTUVqairCw8Mxb968Otd96aWX8P777yMlJQXFxcV46623MGrUKLEVbQrEljLvKRMRUTNIjXHQ1atXIysrC76+vgCAWbNmYcWKFQCAwMBAABC/i/z6668jOzsbffr0AQCMGDECn3zyiRGqrp/CUgFLc0t2XxMRUbNIBFPqB9YDiURikK7u9u+2R5o6DWWry2Am4URpRERURZscYnroiMpahQqhAjnFOcYuhYiIWiiGso7woRRERNRcDGUdEScQ4X1lIiJqIoayjnBWLyIiai6Gso5wAhEiImouhrKOiFNtsqVMRERNxFDWEc7qRUREzcVQ1hF2XxMRUXMxlHWEo6+JiKi5GMo6wvmviYiouRjKOmJvZQ8JJGwpExFRkzGUdcTczByO1o7ILMo0qcdKEhFRy8FQ1iGVtQolmhIUlhcauxQiImqBGMo6xPvKRETUHFqH8vHjxxEfHw8ASEhIwKRJk/DMM88gOTlZ58W1NByBTUREzaF1KL/66quwsrICALz22muQy+VwdHTEggULdF5cS8NZvYiIqDmk2m6QmpoKDw8PaDQa/Pjjj0hKSoJMJoO7u7s+6mtROKsXERE1h9ah3K5dO6SlpSE6Ohrdu3eHQqFAeXk5ysvL9VFfi8JZvYiIqDm0DuXnn38eAQEBKC0txZtvvgkAuHz5Mrp06aLz4loaPr6RiIiaQ+tQ3rBhAx577DFYWFhg+PDhAAALCwts2rRJ17W1OGwpExFRc2gdygAwevRo8f3Vq1dhaWmJkSNH6qyolkoc6MVQJiKiJtB69PWkSZMQGRkJANi+fTsGDx6MwYMHY9euXTovrqVh9zURETWHRNByTkhnZ2ekpKTAwsICfn5+2LNnD+zs7PDUU08hLi5OX3U2mUQiMdi0l6WaUlits4KXvRfiF8cb5JhERGTatMkhrbuvS0tLYWFhgZSUFGRnZ2PIkCEAgLS0NG131erIpDLYymzZfU1ERE2idSj7+fnh7bffRmJiIh5//HEAQHp6OmxsbHReXEukslbhZs5NlGpKIZPKjF0OERG1IFrfU96xYwe+++47/PnnnwgNDQUAfP/99xgzZoyua2uRqkdgcwIRIiLSltb3lFsaQ95TBoAnPnsC3934Dldfvgp/V3+DHZeIiEyTXu8pA8C5c+dw4MABJCcnw8PDA7Nnz8agQYOasqtWh1NtEhFRU2ndff35559jzJgxEAQBw4YNg0QiwdixY/HZZ5/po74Wh1+LIiKiptK6pRwWFoZjx47hkUceEZfNnDkTgYGBmDFjhk6La4k4qxcRETWV1i3llJQU8WtQ1QYPHoy7d+/qrKiWjC1lIiJqKq1DuUePHti9e3etZXv27IGfn5/OimrJONUmERE1ldbd11u2bMG4ceOwfft2dOrUCQkJCUhPT8eJEyf0UV+Lw69EERFRU2kdyv3790d8fDyOHTuG5ORkPPfccxg1ahQGDRqEW7du6aPGFkXsvmZLmYiItNSkr0TZ2dlh5syZ4s+lpaVISEjQVU0tmjjQi/eUiYhIS1rfU66PRCLR1a5aNBsLG1hJrdhSJiIirekslKmKRCKBk7UTsoqyUFFZYexyiIioBWl09/V7771X7+80Go1WBy0vL8c///lPHDx4EBKJBM899xw2b94MqfT+cubMmYNDhw7B0tJSXBYREWHSM4iprFVIzk9GTkmOOBqbiIioIY0O5SNHjjzw9zUnE2lIWFgYIiMjERsbCwAYN24cwsPDERISUuf6CxcuxJYtWxq9f2OreV+ZoUxERI3V6FA+ffq0zg66b98+bN68GW5ubgCAlStXYunSpfWGcktTcwS2L3yNXA0REbUUBr+nnJOTg+TkZPj7+4vL/P39kZSUhLy8vDq3+fjjj+Hg4IAePXpg06ZNqKysNFC1TcNZvYiIqCkMHspqtRoAoFQqxWXV7wsKCu5bf/Hixbh+/ToyMjKwd+9ebN26FVu3bq13/6GhoZBIJOLLGDirFxERNYXBQ1kulwNArVZx9XuFQnHf+v369YNKpYK5uTkGDhyI119/HV988UW9+w8NDYUgCOLLGDirFxERNYXBQ9ne3h4eHh6IiooSl0VFRaFDhw6ws7NrcHszM9P/Fhe7r4mIqCmMknBz587FunXrkJqaitTUVISHh2PevHl1rvvll18iPz8fgiDg8uXLWL9+PZ555hkDV6wdPr6RiIiaoknTbDbX6tWrkZWVBV/fqpHJs2bNwooVKwAAgYGBAIBdu3YBALZt24aXXnoJGo0G7du3x8KFC/Gvf/3LGGU3Gue/JiKippAIxrrxaiASicTg95azi7Ph+I4j+rj0QVRglEGPTUREpkWbHDL9G7QtkNJKCXOJOQd6ERGRVhjKemAmMYOjtSMyijKMNgKciIhaHoaynqisVSirKENB2f3fvSYiIqoLQ1lP+FxlIiLSFkNZTzgCm4iItMVQ1pPqqTY52IuIiBqLoawnnNWLiIi0xVDWE87qRURE2mIo6wlbykREpC2Gsp6wpUxERNpiKOsJB3oREZG2GMp6Ut19nVKQYuRKiIiopWAo64mzjTM6KTshKjUK/4n9j7HLISKiFoChrCfmZub4YOIHAIAFxxZwwBcRETWIoaxHo71G46V+LyGjKANBJ4KMXQ4REZk4Pk9Zz/JL89FrZy8k5SXh8LOHMcVvitFqISIiw+PzlE2IrcwWe5/cCwBYeGwhu7GJiKheDGUDeKzLY3j5oZeRUZSBRccXGbscIiIyUey+NpCC0gL02tkLiXmJ+HLKl3i2x7PGLomIiAyA3dcmSCFT/NWNfXwh0gvTjVwRERGZGoayAY3qMgqBDwUisyiT3dhERHQfdl8bWM1u7C+mfIGpPaYauyQiItIjdl+bMIVMgX2T9gEAFh1fxG5sIiISMZSNYGTnkVjQfwEyizKx8NhCk2rJExGR8bD72kjUZWr02tkLCbkJ+PyZzzGt5zRjl0RERHrA7usWQG4pF0djLzq+CGnqNCNXRERExsZQNqLqbuys4iwsPM5ubCKito7d10ZWsxv7s2c+w/Se041dEhER6RC7r1sQuaUc+56sGo0ddDyI3dhERG0YQ9kEjOg8Agv7L0RWcRYWHFtg0i17IiLSH3Zfmwh1mRq9d/bG7dzbOPT0IczoNcPYJRERkQ6w+7oFklvKxUlFZn89G8sjlqOgtMDIVRERkSExlE3I8E7DsW3cNliYW+Cd/74Dn20+OBB1AJVCpbFLIyIiA2D3tQlKzk/G8h+X41D0IQDAw+0fxnvj3sOA9gOMXBkREWlLmxxiKJuwX5J+weKTi/HrvV8BAHP85+DtUW/DVe5q5MqIiKixGMo1tORQBoCKygrsj9qPN069gYyiDCgsFVj9yGq8MvAVWJpbGrs8IiJqAEO5hpYeytXySvLw1s9v4b2L70FTqUFXh67Y/PhmTPCZYOzSiIjoAUx+9HV5eTmCgoJgb28PBwcHBAcHQ6PRPHCb4uJieHt7Q6lUGqZIE2NnZYdNj29C9IJojPUei7jsOEz8bCLGHxyP65nXjV0eERHpgFFCOSwsDJGRkYiNjUVMTAzOnj2L8PDwB24TEhICT09PA1Vouro7dcfxmcfx7Yxv4e3gjRPxJ9BzZ0+sPLWyVfQIEBG1ZUYJ5X379mHVqlVwc3ODm5sbVq5cib1799a7/pUrV3Dy5EksX77cgFWaLolEgok+E/HHgj+w4bENsJJaITwyHB9FfWTs0oiIqBkMfk85JycHDg4OiIuLg7e3NwAgLi4OPj4+yM3NhZ2dXa31NRoNBgwYgC1btqCyshKTJ09Gbm5uo4/XWu4pP8i5O+cw9KOhsLGwQfSCaHgq2aNARGQqTPqeslqtBoBa94ar3xcU3D+D1caNG9G3b1888sgjjdp/aGgoJBKJ+GoLBnUYhGWDl6GgrAAvHH2Bk40QEbVQBg9luVwOAMjLyxOXVb9XKBS11o2Pj8euXbuwcePGRu8/NDQUgiCIr7YidHgoejr3xE+3f8KOSzuMXQ4RETWBwUPZ3t4eHh4eiIqKEpdFRUWhQ4cO93VdR0ZGIi0tDT4+PnBycsKkSZOQn58PJycnXLhwwcCVmzaZVIaPJ38MqZkUyyKWIS4rztglERGRlozyPeWQkBB89913OH78OABg/PjxmDx5MkJCQmqtV1RUhOzsbPHnc+fOYd68eYiJiYGzszMsLRuePKMt3FOuae3PaxFyJgSDPAbh7NyzMDczN3ZJRERtmjY5JNVzLXVavXo1srKy4OvrCwCYNWsWVqxYAQAIDAwEAOzatQvW1tawtrYWt1OpVJBIJPDw8DB80S3E60Nfx9EbR3Eu+Rw2nduEZUOWGbskIiJqJM7o1QrFZsSi3+5+ECDgyktX0NO5p7FLIiJqs0x69DXpn5/KD+tGrkNZRRmeP/I8yivKjV0SERE1AkO5lVoycAmGdhyKq6lXse7sOmOXQ0REjcDu61bsZvZN9N7VG6WaUlyYdwEPuT9k7JKIiNocdl8TAMDLwQv/Hv1vVAgVeP7r51GiKTF2SURE9AAM5VYusH8gRncZjdiMWIScDml4AyIiMhp2X7cBd/LuoOfOnigoLcDZuWcxpOMQY5dERNRmsPuaaulg1wHvjX0PAgTM/no2CssKjV0SERHVgaHcRjzf53k82e1J3My5ieU/8hGYRESmiN3XbUiqOhU9d/REVnEWIv4Rgce6PGbskoiIWj12X1OdXOWu2DVxFwDghW9eQF5JXgNbEBGRITGU25gpflMwo+cM3Mm/g8UnF6OissLYJRER0f+w+7oNyi7ORo8dPZCqToWnnSdeeuglvND3BbjKXY1dGhFRq6NNDjGU26grd69g+Y/Lcer2KQCA1EyKp7o/hcD+gRjRaQQkEomRKyQiah0YyjUwlB/sRtYNfHDlA3wU9RGyi6ueXe3j6IPAhwIx2382HNo5GLlCIqKWjaFcA0O5cUo0Jfgq9ivsurwLv9z5BQAgM5dhWs9pCHwoEAM9BrL1TETUBAzlGhjK2otOi8auy7vwye+foKCsAADQ26U3Ah8KxHO9n4OtzNbIFRIRtRwM5RoYyk2nLlPjs+jPsPPyTlxNvQoAkFvK8fJDL+PVQa/CXeFu5AqJiEwfQ7kGhnLzCYKAy3cvY+flnTgUfQilFaWwNLfEXP+5WDZkGbrYdzF2iUREJouhXANDWbdS1anYcn4LdlzagYKyAphJzDCj5wy8PvR19HTuaezyiIhMDkO5BoayfuQU52D7pe3Ycn4LsoqzAABPdnsSK4auwMMeDxu5OiIi08FQroGhrF+FZYX48NcPsfG/G5FSkAIAGNl5JFYMXYGRnUc2asS2IAhIK0zDjawbuJF1A9czr+Oe+h5m9JyBCT4T9P0RiIj0iqFcA0PZMEo1pfj090+x/pf1iM+OBwAMaD8Abwx9A092exJmEjPkl+YjLivur/DNui6+rx7l/Xczes7A1rFbobJRGfLjEBHpDEO5BoayYVVUVuCr2K8QHhmO39N+BwB0UnZCiaYEqerUOreRW8rh4+hT9XKo+icAvH7qdSTnJ8PJ2gnvjX0P03tO53eliajFYSjXwFA2DkEQcDzuOMIjw/HfO/+F1EwKL3uvv8LX0QfdHLvBx9EHrnLXOsM2vzQfyyOWY9eVqidbTfSZiJ0TdsLD1sPQH4eIqMkYyjUwlI1LEARkFGXAoZ0DpGbSJu3j54SfMe/beYjPjoetzBYbR2/EvH7zYCbhQ86IyPQxlGtgKLcOxeXFCD0Tin+f+zcqhUoM7zQce57YA28Hb2OXRkT0QAzlGhjKrcvlu5fxwjcvIDo9GlZSK6wdsRZLBi5pciuciEjfGMo1MJRbn7KKMrzzyztY+39rUVZRhgD3AOx9ci96ufQydmlERPdhKNfAUG69YjNi8eLRF3E++TykZlKsGLoCM3rNgMxcBplUBpm5DFZSK8ikMrakichoGMo1MJRbt4rKCmy7uA0rflqBovKietczk5jVCunq4LaSWiHAPQCB/QPR372/ASsnoraCoVwDQ7ltuJ1zG2v/by1SClJQqilFaUUpSjWlKNGU1Pm+vLL8vn0EuAdgQf8FmNZzGqwtrI3wKYioNWIo18BQprpUCpUo1ZSioKwAh2MOY+flnYjJiAEAKK2UmOs/F4H9A8WJTIiImoqhXANDmRpDEAREJkVix+Ud+E/sf8SW9KjOo7AwYCGe7PYk70sTUZMwlGtgKJO20tRp2Hd1H3Zf2Y3EvEQAgLvCHfP7zcf8fvPR3ra9kSskopaEoVwDQ5maqqKyAifiT2Dn5Z04EXcCAgSYS8wxqfskLHl4CYZ5DjN2iUTUAjCUa2Aoky7czrmN3Vd2Y+/VvcgsygRQNRf3hsc2wE/lZ+TqiMiUMZRrYCiTLpVqSvFFzBcIPROK27m3YSYxw7y+8xA6PBRuCjdjl0dEJkibHDLKjP7l5eUICgqCvb09HBwcEBwcDI1GU+e6wcHB6NChA2xtbdG+fXssWbIEZWVlBq6YqIpMKsPzfZ7HtUXX8O6Yd2Ens8MHv36Aru93ReiZUKjL1MYukYhaMKOEclhYGCIjIxEbG4uYmBicPXsW4eHhda67cOFC/Pnnn8jPz8dvv/2G3377De+8846BKyaqTSaV4Z+D/ombi29i6aClKK8sx5s/v4mu73fFB1c+gKay7j8ytVFRWYHs4myUakp1UDERtQRG6b7u0KEDNm/ejClTpgAADh8+jKVLlyIxMfGB22VkZGD69Onw8PDAgQMHGnUsdl+TISTkJmDVT6twMPogAMDXyRcbHtuAiT4T63xWdF0yCjNwIeUCzt05h/Mp53Ex5aLY8rY0t4TCUgGFTFH3Py0VsJXZQiFTwE3uhsndJ0MhU+jt8xJR45n0PeWcnBw4ODggLi4O3t5Vj92Li4uDj48PcnNzYWdnd98269evR1hYGAoLC+Ho6IiTJ0+if//GTYnIUCZDunL3Cl6LeA2nE04DAB71fBQbR29EQPuAWuuVV5QjOj1aDODzyecRnx1fax0bCxt4OXihqLwIBaUFKCgreOBUojUpLBV4vs/zWBSwCL4qX918OCJqEpMO5Tt37qBjx47IyMiAk5MTgKoWsLOzM+7cuQMPD496t7127RoOHjyIwMDAetcLDQ3Fm2++WWsZQ5kMSRAEHI87jmU/LkNsRiwAYHrP6Xi6+9O4fPcyzqecx6WUSyjWFNfarptjNwz0GIhBHoMw0GMgejj3uG/CkorKCqjL1CgoK0BBaQHyS/PF99X/vHLvCj774zOUaEoAVE2AEjQgCBN9JnICFCIjMOlQrm4px8fHw8vLCwAQHx+Prl271ttSrunw4cPYvXs3fvzxx0Ydjy1lMhZNpQb7o/Yj5HQI7qnv1fqdrcwWD7d/WAzgAe0HwNHaUWfHzirKwr6r+7Dj8g4k5CYAADradcSC/gvwYt8XobJRNfsYReVFnCOcqBFMOpSBqnvKW7ZswTPPPAMA+Oqrr/Dqq68iKSmpwW0PHTqEN954o8H7z9UYymRshWWF2H5pO27l3EKAewAGegyEr8oXZhL9j7OsqKzA8bjj2HZpG364+QMAQGYuw7Se0xAUEHRft3pdMgozEJMRg5j0GMRkxCA2IxYxGTHILMqEj6MPxnuPx/iu4/GI5yOQSWX6/khELY7Jh3JISAi+++47HD9+HAAwfvx4TJ48GSEhIbXWU6vVOHz4MJ566inY2dnhjz/+wLRp0zB06FB88MEHjToWQ5moyvXM69hxaQf2/7Yf+aX5AIAB7QcgKCAIz/Z4FuoytRi8NQM4oyjjvn1ZmlvCTe4mTkMKVN0DH9VllBjSHew6GOyzEZkykw/l8vJyLFmyBIcOHQIAzJo1C5s3b4ZUKkVgYCAAYNeuXSgsLMTkyZPx66+/orS0FM7OznjmmWfw5ptvwtq6cd1mDGWi2gpKC/Dp759i+6Xt4pOxLMws6nycpaW5Jbo5dkMP5x7ooeoBP5Ufeqh6wMvBC1IzKZLzk3Ei7gSOxx9HxM0IFJYXitv2cu6F8V2rAnqQxyBYmFsY7DM2RWJuIvJL8+Hj6MMWP+mUyYeyITGUieomCAJ+TvwZ2y5uQ8StCHS064geqhrh69wD3g7ejR4cVqopRWRSJI7HHcexuGO4nnVd/J2dzA5jvMZgnPc4+Kn84GzjDGcbZ9hY2ujr4zWoqLwIPyf8jJPxJ3Hy5kncyLoBADCXmMPbwfu+P0QY1tRUDOUaGMpExnEz+yZOxJ/A8bjjOJ1wWhwNXpO1hTVU1ioxpOt7tVe0h5O1U6O/810XQRBwLfNaVQjHn8T/Jf4fSiv+mpilp3NPuNi44FrmNdwtuHvf9uYSc3R17CqGdA9VD/RwrgprS3PLJtdFrR9DuQaGMpHxFZUX4fTt0/jx1o9IKUhBemG6+MoqzmrUPqwtrNHRriM87TzhaeeJTspO8FRWvfdUesJN7gZzM/Na2+SW5OLUrVNiazg5P1n8ndJKidFdRmOs91iM8RoDD9u/vmaZU5yD2IxYcVBb9X32v4+iB6q6+Hu79EaAe0DVq30AfJ1876uF2i6Gcg0MZSLTpqnUILMos1ZQZxRmiO/TCtNwJ/8OEnMTkVOSU+9+pGZSdLDtAE+lJzradcTN7Js4n3weFUIFAEACCQa0H4DHvR7HWO+xCGgfoPX3tqvDuuYo9KjUKPHJYdVsLGzQz62fGNIB7gHoYt+lWS19arkYyjUwlIlaj4LSAiTmJSIxNxEJuQlV7//3c2JeIlLVqbXWd5W7Yqz3WDzu9ThGdxmt0++CVxMEAYl5ibiUcgmX7la9rty9goKyglrrObRzQH/3/ujv1h8PuT8EH0cfeNl7oZ1FO53Wk1mUiWsZ13At8xru5N1BJ2Un9HLphR6qHka9h9+WMZRrYCgTtR0lmhLcybuDxLxEqKxV6O3S2yit00qhEtczr1eF9P/COio1qtY97Goeth7o6tAVXR26wtvBG10dq/75oMCuFCqRlJckhu+fmX/iWuY1XMu4Vu/tAAkk6GLfBb1ceqGnqid6ufRCL+de6OrYtcXM9FZRWYHcktz7ZrF74D/LCmArs8Vwz+EY1WUUOtp1NHjdDOUaGMpEZArKKsrwR/ofuJRyCb+n/Y647DjEZ8cjMS8RlUJlndt0sO1QFdQOXeFs44ybOTdxLfMarmdev2+aVqBqYhgfRx/4qnzR3bE7Otp1xK2cW4hOj8Yf6X/gdu7tOrfxVfmil3NVSPdw7gEbCxsIEMT/d1a/F/C/n//2HgBsLG2gtFLCTmYHpZUScku51n8QFZUXiX9UVfd+JOUliT8n5yeLtyOaytvBG6M6j8LIziMxotMIncxu1xCGcg0MZSIyZaWaUiTkJiAuOw5xWVVB3VBgK62U8HXyRXen7vB18q0KYafu6Kzs/MABZgWlBYjJiEF0WjSi0//3Sotu9GA7bZhJzGqFdPXLzsoOSlnVP/NL8/8K3tzEOieqqSaBBG4KNzjbON/3hLR6n54mU0BuKcfdgrv46fZPOHX7FP5I/6PWfvu49BFD+hHPR/TydDWGcg0MZSJqqWoGdpo6DV3su8BX5QsXGxeddcsLgoBUdarYmr6WcQ1llWUAqoJQIpFAAkmtn//+HgAKywuRW5KLvJI85Jbkiq+aE8o8iKW5pTi6Xhxl/7/R9R3tOsLD1kMn3xNPU6fhdMJpnLp1Cqdun6rVeyA1k2JA+wEY2WkkRnUZhUEeg3RyTIZyDQxlIiLjKa8oR35pvhjSeaV/hbbcUi6Gr7ONs0Hmg/+72zm3xVb0T7d/Qlphmvi7s3PPYmjHoc0+BkO5BoYyERE1hiAIiM2Ixanbp3A26SwOPn1QJxPDMJRrYCgTEZExaZNDhu8rICIiojoxlImIiEwEQ5mIiMhEMJSJiIhMBEOZiIjIRDCUiYiITARDmYiIyEQwlImIiEwEQ5mIiMhEMJSJiIhMRMt4snUzGeMh50RERNpq9aGsy3mvOY82z0E1nocqPA88B9V4HnRzDth9TUREZCIYykRERCaCoayFNWvWGLsEo+M5qMLzUIXngeegGs+Dbs5Bq3+eMhERUUvBljIREZGJYCgTERGZCIYyERGRiWAoExERmQiGciOUl5cjKCgI9vb2cHBwQHBwMDQajbHLMpg5c+bA0tIScrlcfJ07d87YZendtm3b0L9/f8hkMkyePLnW7/Lz8zFz5kzY2trCxcUFa9euNU6RBvCg8zB8+HDIZLJa18bdu3eNU6gelZaWYv78+ejcuTMUCgW6d++Offv2ib9vK9dDQ+ehrVwPwcHB6NChA2xtbdG+fXssWbIEZWVlAJp/LTCUGyEsLAyRkZGIjY1FTEwMzp49i/DwcGOXZVALFy6EWq0WX4MGDTJ2SXrn7u6OVatWYf78+ff9Ljg4GNnZ2UhKSsLZs2exZ88efPzxx0aoUv8edB4AYMOGDbWuDXd3dwNXqH8ajQZubm748ccfkZ+fj/379+Nf//oXfvjhBwBt53po6DwAbeN6WLhwIf7880/k5+fjt99+w2+//YZ33nkHQPOvBYZyI+zbtw+rVq2Cm5sb3NzcsHLlSuzdu9fYZZGePf3005g8eTKcnJxqLS8qKsLnn3+OsLAwKJVK+Pj4IDg4uNVeE/Wdh7bExsYGb731Fry8vCCRSDBw4ECMGDECkZGRbep6eNB5aEt8fX1hY2MDoGoqZzMzM8TFxenkWmAoNyAnJwfJycnw9/cXl/n7+yMpKQl5eXnGK8zAPv74Yzg4OKBHjx7YtGkTKisrjV2S0Vy/fh1lZWX3XRO///678YoyorCwMDg4OKBv376tsnVYl5KSEly8eBG9e/du09dDzfNQra1cD+vXr4dcLoezszN+++03BAcH6+RaYCg3QK1WAwCUSqW4rPp9QUGBESoyvMWLF+P69evIyMjA3r17sXXrVmzdutXYZRmNWq2GjY0NpNK/nueiVCrbzPVQ09tvv42bN28iLS0N69evR3BwMI4cOWLssvRKEATMmzcPXbt2xdNPP91mr4e/nwegbV0Pr7/+OtRqNWJjYxEYGAhXV1edXAsM5QbI5XIAqNUqrn6vUCiMUpOh9evXDyqVCubm5hg4cCBef/11fPHFF8Yuy2jkcjmKiopqDfbLy8trM9dDTYMGDYKdnR0sLCzw+OOP4+WXX27V14YgCFi4cCGuX7+Or7/+GmZmZm3yeqjrPABt73oAqrqy+/Tpgzlz5ujkWmAoN8De3h4eHh6IiooSl0VFRaFDhw6ws7MzXmFGVP0fYFvVrVs3WFhY4LfffhOXRUVFoVevXkasyjS05mtDEAQsWrQIFy5cwA8//CD+99/Wrof6zkNdWvP1UFN5eTni4uJ0ci20jTPWTHPnzsW6deuQmpqK1NRUhIeHY968ecYuy2C+/PJL5OfnQxAEXL58GevXr8czzzxj7LL0TqPRoKSkBBqNBpWVlSgpKUFZWRmsra0xbdo0rF69Gnl5eYiLi8P777/faq+J+s5Dbm4ujh8/jqKiIlRUVODUqVPYtWtXq702goKC8MsvvyAiIgL29vbi8rZ2PdR3HtrK9aBWq/HRRx8hNzcXgiAgOjoaYWFhePzxx3VzLQjUoLKyMmHhwoWCUqkUlEqlEBQUJJSXlxu7LIMZNmyYYGdnJ9jY2Ag+Pj7Chg0bhIqKCmOXpXdr1qwRANR6Pfroo4IgCEJeXp4wffp0QS6XCyqVSnjzzTeNW6we1Xce0tPThQEDBggKhUJQKBRCr169hL179xq7XL1ISEgQAAgymUywsbERXy+//LIgCG3nenjQeWgr14NarRYee+wxwcHBQbCxsRE6d+4sLF26VCgsLBQEofnXAp8SRUREZCLYfU1ERGQiGMpEREQmgqFMRERkIhjKREREJoKhTEREZCIYykRERCaCoUxERGQiGMpE1CyhoaGYPHmyscsgahUYykStyPDhwyGTySCXy8VXW34OMlFLw1AmamU2bNgAtVotvjIzM41dEhE1EkOZqI2QSCTYunUrunXrBqVSiWnTptV6JOnly5cxZMgQKJVK+Pn54bPPPqu1/WeffYY+ffrA1tYWnp6e2L9/v/i7iooKBAUFQalUomPHjrUe1xcREYHevXtDoVDAxcUFCxYs0PtnJWqpGMpEbcgnn3yC06dPIyEhATk5OViyZAmAqif8jB07FtOnT0dGRgZ27tyJ+fPn45dffgEAfPvttwgKCsLmzZuRm5uLS5cuoU+fPuJ+v//+ezzyyCPIyspCWFgY5s2bJz7Yffbs2XjttddQUFCAW7du4R//+IfBPzdRS8FQJmpl3njjDSiVSvE1evRo8XfLli2Du7s7lEol1q5di0OHDqGyshLHjh2DSqVCcHAwLCws8Oijj2LmzJk4cOAAAGDHjh145ZVXMHLkSJiZmcHZ2Rl9+/YV99uvXz9MnToV5ubm+Mc//oGysjLcuHEDAGBhYYH4+HhkZGTAxsYGgwcPNuwJIWpBGMpErczbb7+N3Nxc8RURESH+ztPTs9b7srIyZGRkIDk5GZ06daq1ny5duiA5ORkAkJiYiK5du9Z7TFdXV/G9RCJBu3btxJbykSNH8Mcff6Bbt27o27cvvvzyS118TKJWiaFM1IYkJiaK75OSkmBpaQmVSgUPDw8kJCTUWjchIQEeHh4AqgI8Pj6+Scfs168f/vOf/yAzMxOrV6/GzJkzkZaW1uTPQNSaMZSJ2pCNGzfi7t27yM3NRUhICKZPnw4zMzOMHz8e6enp2LFjBzQaDc6ePYuDBw/i+eefBwC8/PLL2Lp1K37++WdUVlYiPT0dV69ebfB4ZWVl+OSTT5CTkwMzMzMolUoAgFQq1efHJGqxGMpErczy5ctrfU9ZLpcjKysLADBr1iyMGDECnp6eUCgU2Lp1KwDA3t4eJ06cwKeffgpHR0e89NJL2LlzJ4YOHQoAmDx5Mt59910sWrQIdnZ2CAgIQHR0dKPqOXToELy9vaFQKBAcHIxDhw7B0dFRPx+eqIWTCIIgGLsIItI/iUSCq1evwt/f39ilEFE92FImIiIyEQxlIiIiE8HRFkRtBO9UEZk+tpSJiIhMBEOZiIjIRDCUiYiITARDmYiIyEQwlImIiEwEQ5mIiMhEMJSJiIhMBEOZiIjIRPx/nt/O7XUBjIsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 540x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Training and Test loss\n",
    "plt.plot(epochs_array, train_loss_array, 'g', label='Training loss')\n",
    "# plt.plot(epochs_array, test_loss_array, 'b', label='Test loss')\n",
    "plt.title('Training and Test loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60ae2ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:50.341169Z",
     "iopub.status.busy": "2023-11-17T11:57:50.340568Z",
     "iopub.status.idle": "2023-11-17T11:57:50.345019Z",
     "shell.execute_reply": "2023-11-17T11:57:50.344051Z"
    },
    "papermill": {
     "duration": 0.043513,
     "end_time": "2023-11-17T11:57:50.347057",
     "exception": false,
     "start_time": "2023-11-17T11:57:50.303544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.jit import load\n",
    "# model = UNet()\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# checkpoint = torch.load(pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "677eb83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:50.424406Z",
     "iopub.status.busy": "2023-11-17T11:57:50.424067Z",
     "iopub.status.idle": "2023-11-17T11:57:50.428573Z",
     "shell.execute_reply": "2023-11-17T11:57:50.427634Z"
    },
    "papermill": {
     "duration": 0.047641,
     "end_time": "2023-11-17T11:57:50.430753",
     "exception": false,
     "start_time": "2023-11-17T11:57:50.383112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c51fb61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:50.508447Z",
     "iopub.status.busy": "2023-11-17T11:57:50.507588Z",
     "iopub.status.idle": "2023-11-17T11:57:50.511845Z",
     "shell.execute_reply": "2023-11-17T11:57:50.511016Z"
    },
    "papermill": {
     "duration": 0.044281,
     "end_time": "2023-11-17T11:57:50.513710",
     "exception": false,
     "start_time": "2023-11-17T11:57:50.469429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint['model'].items():\n",
    "#     name = k[7:] # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "# # load params\n",
    "# model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3caa760b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:50.584872Z",
     "iopub.status.busy": "2023-11-17T11:57:50.584070Z",
     "iopub.status.idle": "2023-11-17T11:57:50.735445Z",
     "shell.execute_reply": "2023-11-17T11:57:50.734368Z"
    },
    "papermill": {
     "duration": 0.188411,
     "end_time": "2023-11-17T11:57:50.737770",
     "exception": false,
     "start_time": "2023-11-17T11:57:50.549359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, load in enumerate(train_loader):\n",
    "    img = load['image']\n",
    "    mask = load['mask']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a62fcdc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:50.826548Z",
     "iopub.status.busy": "2023-11-17T11:57:50.825754Z",
     "iopub.status.idle": "2023-11-17T11:57:50.852439Z",
     "shell.execute_reply": "2023-11-17T11:57:50.851540Z"
    },
    "papermill": {
     "duration": 0.070123,
     "end_time": "2023-11-17T11:57:50.854465",
     "exception": false,
     "start_time": "2023-11-17T11:57:50.784342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = []\n",
    "tests_path = \"/kaggle/input/bkai-igh-neopolyp/test/test/\"\n",
    "for root, dirs, files in os.walk(tests_path):\n",
    "    for file in files:\n",
    "        path = os.path.join(root,file)\n",
    "        test_path.append(path)\n",
    "len(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd9b9857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:50.940345Z",
     "iopub.status.busy": "2023-11-17T11:57:50.939276Z",
     "iopub.status.idle": "2023-11-17T11:57:50.946194Z",
     "shell.execute_reply": "2023-11-17T11:57:50.945194Z"
    },
    "papermill": {
     "duration": 0.057489,
     "end_time": "2023-11-17T11:57:50.948391",
     "exception": false,
     "start_time": "2023-11-17T11:57:50.890902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_test_dataset = UnetDataClass(test_path, mode = \"test\")\n",
    "# unet_test_dataset = UNetTestDataClass(test_path, mode = \"test\")\n",
    "test_dataloader = DataLoader(unet_test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55860cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:51.025944Z",
     "iopub.status.busy": "2023-11-17T11:57:51.025544Z",
     "iopub.status.idle": "2023-11-17T11:57:51.058849Z",
     "shell.execute_reply": "2023-11-17T11:57:51.057911Z"
    },
    "papermill": {
     "duration": 0.072965,
     "end_time": "2023-11-17T11:57:51.060980",
     "exception": false,
     "start_time": "2023-11-17T11:57:50.988015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_test_dataset.__getitem__(7)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e18768a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:51.135245Z",
     "iopub.status.busy": "2023-11-17T11:57:51.134829Z",
     "iopub.status.idle": "2023-11-17T11:57:51.360695Z",
     "shell.execute_reply": "2023-11-17T11:57:51.359630Z"
    },
    "papermill": {
     "duration": 0.265108,
     "end_time": "2023-11-17T11:57:51.363277",
     "exception": false,
     "start_time": "2023-11-17T11:57:51.098169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (data, path, h, w) in enumerate(test_dataloader):\n",
    "    img = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "812d4d99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:57:51.441287Z",
     "iopub.status.busy": "2023-11-17T11:57:51.440867Z",
     "iopub.status.idle": "2023-11-17T11:58:08.047464Z",
     "shell.execute_reply": "2023-11-17T11:58:08.046335Z"
    },
    "papermill": {
     "duration": 16.648064,
     "end_time": "2023-11-17T11:58:08.049892",
     "exception": false,
     "start_time": "2023-11-17T11:57:51.401828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def mask2rgb(mask):\n",
    "    color_dict = {0: torch.tensor([0, 0, 0]),\n",
    "                  1: torch.tensor([1, 0, 0]),\n",
    "                  2: torch.tensor([0, 1, 0])}\n",
    "    output = torch.zeros((mask.shape[0], mask.shape[1], 3)).long()\n",
    "    for k in color_dict.keys():\n",
    "        output[mask.long() == k] = color_dict[k]\n",
    "    return output.to(mask.device)\n",
    "\n",
    "if not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n",
    "    os.mkdir(\"/kaggle/working/predicted_masks\")\n",
    "for _, (img, path, H, W) in enumerate(test_dataloader):\n",
    "    a = path\n",
    "    b = img\n",
    "    h = H\n",
    "    w = W\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_mask = model(b)\n",
    "    for i in range(len(a)):\n",
    "        image_id = a[i].split('/')[-1].split('.')[0]\n",
    "        filename = image_id + \".png\"\n",
    "        argmax = torch.argmax(predicted_mask[i], 0)\n",
    "        one_hot = mask2rgb(argmax).float().permute(2, 0, 1)\n",
    "        mask2img = Resize((H[i].item(), W[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(one_hot))\n",
    "#         mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n",
    "        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c912cb1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T11:58:08.121068Z",
     "iopub.status.busy": "2023-11-17T11:58:08.120678Z",
     "iopub.status.idle": "2023-11-17T11:58:10.922368Z",
     "shell.execute_reply": "2023-11-17T11:58:10.920983Z"
    },
    "papermill": {
     "duration": 2.840534,
     "end_time": "2023-11-17T11:58:10.925591",
     "exception": false,
     "start_time": "2023-11-17T11:58:08.085057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/predicted_masks/285e26c90e1797c77826f9a7021bab9f.png\n",
      "/kaggle/working/predicted_masks/1c0e9082ea2c193ac8d551c149b60f29.png\n",
      "/kaggle/working/predicted_masks/7af2ed9fbb63b28163a745959c039830.png\n",
      "/kaggle/working/predicted_masks/39d6aad6bb0170a40ed32deef71fbe08.png\n",
      "/kaggle/working/predicted_masks/dd78294679c9cbb2a365b5574868eb60.png\n",
      "/kaggle/working/predicted_masks/6ddca6ee1af35b65bd9ea42cfcfedb5e.png\n",
      "/kaggle/working/predicted_masks/5b21960c94b0aab4c024a573c692195f.png\n",
      "/kaggle/working/predicted_masks/d6240619ebebe9e9c9d00a4262b4fe4a.png\n",
      "/kaggle/working/predicted_masks/8954bb13d3727c7e5e1069646f2f0bb8.png\n",
      "/kaggle/working/predicted_masks/94a7f32574d6c748c41743c6c08a1d1a.png\n",
      "/kaggle/working/predicted_masks/f7fdb2d45b21960c94b0aab4c024a573.png\n",
      "/kaggle/working/predicted_masks/0619ebebe9e9c9d00a4262b4fe4a5a95.png\n",
      "/kaggle/working/predicted_masks/4ef4d95ceea11957998906d3694abb47.png\n",
      "/kaggle/working/predicted_masks/05b78a91391adc0bb223c4eaf3372eae.png\n",
      "/kaggle/working/predicted_masks/0a5f3601ad4f13ccf1f4b331a412fc44.png\n",
      "/kaggle/working/predicted_masks/318ecf467d7ad048df39beb176363408.png\n",
      "/kaggle/working/predicted_masks/692195f853af7f8a4df1ec859759b7c8.png\n",
      "/kaggle/working/predicted_masks/c695325ded465efde988dfb96d081533.png\n",
      "/kaggle/working/predicted_masks/3b8318ecf467d7ad048df39beb176363.png\n",
      "/kaggle/working/predicted_masks/ad43fe2cd066b9fdbc3bbc04a3afe1f1.png\n",
      "/kaggle/working/predicted_masks/63b8318ecf467d7ad048df39beb17636.png\n",
      "/kaggle/working/predicted_masks/625559c7e610b1531871f2fd85a04fae.png\n",
      "/kaggle/working/predicted_masks/3657e4314fe384eb2ba3adfda6c1899f.png\n",
      "/kaggle/working/predicted_masks/3c692195f853af7f8a4df1ec859759b7.png\n",
      "/kaggle/working/predicted_masks/db5eb2a0e4b50889d874c68c030b9afe.png\n",
      "/kaggle/working/predicted_masks/fe1f119f21b248d152b672ab3492fc62.png\n",
      "/kaggle/working/predicted_masks/6240619ebebe9e9c9d00a4262b4fe4a5.png\n",
      "/kaggle/working/predicted_masks/6231002ec4a1fe748f3085f1ce88cbdf.png\n",
      "/kaggle/working/predicted_masks/26679bff55177a34fc01019eec999fd8.png\n",
      "/kaggle/working/predicted_masks/1b62f15ec83b97bb11e8e0c4416c1931.png\n",
      "/kaggle/working/predicted_masks/fcd6da15fc656702fa602bb3c7abacdb.png\n",
      "/kaggle/working/predicted_masks/3bbc04a3afe1f119f21b248d152b672a.png\n",
      "/kaggle/working/predicted_masks/a9d45c3dbc695325ded465efde988dfb.png\n",
      "/kaggle/working/predicted_masks/ff55177a34fc01019eec999fd84e679b.png\n",
      "/kaggle/working/predicted_masks/7ad1cf2eb9d32a3dc907950289e976c7.png\n",
      "/kaggle/working/predicted_masks/4fda8daadc8dd23ae214d84b5dec33fd.png\n",
      "/kaggle/working/predicted_masks/f8e5ad89d2844837f2a0f1536ad3f6a5.png\n",
      "/kaggle/working/predicted_masks/3c84417fda8019410b1fcf0625f608b4.png\n",
      "/kaggle/working/predicted_masks/a6d9ba9d45c3dbc695325ded465efde9.png\n",
      "/kaggle/working/predicted_masks/c5a0808bee60b246359c68c836f843dc.png\n",
      "/kaggle/working/predicted_masks/4e8bfb905b78a91391adc0bb223c4eaf.png\n",
      "/kaggle/working/predicted_masks/be86f03d900fd197cd955fa095f97845.png\n",
      "/kaggle/working/predicted_masks/27738677a6b1f2c6d40b3bbba8f6c704.png\n",
      "/kaggle/working/predicted_masks/c22268d4b4ef4d95ceea11957998906d.png\n",
      "/kaggle/working/predicted_masks/afe1f119f21b248d152b672ab3492fc6.png\n",
      "/kaggle/working/predicted_masks/80cae6daedd989517cb8041ed86e5822.png\n",
      "/kaggle/working/predicted_masks/6f4d4987ea3b4bae5672a230194c5a08.png\n",
      "/kaggle/working/predicted_masks/50534bca540e24f489284b8e6953ad88.png\n",
      "/kaggle/working/predicted_masks/8eb5a9a8a8d7fcc9df8e5ad89d284483.png\n",
      "/kaggle/working/predicted_masks/710d568df17586ad8f3297c819c90895.png\n",
      "/kaggle/working/predicted_masks/4417fda8019410b1fcf0625f608b4ce9.png\n",
      "/kaggle/working/predicted_masks/e8bfb905b78a91391adc0bb223c4eaf3.png\n",
      "/kaggle/working/predicted_masks/02fa602bb3c7abacdbd7e6afd56ea7bc.png\n",
      "/kaggle/working/predicted_masks/71f2fd85a04faeeb2b535797395305af.png\n",
      "/kaggle/working/predicted_masks/ca4d5060a633a8d5b2b2b55157b7781e.png\n",
      "/kaggle/working/predicted_masks/395e56a6d9ba9d45c3dbc695325ded46.png\n",
      "/kaggle/working/predicted_masks/eb1ef57af2ed9fbb63b28163a745959c.png\n",
      "/kaggle/working/predicted_masks/7330398846f67b5df7cdf3f33c3ca4d5.png\n",
      "/kaggle/working/predicted_masks/314fe384eb2ba3adfda6c1899fdc9837.png\n",
      "/kaggle/working/predicted_masks/97e1c0e9082ea2c193ac8d551c149b60.png\n",
      "/kaggle/working/predicted_masks/60b246359c68c836f843dcf41f4dce3c.png\n",
      "/kaggle/working/predicted_masks/be4d18d5401f659532897255ce2dd4ae.png\n",
      "/kaggle/working/predicted_masks/af35b65bd9ea42cfcfedb5eb2a0e4b50.png\n",
      "/kaggle/working/predicted_masks/aafac813fe3ccba3e032dd2948a80c64.png\n",
      "/kaggle/working/predicted_masks/a15fc656702fa602bb3c7abacdbd7e6a.png\n",
      "/kaggle/working/predicted_masks/4e2a6e51d077bad31c8c5f54ffaa27a6.png\n",
      "/kaggle/working/predicted_masks/72d9e593b6be1ac29adbe86f03d900fd.png\n",
      "/kaggle/working/predicted_masks/d5060a633a8d5b2b2b55157b7781e2c7.png\n",
      "/kaggle/working/predicted_masks/4ca6160127cd1d5ff99c267599fc487b.png\n",
      "/kaggle/working/predicted_masks/eff05dec1eb3a70b145a7d8d3b6c0ed7.png\n",
      "/kaggle/working/predicted_masks/41ed86e58224cb76a67d4dcf9596154e.png\n",
      "/kaggle/working/predicted_masks/cc5cfd263f1f90be28799235026b3550.png\n",
      "/kaggle/working/predicted_masks/019410b1fcf0625f608b4ce97629ab55.png\n",
      "/kaggle/working/predicted_masks/67d4dcf9596154efb7cef748d9cbd617.png\n",
      "/kaggle/working/predicted_masks/30c2f4fc276ed9f178dc2f4af6266509.png\n",
      "/kaggle/working/predicted_masks/8cbdf366e057db382b8564872a27301a.png\n",
      "/kaggle/working/predicted_masks/fb905b78a91391adc0bb223c4eaf3372.png\n",
      "/kaggle/working/predicted_masks/0af3feff05dec1eb3a70b145a7d8d3b6.png\n",
      "/kaggle/working/predicted_masks/05734fbeedd0f9da760db74a29abdb04.png\n",
      "/kaggle/working/predicted_masks/ea42b4eebc9e5a87e443434ac60af150.png\n",
      "/kaggle/working/predicted_masks/bec33b5e3d68f9d4c331587f9b9d49e2.png\n",
      "/kaggle/working/predicted_masks/780fd497e1c0e9082ea2c193ac8d551c.png\n",
      "/kaggle/working/predicted_masks/c7e610b1531871f2fd85a04faeeb2b53.png\n",
      "/kaggle/working/predicted_masks/d6bf62f215f0da4ad3a7ab8df9da7386.png\n",
      "/kaggle/working/predicted_masks/1ad4f13ccf1f4b331a412fc44655fb51.png\n",
      "/kaggle/working/predicted_masks/a6e51d077bad31c8c5f54ffaa27a6235.png\n",
      "/kaggle/working/predicted_masks/343f27ebc5d92b9076135d76d0bbd4ce.png\n",
      "/kaggle/working/predicted_masks/7936140a2d5fc1443c4e445927738677.png\n",
      "/kaggle/working/predicted_masks/45b21960c94b0aab4c024a573c692195.png\n",
      "/kaggle/working/predicted_masks/c193ac8d551c149b60f2965341caf528.png\n",
      "/kaggle/working/predicted_masks/88e16d4ca6160127cd1d5ff99c267599.png\n",
      "/kaggle/working/predicted_masks/d3694abb47953b0e4909384b57bb6a05.png\n",
      "/kaggle/working/predicted_masks/82ea2c193ac8d551c149b60f2965341c.png\n",
      "/kaggle/working/predicted_masks/9fc7330398846f67b5df7cdf3f33c3ca.png\n",
      "/kaggle/working/predicted_masks/782707d7c359e27888daefee82519763.png\n",
      "/kaggle/working/predicted_masks/a6a4248a41e8db8b4ed633b456aaafac.png\n",
      "/kaggle/working/predicted_masks/7b5df7cdf3f33c3ca4d5060a633a8d5b.png\n",
      "/kaggle/working/predicted_masks/3c3ca4d5060a633a8d5b2b2b55157b77.png\n",
      "/kaggle/working/predicted_masks/5c1346e62522325c1b9c4fc9cbe1eca1.png\n",
      "/kaggle/working/predicted_masks/3dd311a65d2b46d0a6085835c525af63.png\n",
      "/kaggle/working/predicted_masks/cb1b387133b51209db6dcdda5cc8a788.png\n",
      "/kaggle/working/predicted_masks/4c1711b62f15ec83b97bb11e8e0c4416.png\n",
      "/kaggle/working/predicted_masks/68d4b4ef4d95ceea11957998906d3694.png\n",
      "/kaggle/working/predicted_masks/7f0019f7e6af7d7147763bdfb928d788.png\n",
      "/kaggle/working/predicted_masks/6679bff55177a34fc01019eec999fd84.png\n",
      "/kaggle/working/predicted_masks/15fc656702fa602bb3c7abacdbd7e6af.png\n",
      "/kaggle/working/predicted_masks/dd094a7f32574d6c748c41743c6c08a1.png\n",
      "/kaggle/working/predicted_masks/a51625559c7e610b1531871f2fd85a04.png\n",
      "/kaggle/working/predicted_masks/9632a3c6f7f7fb2a643f15bd0249ddcc.png\n",
      "/kaggle/working/predicted_masks/f14e1e0ae936de314f2d95e6c487ffa6.png\n",
      "/kaggle/working/predicted_masks/391adc0bb223c4eaf3372eae567c94ea.png\n",
      "/kaggle/working/predicted_masks/6d3694abb47953b0e4909384b57bb6a0.png\n",
      "/kaggle/working/predicted_masks/dc70626ab4ec3d46e602b296cc5cfd26.png\n",
      "/kaggle/working/predicted_masks/c656702fa602bb3c7abacdbd7e6afd56.png\n",
      "/kaggle/working/predicted_masks/60a633a8d5b2b2b55157b7781e2c706c.png\n",
      "/kaggle/working/predicted_masks/98da48d679d7c7c8d3d96fb2b87fbbcf.png\n",
      "/kaggle/working/predicted_masks/5a51625559c7e610b1531871f2fd85a0.png\n",
      "/kaggle/working/predicted_masks/d694539ef2424a9218697283baa3657e.png\n",
      "/kaggle/working/predicted_masks/7f32574d6c748c41743c6c08a1d1ad8f.png\n",
      "/kaggle/working/predicted_masks/0a0317371a966bf4b3466463a3c64db1.png\n",
      "/kaggle/working/predicted_masks/85a04faeeb2b535797395305af926a6f.png\n",
      "/kaggle/working/predicted_masks/268d4b4ef4d95ceea11957998906d369.png\n",
      "/kaggle/working/predicted_masks/fdbc3bbc04a3afe1f119f21b248d152b.png\n",
      "/kaggle/working/predicted_masks/4f437f0019f7e6af7d7147763bdfb928.png\n",
      "/kaggle/working/predicted_masks/461c2a337948a41964c1d4f50a5f3601.png\n",
      "/kaggle/working/predicted_masks/7cdf3f33c3ca4d5060a633a8d5b2b2b5.png\n",
      "/kaggle/working/predicted_masks/dc0bb223c4eaf3372eae567c94ea04c6.png\n",
      "/kaggle/working/predicted_masks/b21960c94b0aab4c024a573c692195f8.png\n",
      "/kaggle/working/predicted_masks/1209db6dcdda5cc8a788edaeb6aa460a.png\n",
      "/kaggle/working/predicted_masks/80c643782707d7c359e27888daefee82.png\n",
      "/kaggle/working/predicted_masks/77e004e8bfb905b78a91391adc0bb223.png\n",
      "/kaggle/working/predicted_masks/f8e26031fbb5e52c41545ba55aadaa77.png\n",
      "/kaggle/working/predicted_masks/2d9e593b6be1ac29adbe86f03d900fd1.png\n",
      "/kaggle/working/predicted_masks/8b8ec74baddc22268d4b4ef4d95ceea1.png\n",
      "/kaggle/working/predicted_masks/5beb48f0be11d0309d1dff09b8405734.png\n",
      "/kaggle/working/predicted_masks/e4a17af18f72c8e6166a915669c99390.png\n",
      "/kaggle/working/predicted_masks/e7998934d417cb2eb1ef57af2ed9fbb6.png\n",
      "/kaggle/working/predicted_masks/1002ec4a1fe748f3085f1ce88cbdf366.png\n",
      "/kaggle/working/predicted_masks/e19769fa2d37d32780fd497e1c0e9082.png\n",
      "/kaggle/working/predicted_masks/cdf3f33c3ca4d5060a633a8d5b2b2b55.png\n",
      "/kaggle/working/predicted_masks/faef7fdb2d45b21960c94b0aab4c024a.png\n",
      "/kaggle/working/predicted_masks/6b83ef461c2a337948a41964c1d4f50a.png\n",
      "/kaggle/working/predicted_masks/8fa8625605da2023387fd56c04414eaa.png\n",
      "/kaggle/working/predicted_masks/5026b3550534bca540e24f489284b8e6.png\n",
      "/kaggle/working/predicted_masks/1531871f2fd85a04faeeb2b535797395.png\n",
      "/kaggle/working/predicted_masks/3f33c3ca4d5060a633a8d5b2b2b55157.png\n",
      "/kaggle/working/predicted_masks/7cb2eb1ef57af2ed9fbb63b28163a745.png\n",
      "/kaggle/working/predicted_masks/e5e8f14e1e0ae936de314f2d95e6c487.png\n",
      "/kaggle/working/predicted_masks/0626ab4ec3d46e602b296cc5cfd263f1.png\n",
      "/kaggle/working/predicted_masks/e1e0ae936de314f2d95e6c487ffa651b.png\n",
      "/kaggle/working/predicted_masks/5e8f14e1e0ae936de314f2d95e6c487f.png\n",
      "/kaggle/working/predicted_masks/677a6b1f2c6d40b3bbba8f6c704801b3.png\n",
      "/kaggle/working/predicted_masks/e3c84417fda8019410b1fcf0625f608b.png\n",
      "/kaggle/working/predicted_masks/6ad1468996b4a9ce6d840b53a6558038.png\n",
      "/kaggle/working/predicted_masks/626650908b1cb932a767bf5487ced51b.png\n",
      "/kaggle/working/predicted_masks/2ed9fbb63b28163a745959c03983064a.png\n",
      "/kaggle/working/predicted_masks/3425b976973f13dd311a65d2b46d0a60.png\n",
      "/kaggle/working/predicted_masks/6f67b5df7cdf3f33c3ca4d5060a633a8.png\n",
      "/kaggle/working/predicted_masks/cf6644589e532a9ee954f81faedbce39.png\n",
      "/kaggle/working/predicted_masks/e73749a0d21db70dd094a7f32574d6c7.png\n",
      "/kaggle/working/predicted_masks/66e057db382b8564872a27301a654864.png\n",
      "/kaggle/working/predicted_masks/f13dd311a65d2b46d0a6085835c525af.png\n",
      "/kaggle/working/predicted_masks/87133b51209db6dcdda5cc8a788edaeb.png\n",
      "/kaggle/working/predicted_masks/0fca6a4248a41e8db8b4ed633b456aaa.png\n",
      "/kaggle/working/predicted_masks/936de314f2d95e6c487ffa651b477422.png\n",
      "/kaggle/working/predicted_masks/54ba59c7de13a35276a476420655433a.png\n",
      "/kaggle/working/predicted_masks/7fda8019410b1fcf0625f608b4ce9762.png\n",
      "/kaggle/working/predicted_masks/e2cd066b9fdbc3bbc04a3afe1f119f21.png\n",
      "/kaggle/working/predicted_masks/0398846f67b5df7cdf3f33c3ca4d5060.png\n",
      "/kaggle/working/predicted_masks/a3657e4314fe384eb2ba3adfda6c1899.png\n",
      "/kaggle/working/predicted_masks/d077bad31c8c5f54ffaa27a623511c38.png\n",
      "/kaggle/working/predicted_masks/c41545ba55aadaa77712a48e11d579d9.png\n",
      "/kaggle/working/predicted_masks/e56a6d9ba9d45c3dbc695325ded465ef.png\n",
      "/kaggle/working/predicted_masks/2a365b5574868eb60861ee1ff0b8a4f6.png\n",
      "/kaggle/working/predicted_masks/df366e057db382b8564872a27301a654.png\n",
      "/kaggle/working/predicted_masks/aeeb2b535797395305af926a6f23c5d6.png\n",
      "/kaggle/working/predicted_masks/cb2eb1ef57af2ed9fbb63b28163a7459.png\n",
      "/kaggle/working/predicted_masks/cbb2a365b5574868eb60861ee1ff0b8a.png\n",
      "/kaggle/working/predicted_masks/c4be73749a0d21db70dd094a7f32574d.png\n",
      "/kaggle/working/predicted_masks/e9082ea2c193ac8d551c149b60f29653.png\n",
      "/kaggle/working/predicted_masks/633a8d5b2b2b55157b7781e2c706c75c.png\n",
      "/kaggle/working/predicted_masks/2cd066b9fdbc3bbc04a3afe1f119f21b.png\n",
      "/kaggle/working/predicted_masks/559c7e610b1531871f2fd85a04faeeb2.png\n",
      "/kaggle/working/predicted_masks/eecd70ebce6347c491b37c8c2e5a64a8.png\n",
      "/kaggle/working/predicted_masks/ff05dec1eb3a70b145a7d8d3b6c0ed75.png\n",
      "/kaggle/working/predicted_masks/cf464aa36bf7c09a3bb0e5ca159410b9.png\n",
      "/kaggle/working/predicted_masks/8395e56a6d9ba9d45c3dbc695325ded4.png\n",
      "/kaggle/working/predicted_masks/5664c1711b62f15ec83b97bb11e8e0c4.png\n",
      "/kaggle/working/predicted_masks/f62f215f0da4ad3a7ab8df9da7386835.png\n",
      "/kaggle/working/predicted_masks/9c7976c1182df0de51d32128c358d1fd.png\n",
      "/kaggle/working/predicted_masks/b70dd094a7f32574d6c748c41743c6c0.png\n",
      "/kaggle/working/predicted_masks/df8e26031fbb5e52c41545ba55aadaa7.png\n",
      "/kaggle/working/predicted_masks/425b976973f13dd311a65d2b46d0a608.png\n",
      "/kaggle/working/predicted_masks/1db239dda50f954ba59c7de13a35276a.png\n",
      "/kaggle/working/predicted_masks/a48847ae8395e56a6d9ba9d45c3dbc69.png\n",
      "/kaggle/working/predicted_masks/13dd311a65d2b46d0a6085835c525af6.png\n",
      "/kaggle/working/predicted_masks/998906d3694abb47953b0e4909384b57.png\n",
      "/kaggle/working/predicted_masks/e1797c77826f9a7021bab9fc73303988.png\n",
      "/kaggle/working/predicted_masks/4baddc22268d4b4ef4d95ceea1195799.png\n",
      "/kaggle/working/predicted_masks/39dda50f954ba59c7de13a35276a4764.png\n"
     ]
    }
   ],
   "source": [
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_encode_one_mask(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels[pixels > 0] = 255\n",
    "    use_padding = False\n",
    "    if pixels[0] or pixels[-1]:\n",
    "        use_padding = True\n",
    "        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n",
    "        pixel_padded[1:-1] = pixels\n",
    "        pixels = pixel_padded\n",
    "    \n",
    "    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    if use_padding:\n",
    "        rle = rle - 1\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    return rle_to_string(rle)\n",
    "\n",
    "def mask2string(dir):\n",
    "    ## mask --> string\n",
    "    strings = []\n",
    "    ids = []\n",
    "    ws, hs = [[] for i in range(2)]\n",
    "    for image_id in os.listdir(dir):\n",
    "        id = image_id.split('.')[0]\n",
    "        path = os.path.join(dir, image_id)\n",
    "        print(path)\n",
    "        img = cv2.imread(path)[:,:,::-1]\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        for channel in range(2):\n",
    "            ws.append(w)\n",
    "            hs.append(h)\n",
    "            ids.append(f'{id}_{channel}')\n",
    "            string = rle_encode_one_mask(img[:,:,channel])\n",
    "            strings.append(string)\n",
    "    r = {\n",
    "        'ids': ids,\n",
    "        'strings': strings,\n",
    "    }\n",
    "    return r\n",
    "\n",
    "\n",
    "MASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\n",
    "dir = MASK_DIR_PATH\n",
    "res = mask2string(dir)\n",
    "df = pd.DataFrame(columns=['Id', 'Expected'])\n",
    "df['Id'] = res['ids']\n",
    "df['Expected'] = res['strings']\n",
    "df.to_csv(r'submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2116.317505,
   "end_time": "2023-11-17T11:58:13.998606",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-17T11:22:57.681101",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
